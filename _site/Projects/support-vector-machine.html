<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="My personal documentation for Machine Learning course"
    />

    <!-- SEO Meta Tags -->
    <meta
      property="og:title"
      content="Project: Support Vector Machine"
    />
    <meta
      property="og:description"
      content="My personal documentation for Machine Learning course"
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://localhost:4000/Machine-Learning/projects/support-vector-machine.html" />
    <meta property="og:site_name" content="Machine Learning Notes" />

    <meta name="twitter:card" content="summary" />
    <meta
      name="twitter:title"
      content="Project: Support Vector Machine"
    />
    <meta
      name="twitter:description"
      content="My personal documentation for Machine Learning course"
    />

    <!-- Favicon -->
    <link
      rel="icon"
      type="image/x-icon"
      href="/Machine-Learning/assets/favicon.ico"
    />

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Project: Support Vector Machine | Machine Learning Notes</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Project: Support Vector Machine" />
<meta name="author" content="Your Name" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My personal documentation for Machine Learning course" />
<meta property="og:description" content="My personal documentation for Machine Learning course" />
<link rel="canonical" href="http://localhost:4000/Machine-Learning/projects/support-vector-machine.html" />
<meta property="og:url" content="http://localhost:4000/Machine-Learning/projects/support-vector-machine.html" />
<meta property="og:site_name" content="Machine Learning Notes" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Project: Support Vector Machine" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Your Name"},"description":"My personal documentation for Machine Learning course","headline":"Project: Support Vector Machine","url":"http://localhost:4000/Machine-Learning/projects/support-vector-machine.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- Google Analytics - Replace with your tracking ID -->
    

    <title>Project: Support Vector Machine</title>
    <style>
      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 20px;
      }
      .container {
        max-width: 1200px;
        margin: 0 auto;
        display: flex;
        gap: 20px;
      }
      .sidebar {
        flex: 0 0 250px;
        background: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
      }
      .content {
        flex: 1;
        background: white;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      .nav-link {
        display: block;
        padding: 8px 12px;
        text-decoration: none;
        color: #333;
        border-radius: 4px;
        margin: 2px 0;
      }
      .nav-link:hover {
        background: #e9ecef;
      }
      h1,
      h2,
      h3 {
        color: #2c3e50;
      }
      code {
        background: #f4f4f4;
        padding: 2px 6px;
        border-radius: 3px;
      }
      pre {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 5px;
        overflow-x: auto;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <nav class="sidebar">
        <h3>Navigation</h3>
        <a href="/Machine-Learning/" class="nav-link">Home</a>
        <h4>Introduction</h4>
        <a href="/Machine-Learning/intro/syllabus.html" class="nav-link"
          >Syllabus</a
        >
        <h4>Lectures</h4>
        <a href="/Machine-Learning/lectures/lecture1.html" class="nav-link"
          >Lecture 1</a
        >
        <a href="/Machine-Learning/lectures/lecture2.html" class="nav-link"
          >Lecture 2</a
        >
        <h4>Topics</h4>
        <a
          href="/Machine-Learning/topics/linear_regression.html"
          class="nav-link"
          >Linear Regression</a
        >
        <a
          href="/Machine-Learning/topics/logistic_regression.html"
          class="nav-link"
          >Logistic Regression</a
        >
        <a
          href="/Machine-Learning/topics/gradient_descent.html"
          class="nav-link"
          >Gradient Descent</a
        >
        <a href="/Machine-Learning/topics/cost_functions.html" class="nav-link"
          >Cost Functions</a
        >
        <h4>Projects</h4>
        <a href="/Machine-Learning/projects/" class="nav-link">Projects</a>
        <h4>References</h4>
        <a href="/Machine-Learning/references/books.html" class="nav-link"
          >Books & Papers</a
        >
        <a href="/Machine-Learning/Foundations/" class="nav-link"
          >Foundations</a
        >
      </nav>
      <main class="content"><h1 id="project-10-support-vector-machine-svm">Project 10: Support Vector Machine (SVM)</h1>

<p>This project explores Support Vector Machines for classification tasks, including kernel tricks and hyperparameter tuning.</p>

<h2 id="summary">Summary</h2>

<ul>
  <li>Goal: Classify data using optimal hyperplanes with maximum margin</li>
  <li>Concepts: Support vectors, kernels (linear, RBF), C parameter, gamma tuning</li>
  <li>Dataset: Iris dataset for multiclass classification</li>
  <li>Pipeline: Data visualization → SVM training → Hyperparameter optimization → Evaluation</li>
</ul>

<h2 id="notebook">Notebook</h2>

<ul>
  <li><a href="/Machine-Learning/projects/support-vector-machine/notebook">Rendered notebook</a></li>
</ul>

<h2 id="key-concepts">Key Concepts</h2>

<h3 id="support-vectors">Support Vectors</h3>

<p>The critical data points closest to the decision boundary that define the optimal hyperplane.</p>

<h3 id="kernel-trick">Kernel Trick</h3>

<p>Transforms data into higher dimensions to make non-linearly separable data linearly separable:</p>

<ul>
  <li><strong>Linear Kernel:</strong> For linearly separable data</li>
  <li><strong>RBF (Radial Basis Function):</strong> For non-linear patterns</li>
  <li><strong>Polynomial Kernel:</strong> For polynomial decision boundaries</li>
</ul>

<h3 id="hyperparameters">Hyperparameters</h3>

<ul>
  <li><strong>C (Regularization):</strong> Controls the trade-off between maximizing margin and minimizing classification errors
    <ul>
      <li>High C: Strict (may overfit)</li>
      <li>Low C: More tolerant (may underfit)</li>
    </ul>
  </li>
  <li><strong>Gamma:</strong> Defines the influence of single training examples
    <ul>
      <li>High gamma: Close influence (may overfit)</li>
      <li>Low gamma: Far-reaching influence (smoother decision boundary)</li>
    </ul>
  </li>
</ul>

<h2 id="applications">Applications</h2>

<ul>
  <li>Image classification</li>
  <li>Text categorization</li>
  <li>Bioinformatics (protein classification)</li>
  <li>Handwriting recognition</li>
</ul>

<h2 id="related-topics">Related Topics</h2>

<ul>
  <li><a href="/Machine-Learning/topics/logistic_regression.html">Logistic Regression</a></li>
  <li><a href="/Machine-Learning/topics/cost_functions.html">Cost Functions</a></li>
</ul>

<h2 id="key-learnings">Key Learnings</h2>

<h2 id="1-core-concept-of-k-nearest-neighbors">1. Core Concept of K-Nearest Neighbors</h2>

<p>KNN is a <strong>non-parametric, lazy learning</strong> algorithm. Unlike models like Linear or Logistic Regression, it does not explicitly learn a function or fit a line during the training phase.</p>

<h3 id="a-lazy-learning">A. Lazy Learning</h3>

<p>In KNN, all the training data is simply stored. Learning, or computation, is only performed at the time of prediction (when a new data point arrives). This is why it is called a “lazy” algorithm.</p>

<h3 id="b-the-mechanism-voting">B. The Mechanism (Voting)</h3>

<p>The fundamental idea is that similar things exist in close proximity. When a new data point (a question) arrives, the algorithm:</p>

<ol>
  <li><strong>Finds its Neighbors:</strong> Identifies the $K$ data points in the training set that are numerically closest to the new point.</li>
  <li><strong>Counts the Votes:</strong> For classification, it tallies the class labels (votes) of these $K$ neighbors.</li>
  <li><strong>Makes the Prediction:</strong> The new data point is assigned to the class that represents the <strong>majority vote</strong> among its $K$ nearest neighbors.</li>
</ol>

<p>[Image of K-Nearest Neighbors classification example]</p>

<h3 id="c-the-critical-role-of-k">C. The Critical Role of $K$</h3>

<p>The letter <strong>$K$</strong> in KNN represents the <strong>number of neighbors</strong> the model will check when making a prediction.</p>

<ul>
  <li><strong>Small $K$ (e.g., $K=1$):</strong> The model is highly flexible and sensitive to noise or outliers, leading to high variance (potential overfitting).</li>
  <li><strong>Large $K$:</strong> The model is more smoothed out, less sensitive to individual outliers, but might miss important, localized patterns (potential underfitting).</li>
</ul>

<p>The video demonstrates that changing $K$ from 3 to 5 can completely change the prediction result for the same point.</p>

<hr />

<h2 id="2-measuring-nearest-distance-calculation">2. Measuring “Nearest”: Distance Calculation</h2>

<p>The definition of “nearest” is based on the numerical distance between data points in the feature space. The most common metric used is <strong>Euclidean Distance</strong>.</p>

<h3 id="a-euclidean-distance">A. Euclidean Distance</h3>

<p>This is the standard straight-line distance between two points in Euclidean space. For two points $(x_1, y_1)$ and $(x_2, y_2)$, the distance ($d$) is calculated using the Pythagorean theorem:</p>

\[d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\]

<p>The model calculates this distance between the new data point and <strong>every single point</strong> in the training set, sorts these distances, and selects the top $K$ points.</p>

<h3 id="b-importance-of-feature-scaling">B. Importance of Feature Scaling</h3>

<p>While not explicitly detailed in the video, the use of distance metrics like Euclidean distance makes KNN highly sensitive to the <strong>scale</strong> of features. If one feature (e.g., salary, measured in hundreds of thousands) is much larger than another (e.g., age, measured in tens), the larger feature will disproportionately influence the distance calculation. In practice, data should be scaled (e.g., using Standardization or Normalization) before applying KNN.</p>

<hr />

<h2 id="3-implementation-and-evaluation-in-scikit-learn">3. Implementation and Evaluation in Scikit-learn</h2>

<p>The video illustrates the practical use of the <code class="language-plaintext highlighter-rouge">KNeighborsClassifier</code> using the famous Iris flower dataset.</p>

<h3 id="a-setup-and-splitting">A. Setup and Splitting</h3>

<ol>
  <li><strong>Prepare Data:</strong> Define $\mathbf{X}$ (features) and $\mathbf{Y}$ (target).</li>
  <li><strong>Split Data:</strong> Divide the dataset into <strong>training</strong> and <strong>testing</strong> sets using <code class="language-plaintext highlighter-rouge">train_test_split</code> (e.g., 80/20 ratio). This is essential for honest model evaluation.</li>
</ol>

<h3 id="b-model-training-and-prediction">B. Model Training and Prediction</h3>

<p>The KNN model is imported from <code class="language-plaintext highlighter-rouge">sklearn.neighbors</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># 1. Initialize the Model, specifying K (n_neighbors)
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># K=3 is used in the example
</span>
<span class="c1"># 2. Train the Model (Fit)
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span> <span class="c1"># Training is just storing the data
</span>
<span class="c1"># 3. Evaluate the Model
</span><span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="c1"># The score is the model's accuracy on the unseen test data.
</span>
<span class="c1"># 4. Make Predictions
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="c-choosing-the-optimal-k">C. Choosing the Optimal $K$</h3>

<p>The video mentions that selecting the right $K$ value is often done through experimentation. This is typically achieved by:</p>

<ol>
  <li><strong>Iterating:</strong> Training the model multiple times with different values of $K$ (e.g., $K=1, 3, 5, 7, \dots$).</li>
  <li><strong>Plotting:</strong> Plotting the resulting accuracy score for each $K$ value.</li>
  <li><strong>Selecting:</strong> Choosing the smallest $K$ value that yields the highest accuracy before the score starts to drop or oscillate (to maintain model simplicity).</li>
</ol>

<h3 id="d-strengths-and-weaknesses">D. Strengths and Weaknesses</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Strengths</th>
      <th style="text-align: left">Weaknesses</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Simple and Intuitive:</strong> Easy to understand and explain.</td>
      <td style="text-align: left"><strong>Slow Prediction Time:</strong> Prediction requires calculating the distance to <em>all</em> training points, making it very slow for large datasets.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Non-Parametric:</strong> Makes no assumptions about the data distribution.</td>
      <td style="text-align: left"><strong>Memory Intensive:</strong> Must store the entire training dataset in memory.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Effective for Classification:</strong> Often yields high accuracy, especially for small, noise-free datasets.</td>
      <td style="text-align: left"><strong>Sensitive to Scale:</strong> Requires feature scaling to perform reliably.</td>
    </tr>
  </tbody>
</table>

</main>
    </div>
  </body>
</html>
