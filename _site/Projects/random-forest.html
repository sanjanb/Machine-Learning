<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="My personal documentation for Machine Learning course"
    />

    <!-- SEO Meta Tags -->
    <meta
      property="og:title"
      content="Project: Random Forest Classifier"
    />
    <meta
      property="og:description"
      content="My personal documentation for Machine Learning course"
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://localhost:4000/Machine-Learning/projects/random-forest.html" />
    <meta property="og:site_name" content="Machine Learning Notes" />

    <meta name="twitter:card" content="summary" />
    <meta
      name="twitter:title"
      content="Project: Random Forest Classifier"
    />
    <meta
      name="twitter:description"
      content="My personal documentation for Machine Learning course"
    />

    <!-- Favicon -->
    <link
      rel="icon"
      type="image/x-icon"
      href="/Machine-Learning/assets/favicon.ico"
    />

    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Project: Random Forest Classifier | Machine Learning Notes</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Project: Random Forest Classifier" />
<meta name="author" content="Your Name" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My personal documentation for Machine Learning course" />
<meta property="og:description" content="My personal documentation for Machine Learning course" />
<link rel="canonical" href="http://localhost:4000/Machine-Learning/projects/random-forest.html" />
<meta property="og:url" content="http://localhost:4000/Machine-Learning/projects/random-forest.html" />
<meta property="og:site_name" content="Machine Learning Notes" />
<meta property="og:image" content="http://localhost:4000/Machine-Learning/Machine-Learning/assets/images/og-default.svg" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/Machine-Learning/Machine-Learning/assets/images/og-default.svg" />
<meta property="twitter:title" content="Project: Random Forest Classifier" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Your Name"},"description":"My personal documentation for Machine Learning course","headline":"Project: Random Forest Classifier","image":"http://localhost:4000/Machine-Learning/Machine-Learning/assets/images/og-default.svg","url":"http://localhost:4000/Machine-Learning/projects/random-forest.html"}</script>
<!-- End Jekyll SEO tag -->


    <!-- Google Analytics - Replace with your tracking ID -->
    

    <title>Project: Random Forest Classifier</title>
    <style>
      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,
          sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 20px;
      }
      .container {
        max-width: 1200px;
        margin: 0 auto;
        display: flex;
        gap: 20px;
      }
      .sidebar {
        flex: 0 0 250px;
        background: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
      }
      .content {
        flex: 1;
        background: white;
        padding: 20px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
      }
      .nav-link {
        display: block;
        padding: 8px 12px;
        text-decoration: none;
        color: #333;
        border-radius: 4px;
        margin: 2px 0;
      }
      .nav-link:hover {
        background: #e9ecef;
      }
      h1,
      h2,
      h3 {
        color: #2c3e50;
      }
      code {
        background: #f4f4f4;
        padding: 2px 6px;
        border-radius: 3px;
      }
      pre {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 5px;
        overflow-x: auto;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <nav class="sidebar">
        <h3>Navigation</h3>
        <a href="/Machine-Learning/" class="nav-link">Home</a>
        <h4>Introduction</h4>
        <a href="/Machine-Learning/intro/syllabus.html" class="nav-link"
          >Syllabus</a
        >
        <h4>Lectures</h4>
        <a href="/Machine-Learning/lectures/lecture1.html" class="nav-link"
          >Lecture 1</a
        >
        <a href="/Machine-Learning/lectures/lecture2.html" class="nav-link"
          >Lecture 2</a
        >
        <h4>Topics</h4>
        <a
          href="/Machine-Learning/topics/linear_regression.html"
          class="nav-link"
          >Linear Regression</a
        >
        <a
          href="/Machine-Learning/topics/logistic_regression.html"
          class="nav-link"
          >Logistic Regression</a
        >
        <a
          href="/Machine-Learning/topics/gradient_descent.html"
          class="nav-link"
          >Gradient Descent</a
        >
        <a href="/Machine-Learning/topics/cost_functions.html" class="nav-link"
          >Cost Functions</a
        >
        <h4>Projects</h4>
        <a href="/Machine-Learning/projects/" class="nav-link">Projects</a>
        <h4>References</h4>
        <a href="/Machine-Learning/references/books.html" class="nav-link"
          >Books & Papers</a
        >
        <a href="/Machine-Learning/Foundations/" class="nav-link"
          >Foundations</a
        >
      </nav>
      <main class="content"><h1 id="project-11-random-forest-classifier">Project 11: Random Forest Classifier</h1>

<p>This project implements a Random Forest for classification, exploring ensemble methods, feature importance, and model evaluation.</p>

<h2 id="summary">Summary</h2>

<ul>
  <li>Goal: Build robust classifiers using ensembles of decision trees</li>
  <li>Concepts: Bagging, feature sampling, out-of-bag error, feature importance</li>
  <li>Pipeline: Preprocess → Train → Evaluate → Interpret</li>
</ul>

<h2 id="notebook">Notebook</h2>

<ul>
  <li><a href="/Machine-Learning/projects/random-forest/notebook">Rendered notebook</a></li>
  <li>Open in Colab: <a href="/redirect?target=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2Fsanjanb%2FMachine-Learning%2Fblob%2Fmain%2Fprojects%2F11.%2520Random%2520Forest%2520Classifier%2Frandom-forest.ipynb">Launch</a></li>
</ul>

<h2 id="exercise">Exercise</h2>

<p>Explore the exercises in the project folder if available.</p>

<h2 id="related-topics">Related Topics</h2>

<ul>
  <li><a href="/Machine-Learning/projects/decision-trees.html">Decision Trees</a></li>
  <li><a href="/Machine-Learning/topics/cost_functions.html">Cost Functions</a></li>
</ul>

<h2 id="key-learnings">Key Learnings</h2>

<h2 id="1-understanding-random-forest-the-ensemble-concept">1. Understanding Random Forest (The Ensemble Concept)</h2>

<p>The Random Forest algorithm is essentially a collection of <strong>Decision Trees</strong> [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=8">00:08</a>]. The term “Forest” highlights the use of multiple trees, and “Random” refers to the specific random processes used to build them.</p>

<h3 id="a-ensemble-learning">A. Ensemble Learning</h3>

<p>Random Forest is a type of <strong>Ensemble Learning</strong>, which means it relies on the principle of combining multiple simpler models (the individual Decision Trees) to create one powerful model. This approach generally leads to better performance than any single constituent model.</p>

<h3 id="b-the-core-idea-majority-voting">B. The Core Idea: Majority Voting</h3>

<p>The model works through a “majority vote” system, analogous to asking multiple experts for their opinion [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=91">01:31</a>]:</p>

<ol>
  <li><strong>Training:</strong> The algorithm builds many independent Decision Trees.</li>
  <li><strong>Prediction:</strong> A new data point is fed to every tree in the forest.</li>
  <li><strong>Aggregation:</strong>
    <ul>
      <li><strong>Classification:</strong> The final prediction is the class that receives the <strong>majority vote</strong> from all the individual trees (e.g., if 7 out of 10 trees predict ‘Yes’, the final answer is ‘Yes’).</li>
      <li><strong>Regression:</strong> The final prediction is the <strong>average</strong> of the outputs from all the individual trees.</li>
    </ul>
  </li>
</ol>

<h3 id="c-why-it-works-reducing-variance">C. Why It Works: Reducing Variance</h3>

<p>A single Decision Tree is often prone to <strong>overfitting</strong> (high variance), meaning it performs perfectly on training data but poorly on unseen data. Random Forest tackles this by:</p>

<ul>
  <li><strong>Averaging Out Errors:</strong> Since each tree is trained on a different subset of data (or features), their individual errors tend to be uncorrelated. By averaging or voting, these errors cancel each other out, leading to a much more stable and generalized model.</li>
</ul>

<hr />

<h2 id="2-the-two-layers-of-randomness">2. The Two Layers of Randomness</h2>

<p>The “Random” part of Random Forest comes from two sources, ensuring that each tree in the ensemble is unique:</p>

<h3 id="a-random-sampling-of-data-bagging">A. Random Sampling of Data (Bagging)</h3>

<p>Each tree is trained on a different, randomly selected subset of the <strong>training data</strong> (often called a bootstrap sample, or <strong>Bagging</strong> - Bootstrap Aggregating). This means that for a dataset with $N$ samples, each tree is trained on a new dataset of $N$ samples, drawn with replacement from the original data. This ensures the trees are diverse [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=54">00:54</a>].</p>

<h3 id="b-random-subset-of-features">B. Random Subset of Features</h3>

<p>When deciding on the best split at any node, each tree only considers a <strong>random subset of the available features</strong> (e.g., if you have 50 features, a tree might only consider 10 of them for a split). This forces the trees to be diverse and prevents any single dominant feature from dictating the structure of every tree [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=490">08:10</a>].</p>

<hr />

<h2 id="3-implementation-in-scikit-learn-digits-dataset">3. Implementation in Scikit-learn (Digits Dataset)</h2>

<p>The video demonstrates the <code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> using the <strong>Digits dataset</strong> (a multi-class classification problem of handwritten digits 0-9) [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=166">02:46</a>].</p>

<h3 id="a-data-preparation">A. Data Preparation</h3>

<p>The digits dataset, consisting of 8x8 pixel arrays (64 features) mapping to a target digit (0-9), is loaded. The data is then split into <strong>training</strong> and <strong>testing</strong> sets using <code class="language-plaintext highlighter-rouge">train_test_split</code> (20% for testing in the example) [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=327">05:27</a>].</p>

<h3 id="b-training-the-classifier">B. Training the Classifier</h3>

<p>The <code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> is imported from <code class="language-plaintext highlighter-rouge">sklearn.ensemble</code>, signifying its status as an ensemble method [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=428">07:08</a>].</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Initialize the model, specifying the number of trees (n_estimators)
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># 10 trees initially
</span>
<span class="c1"># Train the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>The parameter <strong><code class="language-plaintext highlighter-rouge">n_estimators</code></strong> controls the number of Decision Trees (estimators) in the forest [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=498">08:18</a>].</p>

<h3 id="c-hyperparameter-tuning-n_estimators">C. Hyperparameter Tuning (<code class="language-plaintext highlighter-rouge">n_estimators</code>)</h3>

<p>The number of trees (<code class="language-plaintext highlighter-rouge">n_estimators</code>) is a key <strong>hyperparameter</strong> to tune. The video shows that increasing this value often increases accuracy, up to a point where the gains become negligible [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=529">08:49</a>].</p>

<ul>
  <li><strong>Low <code class="language-plaintext highlighter-rouge">n_estimators</code> (e.g., 10):</strong> Low accuracy (e.g., 91%).</li>
  <li><strong>High <code class="language-plaintext highlighter-rouge">n_estimators</code> (e.g., 30 or more):</strong> Higher, more stable accuracy (e.g., 96-97%).</li>
</ul>

<p>The optimal number of estimators balances accuracy improvements with increased computation time.</p>

<hr />

<h2 id="4-model-evaluation-confusion-matrix">4. Model Evaluation: Confusion Matrix</h2>

<p>While the <code class="language-plaintext highlighter-rouge">.score()</code> method gives a single accuracy number, a <strong>Confusion Matrix</strong> is essential for multi-class problems to understand <em>where</em> the model is making errors [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=587">09:47</a>].</p>

<h3 id="a-how-a-confusion-matrix-works">A. How a Confusion Matrix Works</h3>

<p>The matrix plots the <strong>Actual Values</strong> (Truth) on one axis against the <strong>Predicted Values</strong> on the other.</p>

<ul>
  <li><strong>Diagonal:</strong> The numbers along the main diagonal (e.g., 46) show the number of correct predictions for each class (e.g., 46 times the truth was ‘0’ and the model predicted ‘0’) [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=689">11:29</a>].</li>
  <li><strong>Off-Diagonal:</strong> Any number off the diagonal shows an error (a misclassification). For instance, a ‘2’ in the row for ‘Actual 8’ and column for ‘Predicted 1’ means the model mistakenly predicted ‘1’ when the true digit was ‘8’ [<a href="http://www.youtube.com/watch?v=ok2s1vV9XW0&amp;t=701">11:41</a>].</li>
</ul>

<p>By visualizing the confusion matrix (often using libraries like Seaborn), a data scientist can pinpoint which digits or classes are being confused by the model, allowing for targeted model improvements.</p>

<p>http://googleusercontent.com/youtube_content/18</p>

</main>
    </div>
  </body>
</html>
