[
  
  
  
  {
    "title": "Page Not Found",
    "url": "/Machine-Learning/404.html",
    "content": "404 ‚Äì Page Not Found Sorry, the page you‚Äôre looking for doesn‚Äôt exist or has been moved. Possible reasons The link is outdated The page name changed There‚Äôs a typo in the URL You can: Go back to the homepage Check out the Projects Browse using the left sidebar navigation Search for a topic (if search is enabled in your theme) If you think this is an error, please open an issue on GitHub."
  },
  
  
  
  {
    "title": "Books & Papers",
    "url": "/Machine-Learningreferences/books.html",
    "content": "Recommended Books and Papers Books Pattern Recognition and Machine Learning by Christopher M. Bishop An Introduction to Statistical Learning by Gareth James et al. Deep Learning by Ian Goodfellow et al. Papers Key papers on machine learning algorithms and techniques. More references to be added."
  },
  
  
  
  {
    "title": "Cost Functions",
    "url": "/Machine-Learningtopics/cost_functions.html",
    "content": "Cost Functions Cost functions (also called loss functions) measure how well our model predictions match the actual values. Key Concepts Mean Squared Error (MSE) Cross-entropy loss Regularization Bias-variance tradeoff Common Cost Functions 1. Mean Squared Error (Regression) \\[J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\\] 2. Logistic Loss (Classification) \\[J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]\\] 3. Regularized Cost Functions L1 Regularization (Lasso): \\(J(\\theta) = \\text{Original Cost} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\\) L2 Regularization (Ridge): \\(J(\\theta) = \\text{Original Cost} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\\) Practical Implementation üìä See this concept in action: Project 1: Simple Linear Regression Project 3: Gradient Descent Project 2: Logistic Regression (Coming Soon) Choosing the Right Cost Function Problem Type Recommended Cost Function Regression Mean Squared Error (MSE) Binary Classification Logistic Loss Multi-class Classification Cross-entropy Loss Sparse Features L1 Regularized Prevent Overfitting L2 Regularized Detailed examples and visualizations to be added based on project implementations."
  },
  
  
  
  {
    "title": "Gradient Descent",
    "url": "/Machine-Learningtopics/gradient_descent.html",
    "content": "Gradient Descent Gradient descent is an optimization algorithm used to minimize cost functions in machine learning. Key Concepts Learning rate (Œ±) Partial derivatives Local vs global minima Convergence criteria Mathematical Foundation Update rule: \\(\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\\) Where: $\\theta_j$ = parameter j $\\alpha$ = learning rate $J(\\theta)$ = cost function Types of Gradient Descent Batch Gradient Descent - Uses entire dataset Stochastic Gradient Descent (SGD) - Uses one sample at a time Mini-batch Gradient Descent - Uses small batches Practical Implementation üìä See this concept in action: Project 1: Simple Linear Regression Project 3: Gradient Descent Project 2: Logistic Regression (Coming Soon) Learning Rate Selection Too small: Slow convergence Too large: May overshoot minimum Just right: Efficient convergence Detailed notes and visualizations to be added based on project implementations."
  },
  
  
  
  {
    "title": "Project: Logistic Regression",
    "url": "/Machine-Learningprojects/logistic-regression.html",
    "content": "Project: Logistic Regression This project applies logistic regression for binary and multiclass classification. Summary Goal: Classify samples into discrete categories Concepts: Sigmoid, decision boundary, one-vs-rest (OvR), softmax (multiclass) Pipeline: Preprocessing ‚Üí Model training ‚Üí Evaluation (accuracy, precision/recall) Notebook Rendered notebook Datasets insurance_data.csv HR_comma_sep.csv Related Topics Logistic Regression Gradient Descent Cost Functions"
  },
  
  
  
  {
    "title": "Project: Support Vector Machine",
    "url": "/Machine-Learningprojects/support-vector-machine.html",
    "content": "Project 10: Support Vector Machine (SVM) This project explores Support Vector Machines for classification tasks, including kernel tricks and hyperparameter tuning. Summary Goal: Classify data using optimal hyperplanes with maximum margin Concepts: Support vectors, kernels (linear, RBF), C parameter, gamma tuning Dataset: Iris dataset for multiclass classification Pipeline: Data visualization ‚Üí SVM training ‚Üí Hyperparameter optimization ‚Üí Evaluation Notebook Rendered notebook Key Concepts Support Vectors The critical data points closest to the decision boundary that define the optimal hyperplane. Kernel Trick Transforms data into higher dimensions to make non-linearly separable data linearly separable: Linear Kernel: For linearly separable data RBF (Radial Basis Function): For non-linear patterns Polynomial Kernel: For polynomial decision boundaries Hyperparameters C (Regularization): Controls the trade-off between maximizing margin and minimizing classification errors High C: Strict (may overfit) Low C: More tolerant (may underfit) Gamma: Defines the influence of single training examples High gamma: Close influence (may overfit) Low gamma: Far-reaching influence (smoother decision boundary) Applications Image classification Text categorization Bioinformatics (protein classification) Handwriting recognition Related Topics Logistic Regression Cost Functions Key Learnings 1. Core Concept of K-Nearest Neighbors KNN is a non-parametric, lazy learning algorithm. Unlike models like Linear or Logistic Regression, it does not explicitly learn a function or fit a line during the training phase. A. Lazy Learning In KNN, all the training data is simply stored. Learning, or computation, is only performed at the time of prediction (when a new data point arrives). This is why it is called a ‚Äúlazy‚Äù algorithm. B. The Mechanism (Voting) The fundamental idea is that similar things exist in close proximity. When a new data point (a question) arrives, the algorithm: Finds its Neighbors: Identifies the $K$ data points in the training set that are numerically closest to the new point. Counts the Votes: For classification, it tallies the class labels (votes) of these $K$ neighbors. Makes the Prediction: The new data point is assigned to the class that represents the majority vote among its $K$ nearest neighbors. [Image of K-Nearest Neighbors classification example] C. The Critical Role of $K$ The letter $K$ in KNN represents the number of neighbors the model will check when making a prediction. Small $K$ (e.g., $K=1$): The model is highly flexible and sensitive to noise or outliers, leading to high variance (potential overfitting). Large $K$: The model is more smoothed out, less sensitive to individual outliers, but might miss important, localized patterns (potential underfitting). The video demonstrates that changing $K$ from 3 to 5 can completely change the prediction result for the same point. 2. Measuring ‚ÄúNearest‚Äù: Distance Calculation The definition of ‚Äúnearest‚Äù is based on the numerical distance between data points in the feature space. The most common metric used is Euclidean Distance. A. Euclidean Distance This is the standard straight-line distance between two points in Euclidean space. For two points $(x_1, y_1)$ and $(x_2, y_2)$, the distance ($d$) is calculated using the Pythagorean theorem: \\[d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\] The model calculates this distance between the new data point and every single point in the training set, sorts these distances, and selects the top $K$ points. B. Importance of Feature Scaling While not explicitly detailed in the video, the use of distance metrics like Euclidean distance makes KNN highly sensitive to the scale of features. If one feature (e.g., salary, measured in hundreds of thousands) is much larger than another (e.g., age, measured in tens), the larger feature will disproportionately influence the distance calculation. In practice, data should be scaled (e.g., using Standardization or Normalization) before applying KNN. 3. Implementation and Evaluation in Scikit-learn The video illustrates the practical use of the KNeighborsClassifier using the famous Iris flower dataset. A. Setup and Splitting Prepare Data: Define $\\mathbf{X}$ (features) and $\\mathbf{Y}$ (target). Split Data: Divide the dataset into training and testing sets using train_test_split (e.g., 80/20 ratio). This is essential for honest model evaluation. B. Model Training and Prediction The KNN model is imported from sklearn.neighbors: from sklearn.neighbors import KNeighborsClassifier # 1. Initialize the Model, specifying K (n_neighbors) model = KNeighborsClassifier(n_neighbors=3) # K=3 is used in the example # 2. Train the Model (Fit) model.fit(X_train, Y_train) # Training is just storing the data # 3. Evaluate the Model score = model.score(X_test, Y_test) # The score is the model's accuracy on the unseen test data. # 4. Make Predictions predictions = model.predict(X_test) C. Choosing the Optimal $K$ The video mentions that selecting the right $K$ value is often done through experimentation. This is typically achieved by: Iterating: Training the model multiple times with different values of $K$ (e.g., $K=1, 3, 5, 7, \\dots$). Plotting: Plotting the resulting accuracy score for each $K$ value. Selecting: Choosing the smallest $K$ value that yields the highest accuracy before the score starts to drop or oscillate (to maintain model simplicity). D. Strengths and Weaknesses Strengths Weaknesses Simple and Intuitive: Easy to understand and explain. Slow Prediction Time: Prediction requires calculating the distance to all training points, making it very slow for large datasets. Non-Parametric: Makes no assumptions about the data distribution. Memory Intensive: Must store the entire training dataset in memory. Effective for Classification: Often yields high accuracy, especially for small, noise-free datasets. Sensitive to Scale: Requires feature scaling to perform reliably."
  },
  
  
  
  {
    "title": "Machine Learning Notes",
    "url": "/Machine-Learningindex.html",
    "content": "Machine Learning Notes Welcome to my personal notes on Machine Learning. This site is built using Jekyll and is organized for clean, minimal reading and easy navigation. Structure The notes are organized as follows: Introduction ‚Äì Course overview and syllabus Lecture Notes ‚Äì Summaries of each lecture Topics ‚Äì Detailed write-ups on algorithms and concepts References ‚Äì Links, books, and papers Quick Links Syllabus Lecture 1: Introduction Linear Regression Projects Overview Updates I‚Äôll be updating these notes regularly as I progress through the course. Disclaimer: These notes are for personal study and reference. For authoritative material, consult textbooks and papers linked in the Foundations and References sections."
  },
  
  
  
  {
    "title": "Project: Random Forest Classifier",
    "url": "/Machine-Learningprojects/random-forest.html",
    "content": "Project 11: Random Forest Classifier This project implements a Random Forest for classification, exploring ensemble methods, feature importance, and model evaluation. Summary Goal: Build robust classifiers using ensembles of decision trees Concepts: Bagging, feature sampling, out-of-bag error, feature importance Pipeline: Preprocess ‚Üí Train ‚Üí Evaluate ‚Üí Interpret Notebook Rendered notebook Exercise Explore the exercises in the project folder if available. Related Topics Decision Trees Cost Functions Key Learnings 1. Understanding Random Forest (The Ensemble Concept) The Random Forest algorithm is essentially a collection of Decision Trees [00:08]. The term ‚ÄúForest‚Äù highlights the use of multiple trees, and ‚ÄúRandom‚Äù refers to the specific random processes used to build them. A. Ensemble Learning Random Forest is a type of Ensemble Learning, which means it relies on the principle of combining multiple simpler models (the individual Decision Trees) to create one powerful model. This approach generally leads to better performance than any single constituent model. B. The Core Idea: Majority Voting The model works through a ‚Äúmajority vote‚Äù system, analogous to asking multiple experts for their opinion [01:31]: Training: The algorithm builds many independent Decision Trees. Prediction: A new data point is fed to every tree in the forest. Aggregation: Classification: The final prediction is the class that receives the majority vote from all the individual trees (e.g., if 7 out of 10 trees predict ‚ÄòYes‚Äô, the final answer is ‚ÄòYes‚Äô). Regression: The final prediction is the average of the outputs from all the individual trees. C. Why It Works: Reducing Variance A single Decision Tree is often prone to overfitting (high variance), meaning it performs perfectly on training data but poorly on unseen data. Random Forest tackles this by: Averaging Out Errors: Since each tree is trained on a different subset of data (or features), their individual errors tend to be uncorrelated. By averaging or voting, these errors cancel each other out, leading to a much more stable and generalized model. 2. The Two Layers of Randomness The ‚ÄúRandom‚Äù part of Random Forest comes from two sources, ensuring that each tree in the ensemble is unique: A. Random Sampling of Data (Bagging) Each tree is trained on a different, randomly selected subset of the training data (often called a bootstrap sample, or Bagging - Bootstrap Aggregating). This means that for a dataset with $N$ samples, each tree is trained on a new dataset of $N$ samples, drawn with replacement from the original data. This ensures the trees are diverse [00:54]. B. Random Subset of Features When deciding on the best split at any node, each tree only considers a random subset of the available features (e.g., if you have 50 features, a tree might only consider 10 of them for a split). This forces the trees to be diverse and prevents any single dominant feature from dictating the structure of every tree [08:10]. 3. Implementation in Scikit-learn (Digits Dataset) The video demonstrates the RandomForestClassifier using the Digits dataset (a multi-class classification problem of handwritten digits 0-9) [02:46]. A. Data Preparation The digits dataset, consisting of 8x8 pixel arrays (64 features) mapping to a target digit (0-9), is loaded. The data is then split into training and testing sets using train_test_split (20% for testing in the example) [05:27]. B. Training the Classifier The RandomForestClassifier is imported from sklearn.ensemble, signifying its status as an ensemble method [07:08]. from sklearn.ensemble import RandomForestClassifier # Initialize the model, specifying the number of trees (n_estimators) model = RandomForestClassifier(n_estimators=10) # 10 trees initially # Train the model model.fit(X_train, Y_train) The parameter n_estimators controls the number of Decision Trees (estimators) in the forest [08:18]. C. Hyperparameter Tuning (n_estimators) The number of trees (n_estimators) is a key hyperparameter to tune. The video shows that increasing this value often increases accuracy, up to a point where the gains become negligible [08:49]. Low n_estimators (e.g., 10): Low accuracy (e.g., 91%). High n_estimators (e.g., 30 or more): Higher, more stable accuracy (e.g., 96-97%). The optimal number of estimators balances accuracy improvements with increased computation time. 4. Model Evaluation: Confusion Matrix While the .score() method gives a single accuracy number, a Confusion Matrix is essential for multi-class problems to understand where the model is making errors [09:47]. A. How a Confusion Matrix Works The matrix plots the Actual Values (Truth) on one axis against the Predicted Values on the other. Diagonal: The numbers along the main diagonal (e.g., 46) show the number of correct predictions for each class (e.g., 46 times the truth was ‚Äò0‚Äô and the model predicted ‚Äò0‚Äô) [11:29]. Off-Diagonal: Any number off the diagonal shows an error (a misclassification). For instance, a ‚Äò2‚Äô in the row for ‚ÄòActual 8‚Äô and column for ‚ÄòPredicted 1‚Äô means the model mistakenly predicted ‚Äò1‚Äô when the true digit was ‚Äò8‚Äô [11:41]. By visualizing the confusion matrix (often using libraries like Seaborn), a data scientist can pinpoint which digits or classes are being confused by the model, allowing for targeted model improvements. http://googleusercontent.com/youtube_content/18"
  },
  
  
  
  {
    "title": "Project 1: Simple Linear Regression",
    "url": "/Machine-Learningprojects/1-linear-regression.html",
    "content": "Project 1: Simple Linear Regression This project implements simple linear regression using a single feature (area) to predict home prices. Files Data: homeprices.csv Notebook: View rendered notebook Summary 1. Core Concept: Simple Linear Regression The entire video revolves around Simple Linear Regression, a basic yet foundational machine learning algorithm. What is Linear Regression? Linear Regression is a supervised learning algorithm used for regression tasks, meaning it predicts a continuous value (like price, age, or temperature) based on input features. Simple means the model uses only one independent variable (feature) to predict the dependent variable (target). In the video, the single independent variable is the home area (in sq. ft.), and the dependent variable is the home price. The goal is to find a linear relationship between the input ($X$) and the output ($Y$). This relationship is visualized as a straight line that best fits the available data points. The Linear Equation (The Math Behind the Model) The fundamental equation of a straight line, which the machine learning model tries to solve, is discussed in the video [02:04]: \\[\\mathbf{y = mx + b}\\] Variable ML Terminology Video Example Description $y$ Dependent Variable (Target) Price The value we are trying to predict. $x$ Independent Variable (Feature) Area The input used to make the prediction. $m$ Slope (or Coefficient) $m$ The gradient of the line. It represents how much $y$ changes for a one-unit change in $x$. $b$ Y-Intercept $b$ The point where the line crosses the y-axis (i.e., the value of $y$ when $x$ is zero). The machine learning process is essentially finding the optimal values for $m$ (the coefficient) and $b$ (the intercept) that minimize the error. 2. Finding the ‚ÄúBest Fit‚Äù Line and Minimizing Error The video explains that many lines can be drawn through the data points, but the model must choose the one that ‚Äúbest fits‚Äù the data [01:05]. The Error Function: Sum of Squared Errors (SSE) The concept used to determine the best line is minimizing the total error. The specific method used here is based on Least Squares [01:18]: Calculate Delta (Error): For every data point, calculate the vertical distance (the $\\Delta$ or delta) between the actual price (the real data point) and the predicted price (the point on the line). Square the Errors: The error for each point is squared ($\\Delta^2$). This is done for two main reasons: It ensures all error values are positive, so positive and negative errors don‚Äôt cancel each other out. It heavily penalizes large errors, forcing the line to stay close to the majority of the data. Sum the Errors: The model sums up all the squared errors (Sum of Squared Errors - SSE) for a given line. Minimize: The line that results in the smallest possible SSE is mathematically chosen as the best-fit line (also known as the Regression Line) [01:45]. The process of training the model is the iterative optimization process of adjusting $m$ and $b$ until the SSE is minimized. 3. Practical Implementation with Python The second half of the video provides a walkthrough of how to implement the Linear Regression model in a Jupyter Notebook using key Python libraries. A. Libraries Used Pandas: Used for reading and manipulating data, specifically for loading the homeprices.csv file into a structured Data Frame [03:10]. Matplotlib: Used for data visualization, specifically for creating a scatter plot of the area vs. price data points. This is an essential step to visually confirm if a linear relationship is appropriate before modeling [04:05]. The model also uses it to plot the final Regression Line alongside the scatter plot to see the fit [12:11]. Scikit-learn (Sklearn): The primary machine learning library. The specific class imported is sklearn.linear_model.LinearRegression [02:49]. This is the class that contains the core algorithm to calculate the optimal $m$ and $b$ values. B. Steps in the Code Step Action in Video Purpose Data Loading pd.read_csv('homeprices.csv') Reads the raw data from a CSV file into a Pandas DataFrame [03:20]. Model Creation reg = linear_model.LinearRegression() Creates an empty instance (object) of the Linear Regression model [05:40]. Model Training reg.fit(df[['area']], df.price) Trains the model. The fit() method uses the Area column ($X$) and the Price column ($Y$) to perform the least squares calculation and find the optimal $m$ and $b$ [05:55]. Prediction reg.predict([[3300]]) Uses the trained model to predict the price for a new, unseen input (e.g., 3,300 sq. ft.) [06:41]. Inspecting Coefficients reg.coef_ and reg.intercept_ Verifies the learned parameters. These functions allow the user to see the value of $m$ (coefficient) and $b$ (intercept) that the model calculated during training [07:18]. C. Batch Prediction and Export The video demonstrates how to use the trained model on an entire list of new areas from a separate file (areas.csv) to predict all the prices at once (batch prediction) [09:11]. A new column named prices is added to the new DataFrame, holding the predicted values. The results are then exported to a new CSV file named prediction.csv using the .to_csv() method, which includes an optional argument index=False to prevent the DataFrame index from being written to the file [10:46]. 4. Exercise and Conclusion The video concludes by assigning an exercise to reinforce the concepts [13:37]: Exercise: Use the learned Simple Linear Regression technique to predict Canada‚Äôs net income per capita in the year 2020 based on historical data provided in a CSV file. Final Summary: The tutorial successfully covered building a simple linear regression model using one independent variable, setting the stage for future discussions on more complex Multiple Linear Regression models that use more than one feature. http://googleusercontent.com/youtube_content/9"
  },
  
  
  
  {
    "title": "Project: Gradient Descent",
    "url": "/Machine-Learningprojects/gradient-descent.html",
    "content": "Project: Gradient Descent (Applied) This project demonstrates gradient descent and its variants (batch, stochastic, mini-batch) on practical problems. Summary Goal: Optimize parameters to minimize a cost function Concepts: Learning rate, convergence, cost surfaces, updates Experiments: Different learning rates, convergence plots Notebook Rendered notebook Dataset test_scores.csv Related Topics Gradient Descent Theory Cost Functions"
  },
  
  
  
  {
    "title": "Project: K-Fold Cross Validation",
    "url": "/Machine-Learningprojects/k-fold-cross-validation.html",
    "content": "Project 12: K-Fold Cross Validation This project demonstrates K-Fold cross validation to assess model generalization and tune hyperparameters. Summary Goal: Estimate model performance reliably across folds Concepts: Stratified K-Fold, shuffle, leakage prevention, scoring metrics Pipeline: Split ‚Üí Train ‚Üí Validate ‚Üí Aggregate Notebook Rendered notebook Exercise Explore the exercises in the project folder if available. Related Topics Train/Test Split Cost Functions Key Learnings 1. The Model Evaluation Dilemma The video starts by framing the core problem: How do you reliably determine which machine learning model (e.g., SVM, Random Forest, Logistic Regression) is best for a given problem? [00:00] A. Problem with Using All Training Data If you train a model on 100% of your data and then test it on the exact same 100% of data, the model‚Äôs score will be artificially high. This is like giving a student the test questions beforehand‚Äîthe score doesn‚Äôt reflect their true knowledge or ability to generalize to new, unseen problems. [01:26] B. Limitations of Simple Train-Test Split (The Flaw) The train-test split method (e.g., 70% train, 30% test) improves on the first method because the test set is unseen. However, it still has a major flaw: the performance score is highly dependent on how the random split happened [03:00]. Scenario: If the 70% training data contains only easy samples (e.g., all algebra problems), and the 30% test data contains only difficult, unseen samples (e.g., all calculus problems), the model will perform poorly, and the score will be an unreliable underestimate of its true capability [03:07]. Proof: The video demonstrates that running train_test_split multiple times causes the scores of the same model (e.g., Logistic Regression) to fluctuate significantly, proving that a single split is not sufficient for robust evaluation [07:05]. 2. K-Fold Cross-Validation (The Solution) K-Fold Cross-Validation addresses the limitations of a single train-test split by ensuring that every sample in the dataset is used for both training and testing exactly once. A. The Mechanism Divide into Folds: The entire dataset is divided into $K$ equally sized, non-overlapping subsets, called folds (the video typically uses $K=5$ or $K=10$) [03:36]. Iterative Training: The model is trained and tested $K$ times (in $K$ iterations). Iteration $i$: In the $i$-th iteration: Test Set: Fold $i$ is reserved as the testing set. Training Set: The remaining $K-1$ folds are combined to form the training set. A performance score is recorded. Final Score: The $K$ individual scores are averaged together to produce the final, robust performance metric [04:14]. B. Stratified K-Fold The video introduces Stratified K-Fold, a superior version of the standard K-Fold [12:25]. Problem: If the target variable is imbalanced (e.g., 90% Class A, 10% Class B), a random split might result in some folds having very few or zero samples of Class B. Solution: Stratified K-Fold ensures that the proportion of target classes is maintained (stratified) within each of the $K$ folds. This is especially important for classification problems to ensure fair testing [12:37]. 3. Practical Implementation in Scikit-learn The video demonstrates two ways to implement cross-validation using scikit-learn. A. Manual K-Fold (for Understanding) The instructor first manually demonstrates the K-Fold process using the KFold or StratifiedKFold class [08:18]. This involves: Initializing StratifiedKFold(n_splits=K). Looping through the folds using skf.split(X, Y). Inside the loop, manually subsetting the data using the train_index and test_index [14:33]. Training the model (.fit()) and recording the score (.score()) for each iteration. Appending the scores to a list (e.g., scores_lr.append(...)) [17:25]. This manual approach is crucial for understanding the algorithm‚Äôs mechanics but is rarely used in production. B. Using cross_val_score (The Real-World Method) For production use, scikit-learn provides the one-line function cross_val_score, which automates the entire K-Fold process [19:27]. from sklearn.model_selection import cross_val_score # Performs 5-fold cross-validation by default (or set cv=K) scores = cross_val_score( estimator=LogisticRegression(), X=digits.data, y=digits.target, cv=10 # Use 10 folds ) # 'scores' is an array of the 10 individual scores. This single line achieves the same result as the lengthy manual loop, providing a fast and robust way to get all $K$ scores. 4. Key Use Cases for Cross-Validation Cross-validation is not just for measuring a model‚Äôs final performance; it is a critical tool in the development process. A. Model Comparison (Algorithm Selection) By running cross_val_score on multiple different algorithms (Logistic Regression, SVM, Random Forest), you can reliably compare their average performance on your dataset and select the best one [21:08]. The model with the highest average cross-validation score is the best choice for your problem. B. Parameter Tuning (Hyperparameter Optimization) Cross-validation is essential for finding the optimal hyperparameters for a single model (e.g., the number of trees n_estimators in a Random Forest) [21:58]. Run cross_val_score with n_estimators=5. Record the average score. Run cross_val_score with n_estimators=50. Record the average score. By comparing the average scores, you can determine which parameter setting yields the most generalized and accurate model. This process demonstrates that machine learning model selection is not a fixed scientific equation but a trial-and-error process guided by rigorous testing [23:45]. http://googleusercontent.com/youtube_content/19"
  },
  
  
  
  
  
  {
    "title": "Project 2: Multiple Linear Regression",
    "url": "/Machine-Learningprojects/2-multiple-linear-regression.html",
    "content": "Project 2: Multiple Linear Regression This project implements multiple linear regression using features like area, bedrooms, and age to predict home prices. Files Data: homeprices.csv Data: hiring.csv Notebook: View rendered notebook Summary 1. Core Concept: Multiple Linear Regression (MLR) Multiple Linear Regression is a supervised learning algorithm used for predicting a continuous target variable ($Y$) using two or more independent variables ($X_1, X_2, \\dots, X_n$). A. The MLR Equation In MLR, the linear relationship between the independent variables (features) and the dependent variable (target) is represented by the following generalized equation [02:10]: \\[\\mathbf{y = m_1x_1 + m_2x_2 + m_3x_3 + \\dots + m_nx_n + b}\\] In the context of the video‚Äôs example: \\[\\mathbf{Price = m_1(\\text{Area}) + m_2(\\text{Bedrooms}) + m_3(\\text{Age}) + b}\\] Component ML Terminology Video Example Description $y$ Dependent Variable (Target) Price The variable being predicted. $x_1, x_2, \\dots$ Features (Independent Variables) Area, Bedrooms, Age The multiple inputs used to make the prediction. $m_1, m_2, \\dots$ Coefficients (Slopes) $m_1, m_2, m_3$ The weight assigned to each feature. They determine the influence of each feature on the final price. $b$ Y-Intercept $b$ The constant term (the price if all feature values were zero). The training process involves the model determining the optimal values for all the coefficients ($m_i$) and the intercept ($b$) that minimize the total error, similar to simple linear regression. B. Why Use Multiple Variables? The video emphasizes that in real life, a dependent variable (like home price) is influenced by multiple factors, not just one (like area) [00:30]. By incorporating more features (like number of bedrooms and age), the model becomes more accurate and reflective of real-world complexity, leading to better predictions. 2. Essential Step: Data Pre-processing (Handling Missing Data) Before training any machine learning model, the data must be cleaned and pre-processed [00:53]. The video addresses a common data issue: missing values (NaN) in the ‚Äòbedrooms‚Äô column [01:02]. Imputation Technique: Using the Median The video uses a technique called imputation to fill the missing data points. Identify the Missing Data: Locate the NaN (Not a Number) value in the ‚Äòbedrooms‚Äô column [03:51]. Calculate the Median: The median is chosen over the mean (average) because it is less affected by extreme outliers in the data. The median is the middle value in a sorted list of numbers. Round Down: Since the number of bedrooms must be a whole number, the median (which might be a float like 3.5) is rounded down using math.floor to an integer (3) [04:45]. Fill the Value: The missing value is replaced using the Pandas fillna() function, resulting in a complete and clean dataset ready for training [05:13]. In-Depth Imputation Knowledge: Choosing the median is a safe and reliable strategy when dealing with numerical, ordinal, or discrete features like the number of bedrooms, as it maintains the central tendency of the data without skewing the distribution. 3. Python Implementation of MLR The tutorial uses the same structure as simple linear regression but adjusts the input data to accommodate multiple features. A. Training the Model Load and Clean Data: The CSV file is loaded into a Pandas DataFrame (df), and the missing ‚Äòbedrooms‚Äô value is imputed [03:30]-[05:47]. Create Model Object: An instance of the LinearRegression class is created [06:23]. Fit the Data (Training): The fit() method is called, but this time, it is supplied with multiple features as the independent variable ($X$): reg.fit(df[['area', 'bedrooms', 'age']], df.price) By passing a list of column names (['area', 'bedrooms', 'age']), a 2D Pandas DataFrame is created with all the required features, satisfying the model‚Äôs input requirements [06:40]. B. Inspecting and Verifying the Model After training, the model stores the calculated parameters: Coefficients ($m_1, m_2, m_3$): Accessed using reg.coef_. This returns an array where each value corresponds to the weight of the respective input feature (Area, Bedrooms, Age) [07:35]. Intercept ($b$): Accessed using reg.intercept_. This is the constant term in the equation [08:00]. The video manually plugs these learned $m$ and $b$ values back into the MLR equation to demonstrate and verify how the model arrives at its prediction, solidifying the mathematical connection between the code and the underlying formula [09:42]. C. Making Predictions The predict() method is called with a new set of values for the three features. The input must be a 2D array or DataFrame with the three columns: # Predicts price for 3000 sq.ft, 3 bedrooms, 40 years old reg.predict([[3000, 3, 40]]) The resulting prediction shows how the combination of lower area and higher age can result in a lower price compared to newer homes, demonstrating the combined influence of all variables [08:57]-[09:15]. 4. Exercise and Further Practice The tutorial concludes with a practical exercise that involves additional real-world data cleaning challenges [11:30]: Challenge 1: Handling Missing Strings: The ‚Äòexperience‚Äô column has missing values as the string ‚Äòzero‚Äô or is blank. Challenge 2: Converting Strings to Numbers: Some number values (like ‚Äòtwo‚Äô years of experience) are in text format and must be converted to numeric format (like 2) using a tool like the word2number module. Challenge 3: Median Imputation: One score is missing and requires median imputation. This exercise forces the user to go beyond simple MLR implementation and practice the full data pre-processing pipeline, which is crucial for building robust machine learning models. http://googleusercontent.com/youtube_content/10"
  },
  
  
  
  {
    "title": "Project: Decision Trees",
    "url": "/Machine-Learningprojects/decision-trees.html",
    "content": "Project: Decision Trees This project builds decision tree classifiers and regressors, exploring splitting criteria and pruning. Summary Goal: Predict classes or continuous values using tree-based models Concepts: Gini, entropy, information gain, pruning, max depth Pipeline: Preprocessing ‚Üí Training ‚Üí Feature importance ‚Üí Evaluation Notebooks Decision Trees Main Decision Trees Exercise Datasets salaries.csv titanic.csv Related Topics Cost Functions Ensemble methods (planned)"
  },
  
  
  
  {
    "title": "Projects",
    "url": "/Machine-Learningprojects/",
    "content": "Machine Learning Projects Hands-on implementations of core ML concepts with clean notebooks, datasets, and concise write-ups. Start with Linear Regression Explore SVM Search Category All Regression Classification Preprocessing Optimization Difficulty All Beginner Intermediate Advanced Status All Complete In Progress Planned Project 1: Simple Linear Regression Regression Predict home prices using a single feature (area). Concepts: Linear Regression, Gradient Descent, Cost Functions Project page ¬∑ Notebook ¬∑ Dataset Project 2: Multiple Linear Regression Regression Predict home prices using area, bedrooms, and age. Concepts: Multiple Linear Regression, Data Cleaning, Gradient Descent Project page ¬∑ Notebook ¬∑ Dataset ¬∑ Dataset Project 3: Gradient Descent Optimization Optimizing models with gradient descent variants. Project page ¬∑ Notebook ¬∑ Dataset Project 4: Save &amp; Load Models Utility Persist trained models with joblib/pickle. Project page ¬∑ Notebook Project 5: Dummy Variables &amp; One-Hot Preprocessing Encode categorical features for ML models. Project page ¬∑ Notebook ¬∑ Dataset ¬∑ Dataset Project 6: Train/Test Split Evaluation Proper dataset splitting and evaluation. Project page ¬∑ Notebook coming soon Project 7: Logistic Regression Classification Binary classification with logistic regression. Project page ¬∑ Notebook ¬∑ Dataset ¬∑ Dataset Project 8: Multiclass Logistic Regression Classification Softmax regression and OvR strategies. Project page ¬∑ Notebook Project 9: Decision Trees Classification Tree-based models for classification/regression. Project page ¬∑ Main notebook ¬∑ Exercise ¬∑ Dataset ¬∑ Dataset Project 10: Support Vector Machine Classification Classification with optimal hyperplanes and kernel tricks. Project page ¬∑ Notebook Project 11: Random Forest Classifier Classification Ensemble learning with bagging and feature sampling. Project page ¬∑ Notebook Project 12: K-Fold Cross Validation Evaluation Robust performance estimation across folds. Project page ¬∑ Notebook Want to contribute? Open an issue or PR with a new dataset, notebook, or improvement. View on GitHub Project 2: Multiple Linear Regression Status: In Progress Concepts Used: Multiple Linear Regression, Data Cleaning, Gradient Descent Description: Predicts home prices using area, bedrooms, and age. Includes data preprocessing and model evaluation. Project: Classification with Logistic Regression Status: Planned Concepts Used: Logistic Regression, Sigmoid Function, Classification Description: Binary and multiclass classification using logistic regression. Project Structure Each project includes: üìù Problem Statement - What we‚Äôre trying to solve üîß Implementation - Step-by-step code with explanations üìä Results &amp; Analysis - Visualizations and performance metrics üîó Concept Links - Connections to theory pages üí° Key Learnings - Insights and takeaways Quick Links to Related Concepts Concept Theory Page Used in Projects Linear Regression üìñ Theory Project 1 Logistic Regression üìñ Theory Project 2 Gradient Descent üìñ Theory Projects 1, 2, 3 Cost Functions üìñ Theory Projects 1, 2, 3 üöÄ Getting Started Choose a project that matches your current learning level Review the related concept pages first Follow the implementation step-by-step Experiment with the code and parameters Document your findings and insights New projects are added regularly as we progress through the course!"
  },
  
  
  
  {
    "title": "Project: Train/Test Split",
    "url": "/Machine-Learningprojects/train-test-split.html",
    "content": "Project: Train/Test Split This project demonstrates splitting datasets into train/test sets (and validation) with scikit-learn. Summary Goal: Evaluate generalization by splitting data correctly Concepts: Random state, stratification, data leakage Steps: Split ‚Üí Train ‚Üí Validate ‚Üí Test Notebook Rendered notebook: Will appear here once the .ipynb is added. Related Topics Cost Functions"
  },
  
  
  
  {
    "title": "Project: Save and Load Models",
    "url": "/Machine-Learningprojects/save-model.html",
    "content": "Project: Save and Load Models This project demonstrates persisting trained models to disk and loading them for inference. Summary Goal: Persist ML models reliably (pickle/joblib) Concepts: Serialization, versioning, reproducibility Steps: Train ‚Üí Save ‚Üí Load ‚Üí Predict Notebook Rendered notebook Related Topics Linear Regression Logistic Regression"
  },
  
  
  
  {
    "title": "Project: Multiclass Logistic Regression",
    "url": "/Machine-Learningprojects/multiclass-logistic-regression.html",
    "content": "Project: Multiclass Logistic Regression This project implements multiclass classification using softmax regression and one-vs-rest strategies. Summary Goal: Classify into more than two classes Concepts: Softmax, cross-entropy, OvR vs multinomial Steps: Preprocess ‚Üí Train ‚Üí Evaluate (confusion matrix) Notebook Rendered notebook Related Topics Logistic Regression"
  },
  
  
  
  {
    "title": "Project: Dummy Variables & One-Hot Encoding",
    "url": "/Machine-Learningprojects/encoding.html",
    "content": "Project: Dummy Variables &amp; One-Hot Encoding This project covers handling categorical data using dummy variables and one-hot encoding. Summary Goal: Encode categorical features for ML models Concepts: One-hot, drop-first, multicollinearity Steps: Explore categories ‚Üí Encode ‚Üí Train ‚Üí Evaluate Notebook Rendered notebook Datasets homeprices.csv carprices.csv Related Topics Multiple Linear Regression"
  },
  
  
  
  {
    "title": "Lecture 1 - Introduction",
    "url": "/Machine-Learninglectures/lecture1.html",
    "content": "Date Fall 2025 (following Stanford‚Äôs CS229 curriculum) Summary This lecture introduced: Course structure Types of learning (supervised, unsupervised, reinforcement) Notation used in the course Reference: CS229 Official Notes"
  },
  
  
  
  {
    "title": "Lecture 2 - Supervised Learning",
    "url": "/Machine-Learninglectures/lecture2.html",
    "content": "Date Fall 2025 (following Stanford‚Äôs CS229 curriculum) Summary This lecture will cover: Supervised learning fundamentals Linear regression Gradient descent Normal equation Reference: CS229 Official Notes Content to be added as lecture progresses."
  },
  
  
  
  {
    "title": "Linear Regression",
    "url": "/Machine-Learningtopics/linear_regression.html",
    "content": "Linear Regression This section covers Linear Regression in Machine Learning. Key Concepts Hypothesis function Cost function Gradient descent Normal equation Mathematical Foundation The hypothesis function for linear regression: \\(h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n\\) Cost function (Mean Squared Error): \\(J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\\) Gradient descent update rule: \\(\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\\) Practical Implementation üìä See this concept in action: Project 1: Simple Linear Regression and Project 2: Multiple Linear Regression Applications Predicting house prices Sales forecasting Risk assessment Detailed notes to be added based on project learnings."
  },
  
  
  
  {
    "title": "Logistic Regression",
    "url": "/Machine-Learningtopics/logistic_regression.html",
    "content": "Logistic Regression This section covers Logistic Regression for classification tasks. Key Concepts Sigmoid function Cost function for logistic regression Gradient descent Multiclass classification Mathematical Foundation Sigmoid function: \\(g(z) = \\frac{1}{1 + e^{-z}}\\) Hypothesis function: \\(h_\\theta(x) = g(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}\\) Cost function: \\(J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)} \\log(h_\\theta(x^{(i)})) - (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]\\) Practical Implementation üìä See this concept in action: Project 7: Logistic Regression and Project 8: Multiclass Logistic Regression Applications Email spam detection Medical diagnosis Marketing response prediction Detailed notes to be added based on project learnings."
  },
  
  
  
  {
    "title": "Decision Trees - Notebook",
    "url": "/Machine-Learningprojects/decision-trees/notebook",
    "content": "Decision Trees - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session /kaggle/input/salaries/salaries.csv import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(\"/kaggle/input/salaries/salaries.csv\") df.head() company job degree salary_more_then_100k 0 google sales executive bachelors 0 1 google sales executive masters 0 2 google business manager bachelors 1 3 google business manager masters 1 4 google computer programmer bachelors 0 df.shape (16, 4) df.dtypes company object job object degree object salary_more_then_100k int64 dtype: object DATA PROCESSING X = df.drop(columns = ['salary_more_then_100k']) # this has categorical columns y = df['salary_more_then_100k'] X.shape, y.shape ((16, 3), (16,)) X.head() company job degree 0 google sales executive bachelors 1 google sales executive masters 2 google business manager bachelors 3 google business manager masters 4 google computer programmer bachelors from sklearn.preprocessing import LabelEncoder le = LabelEncoder() X['le_company'] = le.fit_transform(X.company) X.le_company.sample(6) 9 0 0 2 14 1 3 2 13 1 10 1 Name: le_company, dtype: int64 X['le_job'] = le.fit_transform(X.job) X.le_job.sample(6) 1 2 4 1 0 2 13 0 5 1 7 1 Name: le_job, dtype: int64 X['le_degree'] = le.fit_transform(X.degree) X.le_degree.sample(6) 14 0 12 0 6 1 2 0 10 0 3 1 Name: le_degree, dtype: int64 X.head() company job degree le_company le_job le_degree 0 google sales executive bachelors 2 2 0 1 google sales executive masters 2 2 1 2 google business manager bachelors 2 0 0 3 google business manager masters 2 0 1 4 google computer programmer bachelors 2 1 0 X = X.drop(columns = ['company','job','degree']) X.head() # Completely encoded columns le_company le_job le_degree 0 2 2 0 1 2 2 1 2 2 0 0 3 2 0 1 4 2 1 0 X.dtypes le_company int64 le_job int64 le_degree int64 dtype: object from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier() model.fit(X_train, y_train) DecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier() model.score(X_test, y_test) 0.75 y_pred = model.predict(X_test) from sklearn.metrics import confusion_matrix import seaborn as sns cm = confusion_matrix(y_pred, y_test) sns.heatmap(cm, cmap = 'Blues', annot = True) &lt;Axes: &gt;"
  },
  
  
  
  {
    "title": "Decision Trees Exercise - Notebook",
    "url": "/Machine-Learningprojects/decision-trees/exercise/notebook",
    "content": "Decision Trees Exercise - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session /kaggle/input/titanic/titanic.csv import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(\"/kaggle/input/titanic/titanic.csv\") df.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S df.drop(columns = ['PassengerId','Name', 'SibSp','Parch','Ticket','Cabin','Embarked'], inplace = True) df.head() Survived Pclass Sex Age Fare 0 0 3 male 22.0 7.2500 1 1 1 female 38.0 71.2833 2 1 3 female 26.0 7.9250 3 1 1 female 35.0 53.1000 4 0 3 male 35.0 8.0500 df.dtypes # Sex column is categorical Survived int64 Pclass int64 Sex object Age float64 Fare float64 dtype: object sex = {'female':0, 'male':1} df['Sex_Encoded'] = df['Sex'].map(sex) df.drop(columns = ['Sex'], inplace = True) df.sample(10) Survived Pclass Age Fare Sex_Encoded 438 0 1 64.0 263.0000 1 288 1 2 42.0 13.0000 1 336 0 1 29.0 66.6000 1 242 0 2 29.0 10.5000 1 394 1 3 24.0 16.7000 0 884 0 3 25.0 7.0500 1 744 1 3 31.0 7.9250 1 226 1 2 19.0 10.5000 1 34 0 1 28.0 82.1708 1 329 1 1 16.0 57.9792 0 df.dtypes # Now all the columns are in correct type Survived int64 Pclass int64 Age float64 Fare float64 Sex_Encoded int64 dtype: object df.isnull().sum() Survived 0 Pclass 0 Age 177 Fare 0 Sex_Encoded 0 dtype: int64 import math mean = math.floor(df.Age.mean()) df.Age.fillna(mean, inplace = True) df.isnull().sum() Survived 0 Pclass 0 Age 0 Fare 0 Sex_Encoded 0 dtype: int64 X = df.drop(columns = ['Survived']) y = df['Survived'] X.shape, y.shape ((891, 4), (891,)) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape, X_test.shape, y_train.shape, y_test.shape ((712, 4), (179, 4), (712,), (179,)) from sklearn.tree import DecisionTreeClassifier model = DecisionTreeClassifier() model.fit(X_train, y_train) DecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier() y_pred = model.predict(X_test) model.score(X_test, y_test) 0.776536312849162 from sklearn.metrics import confusion_matrix, classification_report cr = classification_report(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(cr) precision recall f1-score support 0 0.80 0.82 0.81 105 1 0.74 0.72 0.73 74 accuracy 0.78 179 macro avg 0.77 0.77 0.77 179 weighted avg 0.78 0.78 0.78 179 import seaborn as sns sns.heatmap(cm, cmap='Blues', annot=True) &lt;Axes: &gt;"
  },
  
  
  
  {
    "title": "Multiclass Logistic Regression - Notebook",
    "url": "/Machine-Learningprojects/multiclass-logistic-regression/notebook",
    "content": "Multiclass Logistic Regression - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_digits dir(digits) ['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names'] digits.data[0] array([ 0., 0., 5., 13., 9., 1., 0., 0., 0., 0., 13., 15., 10., 15., 5., 0., 0., 3., 15., 2., 0., 11., 8., 0., 0., 4., 12., 0., 0., 8., 8., 0., 0., 5., 8., 0., 0., 9., 8., 0., 0., 4., 11., 0., 1., 12., 7., 0., 0., 2., 14., 5., 10., 12., 0., 0., 0., 0., 6., 13., 10., 0., 0., 0.]) digits = load_digits() plt.gray() for i in range(5): plt.matshow(digits.images[i]) plt.axis('off') &lt;Figure size 640x480 with 0 Axes&gt; digits.target[0:5] array([0, 1, 2, 3, 4]) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = 0.2, random_state = 42) X_train.shape, X_test.shape, y_train.shape, y_test.shape ((1437, 64), (360, 64), (1437,), (360,)) Create and train logistic regression model from sklearn.linear_model import LogisticRegression model = LogisticRegression() model.fit(X_train, y_train) /usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression() model.score(X_test, y_test) 0.9694444444444444 y_predicted = model.predict(X_test) plt.matshow(digits.images[67]) digits.target[67] 6 model.predict(digits.data[[67]]) array([6]) from sklearn.metrics import confusion_matrix import seaborn as sns cm = confusion_matrix(y_test, y_predicted) sns.heatmap(cm, cmap = 'Blues', annot=True) plt.xlabel('Predicted') plt.ylabel('Truth') Text(50.722222222222214, 0.5, 'Truth') plt.figure(figsize = (10,7)) sns.heatmap(cm, annot=True) plt.xlabel('Predicted') plt.ylabel('Truth') Text(95.72222222222221, 0.5, 'Truth') EXERCISE from sklearn.datasets import load_iris iris = load_iris() dir(iris) ['DESCR', 'data', 'data_module', 'feature_names', 'filename', 'frame', 'target', 'target_names'] iris.data[0:5] array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]]) len(iris.target) 150 len(iris.data) # So the number of rows is 150 150 X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state = 42)\\ X_train.shape, X_test.shape, y_train.shape, y_test.shape ((120, 4), (30, 4), (120,), (30,)) model.fit(X_train, y_train) /usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( LogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression() y_pred = model.predict(X_test) y_pred[0:5], y_test[0:5] # As you can see the predicted and actual output is almost same (array([1, 0, 2, 1, 1]), array([1, 0, 2, 1, 1])) # lets check how much data predicted is correct print(f\"{model.score(X_test, y_test) * 100}%\") 100.0% cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot = True) &lt;Axes: &gt;"
  },
  
  
  
  {
    "title": "Dummy Variables & One-Hot Encoding - Notebook",
    "url": "/Machine-Learningprojects/encoding/notebook",
    "content": "Dummy Variables &amp; One-Hot Encoding - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session /kaggle/input/carprices/carprices.csv /kaggle/input/encoding/homeprices (2).csv import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.read_csv(\"/kaggle/input/encoding/homeprices (2).csv\") df.head() town area price 0 monroe township 2600 550000 1 monroe township 3000 565000 2 monroe township 3200 610000 3 monroe township 3600 680000 4 monroe township 4000 725000 df.shape (13, 3) df.town.value_counts() town monroe township 5 west windsor 4 robinsville 4 Name: count, dtype: int64 GET DUMMIES METHOD FROM PANDAS dummies = pd.get_dummies(df.town).astype(int) dummies.head(10) monroe township robinsville west windsor 0 1 0 0 1 1 0 0 2 1 0 0 3 1 0 0 4 1 0 0 5 0 0 1 6 0 0 1 7 0 0 1 8 0 0 1 9 0 1 0 merged = pd.concat([ df, dummies], axis = 'columns') merged.head() town area price monroe township robinsville west windsor 0 monroe township 2600 550000 1 0 0 1 monroe township 3000 565000 1 0 0 2 monroe township 3200 610000 1 0 0 3 monroe township 3600 680000 1 0 0 4 monroe township 4000 725000 1 0 0 # Lets drop town column final = merged.drop(['town', 'west windsor'], axis = 1) final area price monroe township robinsville 0 2600 550000 1 0 1 3000 565000 1 0 2 3200 610000 1 0 3 3600 680000 1 0 4 4000 725000 1 0 5 2600 585000 0 0 6 2800 615000 0 0 7 3300 650000 0 0 8 3600 710000 0 0 9 2600 575000 0 1 10 2900 600000 0 1 11 3100 620000 0 1 12 3600 695000 0 1 plt.plot(final) [&lt;matplotlib.lines.Line2D at 0x7d972e82fb10&gt;, &lt;matplotlib.lines.Line2D at 0x7d972e6dae90&gt;, &lt;matplotlib.lines.Line2D at 0x7d972e6db190&gt;, &lt;matplotlib.lines.Line2D at 0x7d972e6db510&gt;] from sklearn import linear_model model = linear_model.LinearRegression() X = final.drop(columns = ['price']) y = final['price'] X.shape, y.shape ((13, 3), (13,)) model.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() model.coef_ array([ 126.89744141, -40013.97548914, -14327.56396474]) model.intercept_ 249790.36766292527 import pickle with open('model_pickle', 'wb') as f: pickle.dump(model, f) with open('model_pickle', 'rb') as f: reg = pickle.load(f) reg.coef_ array([ 126.89744141, -40013.97548914, -14327.56396474]) reg.predict([[2800, 0, 1]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([590775.63964739]) # to calculate the model reg.score(X, y) * 100 95.73929037221873 ONE HOT ENCODING METHOD FROM SKLEARN from sklearn.preprocessing import LabelEncoder le = LabelEncoder() df_le = df df_le town area price 0 monroe township 2600 550000 1 monroe township 3000 565000 2 monroe township 3200 610000 3 monroe township 3600 680000 4 monroe township 4000 725000 5 west windsor 2600 585000 6 west windsor 2800 615000 7 west windsor 3300 650000 8 west windsor 3600 710000 9 robinsville 2600 575000 10 robinsville 2900 600000 11 robinsville 3100 620000 12 robinsville 3600 695000 df_le.town = le.fit_transform(df_le.town) df_le.town 0 0 1 0 2 0 3 0 4 0 5 2 6 2 7 2 8 2 9 1 10 1 11 1 12 1 Name: town, dtype: int64 df_le town area price 0 0 2600 550000 1 0 3000 565000 2 0 3200 610000 3 0 3600 680000 4 0 4000 725000 5 2 2600 585000 6 2 2800 615000 7 2 3300 650000 8 2 3600 710000 9 1 2600 575000 10 1 2900 600000 11 1 3100 620000 12 1 3600 695000 X = df_le.drop(columns = ['price']) y = df_le['price'] X.shape, y.shape ((13, 2), (13,)) reg.coef_ array([ 126.89744141, -40013.97548914, -14327.56396474]) model.coef_ array([ 126.89744141, -40013.97548914, -14327.56396474]) model.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() model.coef_ array([20112.74367181, 126.05469887]) model.predict([[2, 3300]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([670283.67762584]) X = df_le[['town', 'area']].values X array([[ 0, 2600], [ 0, 3000], [ 0, 3200], [ 0, 3600], [ 0, 4000], [ 2, 2600], [ 2, 2800], [ 2, 3300], [ 2, 3600], [ 1, 2600], [ 1, 2900], [ 1, 3100], [ 1, 3600]]) y = df_le.price from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder() X = ohe.fit_transform(X[:, 0:1]).toarray() X array([[1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 1., 0.], [0., 1., 0.], [0., 1., 0.], [0., 1., 0.]]) X = X[:, 1:] X array([[0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 1.], [0., 1.], [0., 1.], [0., 1.], [1., 0.], [1., 0.], [1., 0.], [1., 0.]]) model.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() model.coef_ array([-3500., 14000.]) EXERCISE df = pd.read_csv(\"/kaggle/input/carprices/carprices.csv\") df.head() Car Model Mileage Sell Price($) Age(yrs) 0 BMW X5 69000 18000 6 1 BMW X5 35000 34000 3 2 BMW X5 57000 26100 5 3 BMW X5 22500 40000 2 4 BMW X5 46000 31500 4 df['Car Model'].value_counts() Car Model BMW X5 5 Audi A5 4 Mercedez Benz C class 4 Name: count, dtype: int64 GET DUMMIES FUNCTION dummies = pd.get_dummies(df['Car Model']).astype(int) dummies Audi A5 BMW X5 Mercedez Benz C class 0 0 1 0 1 0 1 0 2 0 1 0 3 0 1 0 4 0 1 0 5 1 0 0 6 1 0 0 7 1 0 0 8 1 0 0 9 0 0 1 10 0 0 1 11 0 0 1 12 0 0 1 df = df.drop(columns = ['Car Model']) final = pd.concat([df, dummies], axis = 1) final.head(4) Mileage Sell Price($) Age(yrs) Audi A5 BMW X5 Mercedez Benz C class 0 69000 18000 6 0 1 0 1 35000 34000 3 0 1 0 2 57000 26100 5 0 1 0 3 22500 40000 2 0 1 0 X = final.drop(columns = ['Sell Price($)']) y = final['Sell Price($)'] X.shape, y.shape ((13, 5), (13,)) y 0 18000 1 34000 2 26100 3 40000 4 31500 5 29400 6 32000 7 19300 8 12000 9 22000 10 20000 11 21000 12 33000 Name: Sell Price($), dtype: int64 X Mileage Age(yrs) Audi A5 BMW X5 Mercedez Benz C class 0 69000 6 0 1 0 1 35000 3 0 1 0 2 57000 5 0 1 0 3 22500 2 0 1 0 4 46000 4 0 1 0 5 59000 5 1 0 0 6 52000 5 1 0 0 7 72000 6 1 0 0 8 91000 8 1 0 0 9 67000 6 0 0 1 10 83000 7 0 0 1 11 79000 7 0 0 1 12 59000 5 0 0 1 from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() model.coef_, model.intercept_ (array([-3.70122094e-01, -1.33245363e+03, 6.10375284e+02, -3.67429130e+03, 3.06391602e+03]), 55912.70994756205) model.predict([[6900, 6, 0,1 ,0]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([41689.85442592]) model.predict([[7900, 7, 0, 0 ,1]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([46725.48602958]) model.score(X, y) 0.9417050937281082 import pickle with open('model_name_for_pickle', 'wb') as f: pickle.dump(model, f) with open('model_name_for_pickle', 'rb') as f: reg = pickle.load(f) ONE-HOT ENCODING from sklearn.preprocessing import LabelEncoder le = LabelEncoder() df = pd.read_csv(\"/kaggle/input/carprices/carprices.csv\") df.head() Car Model Mileage Sell Price($) Age(yrs) 0 BMW X5 69000 18000 6 1 BMW X5 35000 34000 3 2 BMW X5 57000 26100 5 3 BMW X5 22500 40000 2 4 BMW X5 46000 31500 4 df['Car Model'] = le.fit_transform(df['Car Model']) df['Car Model'] 0 1 1 1 2 1 3 1 4 1 5 0 6 0 7 0 8 0 9 2 10 2 11 2 12 2 Name: Car Model, dtype: int64 df Car Model Mileage Sell Price($) Age(yrs) 0 1 69000 18000 6 1 1 35000 34000 3 2 1 57000 26100 5 3 1 22500 40000 2 4 1 46000 31500 4 5 0 59000 29400 5 6 0 52000 32000 5 7 0 72000 19300 6 8 0 91000 12000 8 9 2 67000 22000 6 10 2 83000 20000 7 11 2 79000 21000 7 12 2 59000 33000 5 X = df[['Car Model','Mileage', 'Age(yrs)']].values X array([[ 1, 69000, 6], [ 1, 35000, 3], [ 1, 57000, 5], [ 1, 22500, 2], [ 1, 46000, 4], [ 0, 59000, 5], [ 0, 52000, 5], [ 0, 72000, 6], [ 0, 91000, 8], [ 2, 67000, 6], [ 2, 83000, 7], [ 2, 79000, 7], [ 2, 59000, 5]]) y = df['Sell Price($)'] y 0 18000 1 34000 2 26100 3 40000 4 31500 5 29400 6 32000 7 19300 8 12000 9 22000 10 20000 11 21000 12 33000 Name: Sell Price($), dtype: int64 from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder() X = ohe.fit_transform(X) X &lt;Compressed Sparse Row sparse matrix of dtype 'float64' with 39 stored elements and shape (13, 22)&gt; model.coef_ array([-3.70122094e-01, -1.33245363e+03, 6.10375284e+02, -3.67429130e+03, 3.06391602e+03]) model.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() model.score(X, y) 1.0"
  },
  
  
  
  {
    "title": "Save & Load Models - Notebook",
    "url": "/Machine-Learningprojects/save-model/notebook",
    "content": "Save &amp; Load Models - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import pandas as pd import numpy as np import math from sklearn import linear_model df = pd.read_csv(\"/kaggle/input/homeprices-csv/homeprices.csv\") df.head() area price 0 2600 550000 1 3000 565000 2 3200 610000 3 3600 680000 4 4000 725000 reg = linear_model.LinearRegression() reg.fit(df[['area']], df.price) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() reg.coef_ array([135.78767123]) reg.intercept_ 180616.43835616432 SAVE THE MODEL USING PICKLE import pickle # save the python object with open('model_pickle', 'wb') as f: pickle.dump(reg, f) with open('model_pickle', 'rb') as f: model = pickle.load(f) model.coef_ array([135.78767123]) model.intercept_ 180616.43835616432 model.predict(np.array([5000]).reshape(1, -1)) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([859554.79452055]) SAVE THE MODEL USING SKLEARN‚ÄôS JOBLIB if the model consist of large numpy array, this is the go to option # from sklearn.externals import joblib import joblib joblib.dump(model, 'model_jbolib') ['model_jbolib'] mj = joblib.load('model_jbolib') mj.coef_ # Getting same coefficient array([135.78767123])"
  },
  
  
  
  {
    "title": "Gradient Descent - Notebook",
    "url": "/Machine-Learningprojects/gradient-descent/notebook",
    "content": "Gradient Descent - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session GRADIENT DECSENT ALGORITHM import pandas as pd import numpy as np import matplotlib.pyplot as plt def gradient_descent(x, y): m_curr = b_curr = 0 iterations = 1000 n = len(x) learning_rate = 0.02 for i in range(iterations): y_predicted = m_curr * x + b_curr cost = (1/n) * sum([value**2 for value in (y - y_predicted)]) md = -(2/n)*sum(x*(y-y_predicted)) bd = -(2/n)*sum(y-y_predicted) m_curr = m_curr - learning_rate * md b_curr = b_curr - learning_rate * bd print(f\"m {m_curr}, b {b_curr},cost {cost}, iterations {i} \") x = np.array([1, 2, 3, 4 , 5]) y = np.array([5, 7, 9, 11, 13]) gradient_descent(x, y) m 1.24, b 0.36,cost 89.0, iterations 0 m 1.8912, b 0.5568,cost 25.3616, iterations 1 m 2.232256, b 0.667584,cost 7.694359040000001, iterations 2 m 2.40995328, b 0.73300992,cost 2.7832300789760005, iterations 3 m 2.5016126464, b 0.7744951296,cost 1.4117625182879754, iterations 4 m 2.547963666432, b 0.803321806848,cost 1.0225913197820489, iterations 5 m 2.57046103638016, b 0.82543329460224,cost 0.9061020411503913, iterations 6 m 2.580406185020621, b 0.8439606384525312,cost 0.8653906318976792, iterations 7 m 2.583752186997244, b 0.8605534707119554,cost 0.8458189807137511, iterations 8 m 2.583634808233022, b 0.8760810694438079,cost 0.8322246831340234, iterations 9 m 2.581705764277235, b 0.891001649678093,cost 0.8203990003422286, iterations 10 m 2.5788350300338805, b 0.9055568919777011,cost 0.8091726163869117, iterations 11 m 2.5754807897816487, b 0.9198744126945274,cost 0.798219608661616, iterations 12 m 2.57188431275438, b 0.9340217414129485,cost 0.7874480974823337, iterations 13 m 2.568172606172899, b 0.9480347542259049,cost 0.7768311644091502, iterations 14 m 2.564412488949715, b 0.9619326513161208,cost 0.7663599355170038, iterations 15 m 2.560639075653906, b 0.9757258465895102,cost 0.7560305627709811, iterations 16 m 2.556870780775446, b 0.9894201236474611,cost 0.7458406113875823, iterations 17 m 2.5531172223965544, b 1.0030188250085093,cost 0.7357880571547657, iterations 18 m 2.5493833855410495, b 1.0165240053205824,cost 0.7258710079618633, iterations 19 m 2.5456718152645177, b 1.0299370388428333,cost 0.7160876262852294, iterations 20 m 2.54198377188699, b 1.043258939457378,cost 0.7064361074380244, iterations 21 m 2.538319839521829, b 1.056490529252644,cost 0.6969146732965662, iterations 22 m 2.534680246621907, b 1.0696325273399188,cost 0.6875215703243238, iterations 23 m 2.5310650348274777, b 1.082685596651693,cost 0.678255068791568, iterations 24 m 2.5274741479051843, b 1.095650368606328,cost 0.669113462329926, iterations 25 m 2.5239074785941438, b 1.1085274561134526,cost 0.6600950675829437, iterations 26 m 2.520364893279106, b 1.1213174604376173,cost 0.6511982238863403, iterations 27 m 2.5168462449837854, b 1.13402097482662,cost 0.6424212929594983, iterations 28 m 2.5133513802117253, b 1.1466385864355009,cost 0.6337626586030327, iterations 29 m 2.509880142546306, b 1.1591708773526739,cost 0.6252207264009763, iterations 30 m 2.5064323745436106, b 1.1716184251530102,cost 0.6167939234271189, iterations 31 m 2.5030079187260608, b 1.1839818032016565,cost 0.6084806979553533, iterations 32 m 2.4996066181023955, b 1.196261580826463,cost 0.6002795191739324, iterations 33 m 2.496228316438166, b 1.208458323421117,cost 0.5921888769035806, iterations 34 m 2.4928728583948385, b 1.2205725925116924,cost 0.5842072813194079, iterations 35 m 2.4895400895997066, b 1.2326049458038442,cost 0.5763332626765676, iterations 36 m 2.4862298566793744, b 1.2445559372197257,cost 0.5685653710396228, iterations 37 m 2.4829420072740827, b 1.2564261169294118,cost 0.5609021760155413, iterations 38 m 2.479676390041957, b 1.2682160313793454,cost 0.553342266490311, iterations 39 m 2.4764328546579746, b 1.2799262233191366,cost 0.5458842503690889, iterations 40 m 2.473211251810169, b 1.2915572318274142,cost 0.5385267543198599, iterations 41 m 2.4700114331944047, b 1.3031095923370972,cost 0.531268423520549, iterations 42 m 2.466833251508415, b 1.3145838366602847,cost 0.524107921409543, iterations 43 m 2.463676560445478, b 1.3259804930128636,cost 0.5170439294395728, iterations 44 m 2.460541214687924, b 1.3373000860388917,cost 0.5100751468349128, iterations 45 m 2.4574270699005702, b 1.348543136834785,cost 0.5032002903518572, iterations 46 m 2.4543339827241453, b 1.3597101629733253,cost 0.49641809404241655, iterations 47 m 2.4512618107687225, b 1.3708016785274948,cost 0.4897273090212088, iterations 48 m 2.4482104126071853, b 1.3818181940941483,cost 0.483126703235482, iterations 49 m 2.445179647768726, b 1.3927602168175202,cost 0.4766150612382476, iterations 50 m 2.442169376732384, b 1.4036282504125723,cost 0.4701911839644611, iterations 51 m 2.4391794609206263, b 1.4144227951881834,cost 0.4638538885102291, iterations 52 m 2.4362097626929686, b 1.4251443480701809,cost 0.45760200791498234, iterations 53 m 2.433260145339641, b 1.4357934026242174,cost 0.45143439094659055, iterations 54 m 2.430330473075293, b 1.4463704490784919,cost 0.4453499018893776, iterations 55 m 2.427420611032745, b 1.456875974346317,cost 0.4393474203349813, iterations 56 m 2.424530425256779, b 1.467310462048535,cost 0.43342584097604553, iterations 57 m 2.4216597826979718, b 1.4776743925357803,cost 0.42758407340268434, iterations 58 m 2.4188085512065705, b 1.4879682429105925,cost 0.4218210419016897, iterations 59 m 2.415976599526408, b 1.4981924870493804,cost 0.41613568525845435, iterations 60 m 2.413163797288863, b 1.5083475956242363,cost 0.4105269565615509, iterations 61 m 2.4103700150068548, b 1.5184340361246031,cost 0.40499382300995795, iterations 62 m 2.4075951240688864, b 1.5284522728787964,cost 0.399535265722871, iterations 63 m 2.4048389967331207, b 1.5384027670753782,cost 0.39415027955209186, iterations 64 m 2.402101506121502, b 1.5482859767843886,cost 0.388837872896933, iterations 65 m 2.3993825262139143, b 1.558102356978433,cost 0.3835970675216255, iterations 66 m 2.39668193184238, b 1.5678523595536258,cost 0.3784268983751844, iterations 67 m 2.3939995986852978, b 1.5775364333503952,cost 0.3733264134137022, iterations 68 m 2.3913354032617193, b 1.5871550241741437,cost 0.368294673425038, iterations 69 m 2.3886892229256658, b 1.5967085748157717,cost 0.3633307518558684, iterations 70 m 2.3860609358604803, b 1.6061975250720608,cost 0.3584337346410733, iterations 71 m 2.3834504210732215, b 1.6156223117659207,cost 0.35360272003541443, iterations 72 m 2.3808575583890934, b 1.6249833687664974,cost 0.34883681844748926, iterations 73 m 2.378282228445913, b 1.6342811270091464,cost 0.34413515227591934, iterations 74 m 2.3757243126886136, b 1.643516014515271,cost 0.33949685574774674, iterations 75 m 2.373183693363791, b 1.6526884564120266,cost 0.33492107475901006, iterations 76 m 2.37066025351428, b 1.6617988749518906,cost 0.33040696671746606, iterations 77 m 2.3681538769737696, b 1.6708476895321014,cost 0.325953700387438, iterations 78 m 2.365664448361459, b 1.679835316713965,cost 0.3215604557367435, iterations 79 m 2.3631918530767413, b 1.6887621702420315,cost 0.317226423785699, iterations 80 m 2.360735977293931, b 1.6976286610631413,cost 0.31295080645814943, iterations 81 m 2.3582967079570243, b 1.7064351973453438,cost 0.3087328164345099, iterations 82 m 2.355873932774492, b 1.715182184496687,cost 0.30457167700678567, iterations 83 m 2.353467540214113, b 1.7238700251838805,cost 0.30046662193555157, iterations 84 m 2.351077419497838, b 1.7324991193508317,cost 0.29641689530885124, iterations 85 m 2.348703460596689, b 1.741069864237058,cost 0.29242175140300447, iterations 86 m 2.346345554225699, b 1.749582654395973,cost 0.2884804545452885, iterations 87 m 2.3440035918388746, b 1.7580378817130502,cost 0.2845922789784686, iterations 88 m 2.341677465624204, b 1.7664359354238632,cost 0.28075650872715713, iterations 89 m 2.3393670684986905, b 1.7747772021320043,cost 0.27697243746597183, iterations 90 m 2.337072294103426, b 1.7830620658268812,cost 0.2732393683894715, iterations 91 m 2.334793036798693, b 1.7912909079013948,cost 0.2695566140838465, iterations 92 m 2.3325291916591007, b 1.799464107169496,cost 0.26592349640033686, iterations 93 m 2.330280654468757, b 1.807582039883624,cost 0.2623393463303539, iterations 94 m 2.328047321716469, b 1.8156450797520283,cost 0.2588035038822929, iterations 95 m 2.3258290905909793, b 1.823653597955971,cost 0.25531531795999707, iterations 96 m 2.3236258589762318, b 1.8316079631668145,cost 0.25187414624286397, iterations 97 m 2.321437525446672, b 1.8395085415629941,cost 0.24847935506757024, iterations 98 m 2.319263989262577, b 1.8473556968468738,cost 0.24513031931138451, iterations 99 m 2.317105150365418, b 1.8551497902614897,cost 0.24182642227705822, iterations 100 m 2.3149609093732555, b 1.86289118060718,cost 0.23856705557926494, iterations 101 m 2.3128311675761615, b 1.870580224258102,cost 0.2353516190325724, iterations 102 m 2.3107158269316783, b 1.8782172751786386,cost 0.23217952054092159, iterations 103 m 2.3086147900603033, b 1.8858026849396916,cost 0.2290501759885978, iterations 104 m 2.3065279602410067, b 1.8933368027348676,cost 0.22596300913267228, iterations 105 m 2.30445524140678, b 1.900819975396552,cost 0.22291745149688827, iterations 106 m 2.3023965381402105, b 1.9082525474118763,cost 0.21991294226698505, iterations 107 m 2.3003517556690927, b 1.915634860938576,cost 0.21694892818742614, iterations 108 m 2.2983207998620627, b 1.9229672558207418,cost 0.21402486345952, iterations 109 m 2.296303577224266, b 1.9302500696044647,cost 0.2111402096409213, iterations 110 m 2.2942999948930534, b 1.9374836375533742,cost 0.208294435546475, iterations 111 m 2.292309960633705, b 1.9446682926640728,cost 0.20548701715040749, iterations 112 m 2.290333382835186, b 1.9518043656814654,cost 0.20271743748982962, iterations 113 m 2.2883701705059285, b 1.9588921851139844,cost 0.19998518656954195, iterations 114 m 2.286420233269642, b 1.9659320772487137,cost 0.19728976126812534, iterations 115 m 2.284483481361154, b 1.972924366166408,cost 0.19463066524529227, iterations 116 m 2.282559825622277, b 1.9798693737564133,cost 0.19200740885049272, iterations 117 m 2.280649177497706, b 1.9867674197314835,cost 0.18941950903274735, iterations 118 m 2.278751449030937, b 1.9936188216424995,cost 0.18686648925169824, iterations 119 m 2.2768665528602248, b 2.0004238948930873,cost 0.18434787938985833, iterations 120 m 2.2749944022145554, b 2.0071829527541367,cost 0.181863215666042, iterations 121 m 2.2731349109096546, b 2.0138963063782245,cost 0.17941204054996537, iterations 122 m 2.2712879933440195, b 2.020564264813937,cost 0.17699390267799314, iterations 123 m 2.2694535644949787, b 2.027187135020097,cost 0.17460835677002737, iterations 124 m 2.2676315399147766, b 2.0337652218798956,cost 0.17225496354750947, iterations 125 m 2.2658218357266873, b 2.0402988282149264,cost 0.16993328965253274, iterations 126 m 2.2640243686211536, b 2.046788254799127,cost 0.1676429075680415, iterations 127 m 2.2622390558519507, b 2.0532338003726234,cost 0.1653833955391102, iterations 128 m 2.2604658152323776, b 2.0596357616554846,cost 0.16315433749527697, iterations 129 m 2.2587045651314734, b 2.06599443336138,cost 0.16095532297393003, iterations 130 m 2.2569552244702593, b 2.072310108211148,cost 0.15878594704472473, iterations 131 m 2.2552177127180073, b 2.0785830769462708,cost 0.1566458102350166, iterations 132 m 2.2534919498885317, b 2.084813628342259,cost 0.1545345184563048, iterations 133 m 2.2517778565365068, b 2.0910020492219448,cost 0.15245168293166167, iterations 134 m 2.2500753537538105, b 2.0971486244686863,cost 0.1503969201241444, iterations 135 m 2.2483843631658917, b 2.1032536370394816,cost 0.1483698516661673, iterations 136 m 2.2467048069281614, b 2.1093173679779955,cost 0.14637010428983166, iterations 137 m 2.245036607722411, b 2.1153400964274964,cost 0.14439730975818935, iterations 138 m 2.2433796887532504, b 2.121322099643707,cost 0.1424511047974366, iterations 139 m 2.241733973744575, b 2.127263653007569,cost 0.14053113103001819, iterations 140 m 2.2400993869360537, b 2.1331650300379175,cost 0.1386370349086369, iterations 141 m 2.23847585307964, b 2.139026502404074,cost 0.1367684676511506, iterations 142 m 2.2368632974361096, b 2.1448483399383544,cost 0.13492508517634536, iterations 143 m 2.235261645771619, b 2.150630810648487,cost 0.13310654804057775, iterations 144 m 2.233670824354288, b 2.1563741807299532,cost 0.13131252137526755, iterations 145 m 2.232090759950807, b 2.1620787145782407,cost 0.12954267482523477, iterations 146 m 2.230521379823063, b 2.167744674801014,cost 0.12779668248786885, iterations 147 m 2.2289626117247936, b 2.1733723222302057,cost 0.12607422285311398, iterations 148 m 2.22741438389826, b 2.178961915934022,cost 0.12437497874426806, iterations 149 m 2.225876625070943, b 2.18451371322887,cost 0.12269863725957603, iterations 150 m 2.2243492644522638, b 2.190027969691202,cost 0.12104488971461036, iterations 151 m 2.2228322317303233, b 2.1955049391692825,cost 0.11941343158543305, iterations 152 m 2.221325457068667, b 2.2009448737948722,cost 0.11780396245251534, iterations 153 m 2.219828871103069, b 2.2063480239948374,cost 0.11621618594542303, iterations 154 m 2.2183424049383382, b 2.211714638502676,cost 0.11464980968823649, iterations 155 m 2.2168659901451484, b 2.2170449643699683,cost 0.11310454524571843, iterations 156 m 2.215399558756887, b 2.222339246977752,cost 0.11158010807019521, iterations 157 m 2.2139430432665264, b 2.2275977300478154,cost 0.1100762174491634, iterations 158 m 2.212496376623517, b 2.2328206556539194,cost 0.10859259645359752, iterations 159 m 2.2110594922306994, b 2.2380082642329406,cost 0.1071289718869558, iterations 160 m 2.2096323239412388, b 2.2431607945959393,cost 0.1056850742348745, iterations 161 m 2.208214806055581, b 2.248278483939153,cost 0.1042606376155363, iterations 162 m 2.2068068733184267, b 2.2533615678549173,cost 0.102855399730714, iterations 163 m 2.205408460915729, b 2.2584102803425092,cost 0.10146910181746785, iterations 164 m 2.2040195044717072, b 2.2634248538189214,cost 0.10010148860049717, iterations 165 m 2.2026399400458856, b 2.26840551912956,cost 0.09875230824513428, iterations 166 m 2.2012697041301488, b 2.2733525055588713,cost 0.09742131231096945, iterations 167 m 2.1999087336458185, b 2.2782660408408986,cost 0.0961082557061047, iterations 168 m 2.1985569659407505, b 2.2831463511697643,cost 0.09481289664202112, iterations 169 m 2.1972143387864485, b 2.2879936612100837,cost 0.09353499658905558, iterations 170 m 2.195880790375201, b 2.2928081941073066,cost 0.09227432023247761, iterations 171 m 2.1945562593172356, b 2.29759017149799,cost 0.09103063542915772, iterations 172 m 2.1932406846378933, b 2.3023398135200024,cost 0.08980371316482053, iterations 173 m 2.19193400577482, b 2.307057338822655,cost 0.08859332751187383, iterations 174 m 2.1906361625751805, b 2.3117429645767706,cost 0.08739925558780584, iterations 175 m 2.1893470952928884, b 2.316396906484678,cost 0.08622127751414266, iterations 176 m 2.188066744585856, b 2.3210193787901443,cost 0.08505917637596, iterations 177 m 2.186795051513262, b 2.3256105942882357,cost 0.08391273818194048, iterations 178 m 2.1855319575328385, b 2.3301707643351146,cost 0.0827817518249678, iterations 179 m 2.1842774044981756, b 2.3347000988577693,cost 0.08166600904325434, iterations 180 m 2.183031334656046, b 2.3391988063636773,cost 0.08056530438198997, iterations 181 m 2.1817936906437447, b 2.3436670939504047,cost 0.07947943515551098, iterations 182 m 2.1805644154864483, b 2.348105167315139,cost 0.07840820140997531, iterations 183 m 2.1793434525945945, b 2.3525132307641594,cost 0.07735140588654536, iterations 184 m 2.178130745761274, b 2.3568914872222417,cost 0.07630885398506115, iterations 185 m 2.1769262391596444, b 2.361240138241999,cost 0.07528035372820886, iterations 186 m 2.175729877340361, b 2.365559384013162,cost 0.07426571572616834, iterations 187 m 2.1745416052290225, b 2.369849423371792,cost 0.0732647531417393, iterations 188 m 2.1733613681236377, b 2.374110453809438,cost 0.07227728165593673, iterations 189 m 2.1721891116921044, b 2.378342671482224,cost 0.07130311943404995, iterations 190 m 2.1710247819697117, b 2.3825462712198826,cost 0.07034208709215872, iterations 191 m 2.1698683253566524, b 2.3867214465347217,cost 0.06939400766410185, iterations 192 m 2.1687196886155586, b 2.3908683896305347,cost 0.06845870656889064, iterations 193 m 2.167578818869049, b 2.394987291411446,cost 0.06753601157855987, iterations 194 m 2.166445663597294, b 2.399078341490702,cost 0.06662575278645548, iterations 195 m 2.1653201706356002, b 2.4031417281993988,cost 0.065727762575945, iterations 196 m 2.1642022881720084, b 2.407177638595151,cost 0.064841875589555, iterations 197 m 2.1630919647449067, b 2.411186258470704,cost 0.06396792869852047, iterations 198 m 2.161989149240663, b 2.4151677723624867,cost 0.06310576097274612, iterations 199 m 2.160893790891273, b 2.4191223635591075,cost 0.062255213651172675, iterations 200 m 2.15980583927202, b 2.4230502141097903,cost 0.06141613011254206, iterations 201 m 2.1587252442991565, b 2.4269515048327563,cost 0.06058835584655702, iterations 202 m 2.157651956227597, b 2.4308264153235473,cost 0.059771738425429835, iterations 203 m 2.1565859256486286, b 2.4346751239632938,cost 0.05896612747581329, iterations 204 m 2.155527103487637, b 2.4384978079269266,cost 0.05817137465111021, iterations 205 m 2.1544754410018454, b 2.442294643191333,cost 0.05738733360415889, iterations 206 m 2.1534308897780736, b 2.4460658045434585,cost 0.05661385996028128, iterations 207 m 2.152393401730506, b 2.4498114655883514,cost 0.05585081129070002, iterations 208 m 2.151362929098481, b 2.4535317987571568,cost 0.05509804708631118, iterations 209 m 2.1503394244442906, b 2.4572269753150526,cost 0.05435542873180857, iterations 210 m 2.1493228406509965, b 2.460897165369136,cost 0.053622819480162204, iterations 211 m 2.1483131309202617, b 2.464542537876251,cost 0.052900084427434435, iterations 212 m 2.1473102487701965, b 2.4681632606507695,cost 0.05218709048793955, iterations 213 m 2.146314148033218, b 2.471759500372315,cost 0.051483706369738605, iterations 214 m 2.1453247828539244, b 2.4753314225934364,cost 0.05078980255046022, iterations 215 m 2.144342107686985, b 2.478879191747228,cost 0.05010525125345261, iterations 216 m 2.1433660772950445, b 2.4824029711549005,cost 0.04942992642425346, iterations 217 m 2.142396646746637, b 2.485902923033299,cost 0.04876370370737887, iterations 218 m 2.1414337714141207, b 2.4893792085023705,cost 0.04810646042342204, iterations 219 m 2.1404774069716233, b 2.4928319875925813,cost 0.04745807554646611, iterations 220 m 2.1395275093929995, b 2.496261419252283,cost 0.04681842968179604, iterations 221 m 2.1385840349498055, b 2.4996676613550317,cost 0.046187405043913436, iterations 222 m 2.1376469402092875, b 2.5030508707068537,cost 0.04556488543484799, iterations 223 m 2.1367161820323783, b 2.506411203053465,cost 0.044950756222759944, iterations 224 m 2.135791717571716, b 2.509748813087441,cost 0.044344904320831526, iterations 225 m 2.134873504269668, b 2.5130638544553374,cost 0.04374721816644389, iterations 226 m 2.1339614998563734, b 2.516356479764764,cost 0.043157587700632126, iterations 227 m 2.1330556623477976, b 2.5196268405914086,cost 0.04257590434782047, iterations 228 m 2.132155950043798, b 2.5228750874860166,cost 0.04200206099582856, iterations 229 m 2.1312623215262048, b 2.5261013699813204,cost 0.04143595197614648, iterations 230 m 2.130374735656916, b 2.529305836598923,cost 0.040877473044478495, iterations 231 m 2.129493151576002, b 2.532488634856136,cost 0.04032652136154589, iterations 232 m 2.128617528699825, b 2.5356499112727704,cost 0.03978299547415109, iterations 233 m 2.1277478267191694, b 2.5387898113778804,cost 0.03924679529649494, iterations 234 m 2.126884005597389, b 2.541908479716465,cost 0.03871782209174746, iterations 235 m 2.1260260255685623, b 2.54500605985612,cost 0.038195978453866075, iterations 236 m 2.1251738471356605, b 2.5480826943936474,cost 0.03768116828965854, iterations 237 m 2.1243274310687323, b 2.551138524961622,cost 0.03717329680108927, iterations 238 m 2.1234867384030953, b 2.5541736922349094,cost 0.036672270467821975, iterations 239 m 2.122651730437544, b 2.5571883359371417,cost 0.036177997029999125, iterations 240 m 2.121822368732568, b 2.560182594847151,cost 0.03569038547125328, iterations 241 m 2.12099861510858, b 2.563156606805357,cost 0.03520934600194705, iterations 242 m 2.1201804316441617, b 2.566110508720113,cost 0.03473479004263852, iterations 243 m 2.1193677806743167, b 2.569044436574009,cost 0.034266630207771036, iterations 244 m 2.1185606247887363, b 2.5719585254301305,cost 0.033804780289581275, iterations 245 m 2.1177589268300765, b 2.574852909438277,cost 0.033349155242224614, iterations 246 m 2.1169626498922494, b 2.577727721841137,cost 0.03289967116611525, iterations 247 m 2.1161717573187233, b 2.5805830949804216,cost 0.032456245292476225, iterations 248 m 2.1153862127008343, b 2.5834191603029577,cost 0.032018795968098826, iterations 249 m 2.1146059798761123, b 2.5862360483667395,cost 0.03158724264030631, iterations 250 m 2.113831022926614, b 2.5890338888469366,cost 0.031161505842120647, iterations 251 m 2.1130613061772716, b 2.5918128105418656,cost 0.030741507177630047, iterations 252 m 2.1122967941942483, b 2.5945729413789183,cost 0.03032716930755256, iterations 253 m 2.1115374517833088, b 2.5973144084204516,cost 0.02991841593499473, iterations 254 m 2.1107832439881986, b 2.6000373378696366,cost 0.02951517179140192, iterations 255 m 2.110034136089035, b 2.6027418550762675,cost 0.029117362622698456, iterations 256 m 2.1092900936007073, b 2.605428084542533,cost 0.02872491517561477, iterations 257 m 2.1085510822712923, b 2.6080961499287465,cost 0.028337757184197884, iterations 258 m 2.107817068080474, b 2.6107461740590416,cost 0.027955817356504056, iterations 259 m 2.1070880172379804, b 2.6133782789270232,cost 0.02757902536147144, iterations 260 m 2.1063638961820264, b 2.6159925857013846,cost 0.027207311815968978, iterations 261 m 2.105644671577769, b 2.618589214731486,cost 0.026840608272020273, iterations 262 m 2.104930310315772, b 2.621168285552894,cost 0.02647884720419907, iterations 263 m 2.1042207795104853, b 2.6237299168928856,cost 0.026121961997195367, iterations 264 m 2.1035160464987257, b 2.626274226675912,cost 0.025769886933548605, iterations 265 m 2.102816078838177, b 2.6288013320290284,cost 0.02542255718154624, iterations 266 m 2.1021208443058956, b 2.631311349287286,cost 0.025079908783286038, iterations 267 m 2.101430310896827, b 2.633804393999087,cost 0.02474187864289761, iterations 268 m 2.1007444468223326, b 2.6362805809315044,cost 0.02440840451492526, iterations 269 m 2.1000632205087255, b 2.6387400240755645,cost 0.024079424992865115, iterations 270 m 2.0993866005958184, b 2.641182836651495,cost 0.023754879497857956, iterations 271 m 2.0987145559354787, b 2.6436091311139367,cost 0.023434708267533407, iterations 272 m 2.0980470555901958, b 2.6460190191571216,cost 0.023118852345006557, iterations 273 m 2.097384068831655, b 2.6484126117200133,cost 0.02280725356802024, iterations 274 m 2.0967255651393253, b 2.650790018991414,cost 0.02249985455823556, iterations 275 m 2.0960715141990525, b 2.6531513504150386,cost 0.022196598710666177, iterations 276 m 2.095421885901665, b 2.6554967146945505,cost 0.021897430183254754, iterations 277 m 2.0947766503415863, b 2.6578262197985687,cost 0.021602293886590018, iterations 278 m 2.09413577781546, b 2.6601399729656356,cost 0.02131113547376268, iterations 279 m 2.0934992388207814, b 2.662438080709155,cost 0.021023901330358138, iterations 280 m 2.092867004054539, b 2.6647206488222954,cost 0.020740538564583334, iterations 281 m 2.0922390444118664, b 2.6669877823828587,cost 0.020460994997527474, iterations 282 m 2.0916153309847023, b 2.6692395857581204,cost 0.020185219153553365, iterations 283 m 2.090995835060459, b 2.6714761626096313,cost 0.01991316025081965, iterations 284 m 2.090380528120701, b 2.673697615897991,cost 0.019644768191927994, iterations 285 m 2.0897693818398335, b 2.6759040478875873,cost 0.019379993554699673, iterations 286 m 2.0891623680837963, b 2.678095560151304,cost 0.019118787583073903, iterations 287 m 2.0885594589087693, b 2.6802722535751964,cost 0.018861102178130713, iterations 288 m 2.0879606265598873, b 2.682434228363136,cost 0.018606889889232675, iterations 289 m 2.0873658434699607, b 2.684581584041424,cost 0.0183561039052884, iterations 290 m 2.086775082258207, b 2.686714419463372,cost 0.01810869804613219, iterations 291 m 2.0861883157289913, b 2.688832832813852,cost 0.017864626754020348, iterations 292 m 2.0856055168705727, b 2.6909369216138193,cost 0.01762384508524208, iterations 293 m 2.0850266588538626, b 2.6930267827247976,cost 0.01738630870184344, iterations 294 m 2.084451715031187, b 2.6951025123533423,cost 0.01715197386346352, iterations 295 m 2.0838806589350636, b 2.6971642060554664,cost 0.016920797419278395, iterations 296 m 2.0833134642769795, b 2.69921195874104,cost 0.016692736800057357, iterations 297 m 2.0827501049461836, b 2.7012458646781607,cost 0.016467750010322836, iterations 298 m 2.0821905550084834, b 2.703266017497492,cost 0.01624579562061702, iterations 299 m 2.0816347887050517, b 2.705272510196574,cost 0.016026832759874293, iterations 300 m 2.08108278045124, b 2.7072654351441052,cost 0.015810821107893586, iterations 301 m 2.0805345048354016, b 2.709244884084192,cost 0.015597720887915171, iterations 302 m 2.079989936617722, b 2.711210948140576,cost 0.015387492859295216, iterations 303 m 2.079449050729055, b 2.7131637178208265,cost 0.0151800983102799, iterations 304 m 2.0789118222697716, b 2.715103283020507,cost 0.014975499050877805, iterations 305 m 2.0783782265086113, b 2.717029733027314,cost 0.01477365740582659, iterations 306 m 2.0778482388815447, b 2.7189431565251883,cost 0.01457453620765577, iterations 307 m 2.0773218349906424, b 2.720843641598395,cost 0.014378098789843051, iterations 308 m 2.0767989906029523, b 2.7227312757355824,cost 0.014184308980061587, iterations 309 m 2.0762796816493836, b 2.7246061458338047,cost 0.013993131093520104, iterations 310 m 2.075763884223598, b 2.7264683382025265,cost 0.013804529926391102, iterations 311 m 2.075251574580912, b 2.7283179385675935,cost 0.013618470749328865, iterations 312 m 2.0747427291371996, b 2.73015503207518,cost 0.01343491930107421, iterations 313 m 2.07423732446781, b 2.731979703295709,cost 0.013253841782144891, iterations 314 m 2.0737353373064886, b 2.7337920362277437,cost 0.013075204848613055, iterations 315 m 2.0732367445443045, b 2.735592114301855,cost 0.012898975605964123, iterations 316 m 2.0727415232285877, b 2.7373800203844643,cost 0.012725121603039978, iterations 317 m 2.0722496505618735, b 2.7391558367816553,cost 0.012553610826062876, iterations 318 m 2.0717611039008506, b 2.7409196452429643,cost 0.01238441169274141, iterations 319 m 2.0712758607553208, b 2.7426715269651436,cost 0.012217493046453731, iterations 320 m 2.0707938987871626, b 2.7444115625958996,cost 0.012052824150510996, iterations 321 m 2.070315195809303, b 2.746139832237604,cost 0.011890374682497441, iterations 322 m 2.069839729784697, b 2.7478564154509835,cost 0.011730114728686453, iterations 323 m 2.0693674788253125, b 2.7495613912587804,cost 0.011572014778532154, iterations 324 m 2.0688984211911214, b 2.7512548381493915,cost 0.01141604571923599, iterations 325 m 2.068432535289101, b 2.752936834080481,cost 0.011262178830384915, iterations 326 m 2.067969799672239, b 2.75460745648257,cost 0.01111038577866335, iterations 327 m 2.067510193038545, b 2.7562667822625984,cost 0.010960638612635697, iterations 328 m 2.0670536942300735, b 2.757914887807469,cost 0.01081290975759911, iterations 329 m 2.0666002822319447, b 2.7595518489875612,cost 0.01066717201050644, iterations 330 m 2.0661499361713815, b 2.761177741160225,cost 0.010523398534956067, iterations 331 m 2.0657026353167467, b 2.7627926391732505,cost 0.010381562856251218, iterations 332 m 2.065258359076588, b 2.7643966173683108,cost 0.010241638856524038, iterations 333 m 2.064817086998692, b 2.7659897495843877,cost 0.010103600769926802, iterations 334 m 2.064378798769141, b 2.7675721091611694,cost 0.009967423177887062, iterations 335 m 2.0639434742113787, b 2.769143768942426,cost 0.009833081004426715, iterations 336 m 2.063511093285281, b 2.7707048012793636,cost 0.009700549511545375, iterations 337 m 2.063081636086234, b 2.7722552780339553,cost 0.00956980429466415, iterations 338 m 2.0626550828442163, b 2.773795270582249,cost 0.009440821278132278, iterations 339 m 2.062231413922891, b 2.775324849817653,cost 0.009313576710793291, iterations 340 m 2.061810609818701, b 2.7768440861542,cost 0.009188047161612303, iterations 341 m 2.0613926511599687, b 2.778353049529788,cost 0.00906420951536031, iterations 342 m 2.0609775187060078, b 2.7798518094094002,cost 0.008942040968358723, iterations 343 m 2.0605651933462363, b 2.7813404347883033,cost 0.008821519024279485, iterations 344 m 2.060155656099296, b 2.782818994195223,cost 0.008702621490002818, iterations 345 m 2.0597488881121793, b 2.7842875556954985,cost 0.00858532647153081, iterations 346 m 2.0593448706593604, b 2.785746186894217,cost 0.008469612369955385, iterations 347 m 2.058943585141936, b 2.787194954939325,cost 0.008355457877481338, iterations 348 m 2.058545013086765, b 2.7886339265247195,cost 0.00824284197350259, iterations 349 m 2.058149136145622, b 2.7900631678933188,cost 0.008131743920731368, iterations 350 m 2.0577559360943503, b 2.791482744840111,cost 0.008022143261379852, iterations 351 m 2.057365394832023, b 2.7928927227151847,cost 0.007914019813392419, iterations 352 m 2.056977494380111, b 2.7942931664267348,cost 0.007807353666729917, iterations 353 m 2.056592216881654, b 2.795684140444052,cost 0.00770212517970354, iterations 354 m 2.0562095446004403, b 2.7970657088004915,cost 0.007598314975357035, iterations 355 m 2.0558294599201874, b 2.7984379350964192,cost 0.007495903937899565, iterations 356 m 2.0554519453437345, b 2.79980088250214,cost 0.007394873209185192, iterations 357 m 2.0550769834922344, b 2.8011546137608065,cost 0.007295204185240327, iterations 358 m 2.0547045571043547, b 2.802499191191306,cost 0.007196878512838298, iterations 359 m 2.054334649035482, b 2.8038346766911313,cost 0.007099878086119337, iterations 360 m 2.053967242256934, b 2.805161131739228,cost 0.007004185043256591, iterations 361 m 2.053602319855176, b 2.806478617398827,cost 0.006909781763167505, iterations 362 m 2.0532398650310393, b 2.8077871943202526,cost 0.006816650862268388, iterations 363 m 2.0528798610989516, b 2.8090869227437176,cost 0.006724775191273555, iterations 364 m 2.0525222914861665, b 2.8103778625020945,cost 0.006634137832038017, iterations 365 m 2.052167139732002, b 2.811660073023671,cost 0.006544722094441161, iterations 366 m 2.0518143894870806, b 2.812933613334884,cost 0.006456511513314127, iterations 367 m 2.0514640245125793, b 2.8141985420630387,cost 0.00636948984540767, iterations 368 m 2.0511160286794796, b 2.8154549174390078,cost 0.00628364106640095, iterations 369 m 2.0507703859678275, b 2.81670279729991,cost 0.006198949367950968, iterations 370 m 2.0504270804659943, b 2.8179422390917743,cost 0.00611539915478173, iterations 371 m 2.050086096369944, b 2.819173299872184,cost 0.006032975041812088, iterations 372 m 2.0497474179825064, b 2.8203960363129035,cost 0.005951661851322984, iterations 373 m 2.049411029712655, b 2.8216105047024866,cost 0.005871444610162711, iterations 374 m 2.0490769160747884, b 2.822816760948869,cost 0.005792308546989337, iterations 375 m 2.048745061688017, b 2.8240148605819395,cost 0.00571423908955121, iterations 376 m 2.048415451275457, b 2.8252048587561,cost 0.005637221862002823, iterations 377 m 2.0480880696635237, b 2.826386810252801,cost 0.005561242682258636, iterations 378 m 2.0477629017812373, b 2.8275607694830662,cost 0.005486287559380737, iterations 379 m 2.047439932659525, b 2.8287267904899953,cost 0.005412342691002845, iterations 380 m 2.0471191474305344, b 2.8298849269512525,cost 0.005339394460788852, iterations 381 m 2.046800531326949, b 2.8310352321815384,cost 0.005267429435925081, iterations 382 m 2.046484069681307, b 2.832177759135043,cost 0.005196434364647206, iterations 383 m 2.046169747925327, b 2.8333125604078844,cost 0.005126396173799798, iterations 384 m 2.0458575515892368, b 2.8344396882405296,cost 0.005057301966428905, iterations 385 m 2.0455474663011093, b 2.8355591945202,cost 0.004989139019407402, iterations 386 m 2.0452394777861973, b 2.836671130783259,cost 0.004921894781092172, iterations 387 m 2.0449335718662796, b 2.837775548217585,cost 0.004855556869012658, iterations 388 m 2.0446297344590065, b 2.8388724976649278,cost 0.0047901130675905915, iterations 389 m 2.044327951577252, b 2.83996202962325,cost 0.004725551325891127, iterations 390 m 2.0440282093284714, b 2.84104419424905,cost 0.004661859755403072, iterations 391 m 2.043730493914058, b 2.842119041359671,cost 0.004599026627850468, iterations 392 m 2.043434791628712, b 2.8431866204355973,cost 0.004537040373032051, iterations 393 m 2.043141088859807, b 2.844246980622728,cost 0.0044758895766916175, iterations 394 m 2.0428493720867644, b 2.845300170734642,cost 0.004415562978415472, iterations 395 m 2.042559627880431, b 2.8463462392548444,cost 0.0043560494695592525, iterations 396 m 2.04227184290246, b 2.847385234338999,cost 0.0042973380912023065, iterations 397 m 2.041986003904698, b 2.8484172038171436,cost 0.004239418032129606, iterations 398 m 2.0417020977285736, b 2.849442195195894,cost 0.004182278626841194, iterations 399 m 2.0414201113044936, b 2.850460255660629,cost 0.004125909353587855, iterations 400 m 2.041140031651241, b 2.851471432077665,cost 0.0040702998324338435, iterations 401 m 2.040861845875375, b 2.8524757709964095,cost 0.0040154398233457475, iterations 402 m 2.040585541170641, b 2.853473318651508,cost 0.003961319224306282, iterations 403 m 2.040311104817378, b 2.8544641209649706,cost 0.0039079280694545625, iterations 404 m 2.040038524181935, b 2.8554482235482865,cost 0.003855256527250764, iterations 405 m 2.0397677867160895, b 2.856425671704523,cost 0.0038032948986658982, iterations 406 m 2.0394988799564673, b 2.8573965104304113,cost 0.003752033615395589, iterations 407 m 2.039231791523972, b 2.858360784418419,cost 0.003701463238098197, iterations 408 m 2.038966509123214, b 2.8593185380588055,cost 0.0036515744546569587, iterations 409 m 2.038703020541943, b 2.8602698154416677,cost 0.0036023580784645994, iterations 410 m 2.038441313650488, b 2.861214660358968,cost 0.003553805046732554, iterations 411 m 2.038181376401197, b 2.862153116306551,cost 0.003505906418821263, iterations 412 m 2.0379231968278844, b 2.8630852264861453,cost 0.003458653374594417, iterations 413 m 2.037666763045278, b 2.8640110338073534,cost 0.003412037212794447, iterations 414 m 2.0374120632484733, b 2.864930580889626,cost 0.003366049349440654, iterations 415 m 2.03715908571239, b 2.865843910064224,cost 0.0033206813162481074, iterations 416 m 2.0369078187912315, b 2.866751063376168,cost 0.0032759247590686304, iterations 417 m 2.0366582509179496, b 2.8676520825861735,cost 0.0032317714363520756, iterations 418 m 2.036410370603711, b 2.8685470091725724,cost 0.003188213217629183, iterations 419 m 2.0361641664373695, b 2.869435884333224,cost 0.0031452420820140105, iterations 420 m 2.03591962708494, b 2.870318748987411,cost 0.0031028501167271747, iterations 421 m 2.0356767412890773, b 2.8711956437777215,cost 0.003061029515638672, iterations 422 m 2.0354354978685567, b 2.8720666090719233,cost 0.003019772577830562, iterations 423 m 2.0351958857177608, b 2.8729316849648194,cost 0.00297907170617887, iterations 424 m 2.0349578938061677, b 2.8737909112800955,cost 0.0029389194059544774, iterations 425 m 2.0347215111778425, b 2.8746443275721516,cost 0.002899308283443267, iterations 426 m 2.0344867269509335, b 2.8754919731279243,cost 0.0028602310445844975, iterations 427 m 2.0342535303171716, b 2.876333886968695,cost 0.002821680493627645, iterations 428 m 2.0340219105413726, b 2.877170107851887,cost 0.002783649531807347, iterations 429 m 2.033791856960942, b 2.878000674272847,cost 0.0027461311560365576, iterations 430 m 2.033563358985386, b 2.87882562446662,cost 0.002709118457616413, iterations 431 m 2.0333364060958217, b 2.879644996409709,cost 0.002672604620964549, iterations 432 m 2.033110987844495, b 2.8804588278218217,cost 0.0026365829223597976, iterations 433 m 2.032887093854298, b 2.8812671561676093,cost 0.00260104672870406, iterations 434 m 2.032664713818294, b 2.882070018658389,cost 0.002565989496301132, iterations 435 m 2.0324438374992377, b 2.8828674522538584,cost 0.00253140476965138, iterations 436 m 2.03222445472911, b 2.8836594936637954,cost 0.002497286180263369, iterations 437 m 2.032006555408646, b 2.8844461793497502,cost 0.00246362744548099, iterations 438 m 2.031790129506872, b 2.885227545526723,cost 0.002430422367326332, iterations 439 m 2.0315751670606415, b 2.886003628164829,cost 0.0023976648313587976, iterations 440 m 2.0313616581741796, b 2.886774462990959,cost 0.0023653488055489953, iterations 441 m 2.0311495930186254, b 2.8875400854904187,cost 0.0023334683391678696, iterations 442 m 2.03093896183158, b 2.888300530908567,cost 0.002302017561691101, iterations 443 m 2.0307297549166567, b 2.8890558342524346,cost 0.0022709906817180675, iterations 444 m 2.0305219626430357, b 2.8898060302923385,cost 0.0022403819859051787, iterations 445 m 2.0303155754450195, b 2.890551153563481,cost 0.002210185837914231, iterations 446 m 2.0301105838215934, b 2.8912912383675393,cost 0.002180396677374227, iterations 447 m 2.0299069783359878, b 2.8920263187742465,cost 0.0021510090188574388, iterations 448 m 2.029704749615244, b 2.8927564286229583,cost 0.0021220174508695022, iterations 449 m 2.029503888349782, b 2.893481601524211,cost 0.0020934166348529023, iterations 450 m 2.0293043852929724, b 2.8942018708612687,cost 0.002065201304203791, iterations 451 m 2.0291062312607124, b 2.8949172697916614,cost 0.0020373662633022364, iterations 452 m 2.0289094171309996, b 2.8956278312487096,cost 0.0020099063865555654, iterations 453 m 2.0287139338435147, b 2.896333587943041,cost 0.001982816617454522, iterations 454 m 2.0285197723992034, b 2.8970345723640976,cost 0.0019560919676419766, iterations 455 m 2.0283269238598622, b 2.897730816781629,cost 0.0019297275159947697, iterations 456 m 2.0281353793477273, b 2.8984223532471804,cost 0.0019037184077168802, iterations 457 m 2.0279451300450657, b 2.899109213595566,cost 0.001878059853446186, iterations 458 m 2.0277561671937687, b 2.8997914294463354,cost 0.0018527471283719494, iterations 459 m 2.02756848209495, b 2.90046903220523,cost 0.0018277755713651238, iterations 460 m 2.0273820661085447, b 2.9011420530656267,cost 0.0018031405841199282, iterations 461 m 2.02719691065291, b 2.9018105230099764,cost 0.0017788376303071026, iterations 462 m 2.0270130072044323, b 2.902474472811228,cost 0.0017548622347386082, iterations 463 m 2.0268303472971345, b 2.903133933034247,cost 0.001731209982543586, iterations 464 m 2.0266489225222855, b 2.903788934037221,cost 0.0017078765183553045, iterations 465 m 2.0264687245280135, b 2.9044395059730577,cost 0.0016848575455091824, iterations 466 m 2.0262897450189206, b 2.905085678790774,cost 0.0016621488252516797, iterations 467 m 2.0261119757557027, b 2.9057274822368724,cost 0.0016397461759597495, iterations 468 m 2.0259354085547687, b 2.9063649458567133,cost 0.0016176454723706127, iterations 469 m 2.0257600352878646, b 2.9069980989958726,cost 0.0015958426448225472, iterations 470 m 2.0255858478816995, b 2.907626970801494,cost 0.001574333678504963, iterations 471 m 2.0254128383175725, b 2.9082515902236303,cost 0.0015531146127196437, iterations 472 m 2.025240998631005, b 2.9088719860165764,cost 0.0015321815401509106, iterations 473 m 2.025070320911374, b 2.9094881867401927,cost 0.0015115306061466082, iterations 474 m 2.0249007973015463, b 2.9101002207612203,cost 0.001491158008007918, iterations 475 m 2.0247324199975196, b 2.910708116254586,cost 0.0014710599942892796, iterations 476 m 2.0245651812480605, b 2.9113119012047,cost 0.0014512328641076747, iterations 477 m 2.02439907335435, b 2.911911603406745,cost 0.0014316729664609366, iterations 478 m 2.0242340886696266, b 2.9125072504679532,cost 0.0014123766995556277, iterations 479 m 2.0240702195988365, b 2.9130988698088798,cost 0.0013933405101437262, iterations 480 m 2.023907458598283, b 2.913686488664664,cost 0.0013745608928683111, iterations 481 m 2.0237457981752787, b 2.9142701340862835,cost 0.0013560343896180944, iterations 482 m 2.023585230887802, b 2.914849832941799,cost 0.0013377575888906688, iterations 483 m 2.023425749344153, b 2.9154256119175908,cost 0.001319727125164258, iterations 484 m 2.0232673462026147, b 2.9159974975195886,cost 0.001301939678278064, iterations 485 m 2.0231100141711136, b 2.9165655160744914,cost 0.0012843919728207727, iterations 486 m 2.0229537460068845, b 2.917129693730978,cost 0.0012670807775274193, iterations 487 m 2.022798534516138, b 2.9176900564609127,cost 0.001250002904684557, iterations 488 m 2.0226443725537275, b 2.9182466300605396,cost 0.0012331552095431202, iterations 489 m 2.0224912530228227, b 2.9187994401516706,cost 0.0012165345897392687, iterations 490 m 2.02233916887458, b 2.919348512182865,cost 0.0012001379847232673, iterations 491 m 2.022188113107821, b 2.9198938714306006,cost 0.0011839623751958592, iterations 492 m 2.022038078768708, b 2.9204355430004383,cost 0.001168004782552273, iterations 493 m 2.021889058950424, b 2.9209735518281756,cost 0.0011522622683337383, iterations 494 m 2.0217410467928563, b 2.921507922680998,cost 0.0011367319336864302, iterations 495 m 2.0215940354822797, b 2.9220386801586153,cost 0.0011214109188275154, iterations 496 m 2.021448018251043, b 2.922565848694397,cost 0.0011062964025188753, iterations 497 m 2.0213029883772564, b 2.923089452556496,cost 0.0010913856015472361, iterations 498 m 2.021158939184484, b 2.9236095158489652,cost 0.0010766757702118637, iterations 499 m 2.0210158640414355, b 2.9241260625128684,cost 0.0010621641998189199, iterations 500 m 2.0208737563616594, b 2.9246391163273815,cost 0.0010478482181826851, iterations 501 m 2.0207326096032436, b 2.925148700910887,cost 0.0010337251891334419, iterations 502 m 2.02059241726851, b 2.9256548397220623,cost 0.001019792512032204, iterations 503 m 2.0204531729037183, b 2.9261575560609585,cost 0.0010060476212916758, iterations 504 m 2.020314870098767, b 2.926656873070074,cost 0.0009924879859038033, iterations 505 m 2.020177502486901, b 2.927152813735419,cost 0.000979111108973823, iterations 506 m 2.020041063744414, b 2.9276454008875743,cost 0.0009659145272604534, iterations 507 m 2.019905547590363, b 2.9281346572027416,cost 0.0009528958107222865, iterations 508 m 2.019770947786274, b 2.9286206052037884,cost 0.000940052562070263, iterations 509 m 2.019637258135859, b 2.929103267261284,cost 0.0009273824163262987, iterations 510 m 2.019504472484727, b 2.929582665594529,cost 0.000914883040387855, iterations 511 m 2.0193725847201036, b 2.930058822272581,cost 0.000902552132598133, iterations 512 m 2.0192415887705484, b 2.930531759215265,cost 0.0008903874223224499, iterations 513 m 2.019111478605675, b 2.9310014981941888,cost 0.0008783866695299, iterations 514 m 2.0189822482358757, b 2.93146806083374,cost 0.0008665476643810914, iterations 515 m 2.0188538917120415, b 2.931931468612085,cost 0.0008548682268211402, iterations 516 m 2.018726403125293, b 2.9323917428621566,cost 0.0008433462061781243, iterations 517 m 2.0185997766067056, b 2.9328489047726354,cost 0.0008319794807672252, iterations 518 m 2.0184740063270388, b 2.9333029753889255,cost 0.0008207659574998923, iterations 519 m 2.0183490864964706, b 2.933753975614124,cost 0.0008097035714985473, iterations 520 m 2.0182250113643287, b 2.9342019262099828,cost 0.0007987902857161965, iterations 521 m 2.018101775218826, b 2.934646847797864,cost 0.0007880240905615668, iterations 522 m 2.0179793723867987, b 2.9350887608596907,cost 0.0007774030035287504, iterations 523 m 2.0178577972334444, b 2.9355276857388874,cost 0.0007669250688324104, iterations 524 m 2.0177370441620623, b 2.9359636426413185,cost 0.000756588357047506, iterations 525 m 2.0176171076137965, b 2.936396651636218,cost 0.0007463909647539881, iterations 526 m 2.01749798206738, b 2.9368267326571136,cost 0.0007363310141863485, iterations 527 m 2.0173796620388793, b 2.9372539055027436,cost 0.000726406652887885, iterations 528 m 2.017262142081443, b 2.9376781898379685,cost 0.0007166160533694976, iterations 529 m 2.017145416785052, b 2.9380996051946764,cost 0.0007069574127732173, iterations 530 m 2.017029480776268, b 2.9385181709726833,cost 0.0006974289525402686, iterations 531 m 2.0169143287179883, b 2.938933906440624,cost 0.0006880289180833856, iterations 532 m 2.0167999553091986, b 2.93934683073684,cost 0.0006787555784639849, iterations 533 m 2.0166863552847305, b 2.9397569628702627,cost 0.0006696072260732263, iterations 534 m 2.0165735234150177, b 2.9401643217212845,cost 0.0006605821763176377, iterations 535 m 2.0164614545058557, b 2.940568926042631,cost 0.0006516787673089258, iterations 536 m 2.0163501433981637, b 2.940970794460223,cost 0.0006428953595579223, iterations 537 m 2.0162395849677446, b 2.9413699454740345,cost 0.0006342303356727239, iterations 538 m 2.016129774125053, b 2.941766397458944,cost 0.0006256821000607561, iterations 539 m 2.0160207058149564, b 2.9421601686655796,cost 0.000617249078635131, iterations 540 m 2.015912375016506, b 2.9425512772211615,cost 0.0006089297185246581, iterations 541 m 2.015804776742704, b 2.9429397411303344,cost 0.0006007224877879873, iterations 542 m 2.015697906040274, b 2.9433255782759966,cost 0.0005926258751313963, iterations 543 m 2.015591757989434, b 2.943708806420124,cost 0.0005846383896305382, iterations 544 m 2.0154863277036683, b 2.9440894432045868,cost 0.0005767585604560989, iterations 545 m 2.015381610329504, b 2.944467506151963,cost 0.000568984936602594, iterations 546 m 2.0152776010462867, b 2.944843012666344,cost 0.0005613160866215222, iterations 547 m 2.0151742950659592, b 2.9452159800341358,cost 0.0005537505983574642, iterations 548 m 2.0150716876328407, b 2.9455864254248554,cost 0.0005462870786884007, iterations 549 m 2.0149697740234083, b 2.9459543658919203,cost 0.0005389241532687948, iterations 550 m 2.0148685495460783, b 2.9463198183734343,cost 0.0005316604662768579, iterations 551 m 2.0147680095409917, b 2.9466827996929674,cost 0.0005244946801646624, iterations 552 m 2.014668149379799, b 2.9470433265603297,cost 0.000517425475411937, iterations 553 m 2.014568964465448, b 2.9474014155723407,cost 0.0005104515502830471, iterations 554 m 2.01447045023197, b 2.947757083213593,cost 0.0005035716205873444, iterations 555 m 2.014372602144272, b 2.948110345857213,cost 0.000496784419442649, iterations 556 m 2.014275415697927, b 2.948461219765612,cost 0.0004900886970419901, iterations 557 m 2.0141788864189656, b 2.9488097210912363,cost 0.00048348322042341514, iterations 558 m 2.0140830098636724, b 2.949155865877311,cost 0.0004769667732430449, iterations 559 m 2.013987781618379, b 2.949499670058578,cost 0.0004705381555509425, iterations 560 m 2.013893197299263, b 2.949841149462029,cost 0.00046419618357038464, iterations 561 m 2.0137992525521438, b 2.9501803198076364,cost 0.0004579396894795986, iterations 562 m 2.013705943052284, b 2.9505171967090735,cost 0.0004517675211969252, iterations 563 m 2.0136132645041904, b 2.9508517956744367,cost 0.00044567854216863575, iterations 564 m 2.013521212641414, b 2.9511841321069565,cost 0.0004396716311595452, iterations 565 m 2.013429783226357, b 2.9515142213057084,cost 0.0004337456820466517, iterations 566 m 2.013338972050075, b 2.951842078466317,cost 0.0004278996036154244, iterations 567 m 2.013248774932084, b 2.9521677186816553,cost 0.00042213231935885574, iterations 568 m 2.0131591877201687, b 2.952491156942539,cost 0.0004164427672792113, iterations 569 m 2.0130702062901897, b 2.9528124081384175,cost 0.0004108298996925079, iterations 570 m 2.012981826545896, b 2.953131487058058,cost 0.00040529268303557517, iterations 571 m 2.012894044418735, b 2.953448408390228,cost 0.0003998300976757864, iterations 572 m 2.012806855867664, b 2.953763186724371,cost 0.00039444113772314727, iterations 573 m 2.0127202568789673, b 2.9540758365512763,cost 0.00038912481084525023, iterations 574 m 2.0126342434660685, b 2.954386372263749,cost 0.000383880138084475, iterations 575 m 2.0125488116693484, b 2.954694808157271,cost 0.00037870615367767294, iterations 576 m 2.0124639575559624, b 2.955001158430658,cost 0.00037360190487835897, iterations 577 m 2.01237967721966, b 2.9553054371867162,cost 0.00036856645178136413, iterations 578 m 2.0122959667806035, b 2.9556076584328883,cost 0.0003635988671495837, iterations 579 m 2.0122128223851914, b 2.9559078360819004,cost 0.00035869823624342154, iterations 580 m 2.012130240205879, b 2.9562059839524015,cost 0.00035386365665214655, iterations 581 m 2.0120482164410043, b 2.9565021157696,cost 0.00034909423812794254, iterations 582 m 2.0119667473146103, b 2.956796245165896,cost 0.0003443891024217891, iterations 583 m 2.0118858290762742, b 2.957088385681507,cost 0.00033974738312184116, iterations 584 m 2.0118054580009326, b 2.957378550765094,cost 0.00033516822549390334, iterations 585 m 2.0117256303887108, b 2.957666753774378,cost 0.0003306507863239103, iterations 586 m 2.011646342564753, b 2.9579530079767578,cost 0.00032619423376278655, iterations 587 m 2.011567590879051, b 2.9582373265499173,cost 0.0003217977471732382, iterations 588 m 2.0114893717062787, b 2.9585197225824347,cost 0.00031746051697859935, iterations 589 m 2.0114116814456238, b 2.9588002090743837,cost 0.0003131817445138089, iterations 590 m 2.0113345165206233, b 2.9590787989379335,cost 0.0003089606418782307, iterations 591 m 2.0112578733789968, b 2.959355504997941,cost 0.0003047964317907427, iterations 592 m 2.0111817484924854, b 2.959630339992544,cost 0.00030068834744646864, iterations 593 m 2.0111061383566864, b 2.9599033165737443,cost 0.0002966356323756686, iterations 594 m 2.011031039490895, b 2.9601744473079923,cost 0.00029263754030432737, iterations 595 m 2.010956448437942, b 2.9604437446767653,cost 0.00028869333501688356, iterations 596 m 2.0108823617640357, b 2.960711221077142,cost 0.0002848022902204955, iterations 597 m 2.010808776058603, b 2.9609768888223718,cost 0.0002809636894114613, iterations 598 m 2.010735687934133, b 2.9612407601424446,cost 0.00027717682574315497, iterations 599 m 2.0106630940260213, b 2.9615028471846507,cost 0.0002734410018959369, iterations 600 m 2.010590990992414, b 2.961763162014142,cost 0.00026975552994875626, iterations 601 m 2.010519375514055, b 2.962021716614487,cost 0.00026611973125239896, iterations 602 m 2.0104482442941323, b 2.9622785228882207,cost 0.00026253293630458177, iterations 603 m 2.0103775940581277, b 2.962533592657396,cost 0.0002589944846266995, iterations 604 m 2.010307421553664, b 2.962786937664125,cost 0.0002555037246420853, iterations 605 m 2.010237723550357, b 2.96303856957112,cost 0.00025206001355618316, iterations 606 m 2.0101684968396656, b 2.9632884999622324,cost 0.00024866271723805207, iterations 607 m 2.0100997382347447, b 2.9635367403429833,cost 0.0002453112101036409, iterations 608 m 2.010031444570299, b 2.9637833021410946,cost 0.00024200487500065815, iterations 609 m 2.0099636127024363, b 2.964028196707015,cost 0.00023874310309476805, iterations 610 m 2.0098962395085227, b 2.964271435314442,cost 0.00023552529375770598, iterations 611 m 2.0098293218870396, b 2.9645130291608415,cost 0.00023235085445643148, iterations 612 m 2.0097628567574413, b 2.964752989367963,cost 0.00022921920064422412, iterations 613 m 2.009696841060012, b 2.9649913269823513,cost 0.00022612975565288232, iterations 614 m 2.0096312717557243, b 2.965228052975856,cost 0.0002230819505867139, iterations 615 m 2.0095661458261027, b 2.965463178246135,cost 0.00022007522421753345, iterations 616 m 2.009501460273081, b 2.9656967136171573,cost 0.00021710902288159923, iterations 617 m 2.0094372121188666, b 2.9659286698397014,cost 0.00021418280037743205, iterations 618 m 2.009373398405801, b 2.9661590575918493,cost 0.00021129601786532674, iterations 619 m 2.0093100161962267, b 2.9663878874794793,cost 0.0002084481437681981, iterations 620 m 2.0092470625723493, b 2.966615170036753,cost 0.00020563865367353293, iterations 621 m 2.009184534636105, b 2.966840915726601,cost 0.00020286703023696335, iterations 622 m 2.0091224295090266, b 2.9670651349412043,cost 0.00020013276308697344, iterations 623 m 2.0090607443321105, b 2.967287838002473,cost 0.0001974353487308404, iterations 624 m 2.008999476265685, b 2.9675090351625206,cost 0.0001947742904620034, iterations 625 m 2.008938622489281, b 2.9677287366041374,cost 0.00019214909826862568, iterations 626 m 2.008878180201501, b 2.9679469524412583,cost 0.00018955928874321253, iterations 627 m 2.0088181466198898, b 2.9681636927194277,cost 0.00018700438499378688, iterations 628 m 2.008758518980807, b 2.968378967416264,cost 0.00018448391655591625, iterations 629 m 2.0086992945393, b 2.9685927864419166,cost 0.00018199741930620568, iterations 630 m 2.008640470568978, b 2.968805159639524,cost 0.0001795444353767189, iterations 631 m 2.008582044361885, b 2.9690160967856656,cost 0.00017712451307074975, iterations 632 m 2.0085240132283757, b 2.9692256075908126,cost 0.0001747372067796095, iterations 633 m 2.008466374496993, b 2.969433701699775,cost 0.0001723820769005932, iterations 634 m 2.008409125514343, b 2.969640388692145,cost 0.00017005868975597006, iterations 635 m 2.0083522636449747, b 2.9698456780827382,cost 0.00016776661751323582, iterations 636 m 2.0082957862712574, b 2.9700495793220316,cost 0.00016550543810622877, iterations 637 m 2.0082396907932605, b 2.9702521017965995,cost 0.00016327473515744548, iterations 638 m 2.008183974628634, b 2.970453254829544,cost 0.00016107409790137666, iterations 639 m 2.00812863521249, b 2.9706530476809263,cost 0.00015890312110888806, iterations 640 m 2.008073669997283, b 2.9708514895481906,cost 0.00015676140501251533, iterations 641 m 2.0080190764526957, b 2.971048589566589,cost 0.0001546485552329648, iterations 642 m 2.007964852065519, b 2.971244356809602,cost 0.00015256418270641583, iterations 643 m 2.007910994339538, b 2.9714388002893557,cost 0.00015050790361290472, iterations 644 m 2.0078575007954185, b 2.971631928957037,cost 0.0001484793393056058, iterations 645 m 2.00780436897059, b 2.9718237517033055,cost 0.00014647811624120773, iterations 646 m 2.0077515964191335, b 2.9720142773587024,cost 0.00014450386591099296, iterations 647 m 2.00769918071167, b 2.9722035146940584,cost 0.00014255622477311454, iterations 648 m 2.0076471194352483, b 2.9723914724208957,cost 0.00014063483418554784, iterations 649 m 2.0075954101932316, b 2.9725781591918303,cost 0.00013873934034008785, iterations 650 m 2.00754405060519, b 2.9727635836009694,cost 0.0001368693941972331, iterations 651 m 2.00749303830679, b 2.972947754184308,cost 0.00013502465142184415, iterations 652 m 2.0074423709496854, b 2.973130679420121,cost 0.0001332047723197932, iterations 653 m 2.007392046201409, b 2.9733123677293536,cost 0.00013140942177539787, iterations 654 m 2.0073420617452666, b 2.97349282747601,cost 0.0001296382691896724, iterations 655 m 2.007292415280228, b 2.973672066967538,cost 0.0001278909884195289, iterations 656 m 2.007243104520823, b 2.973850094455209,cost 0.00012616725771766484, iterations 657 m 2.0071941271970357, b 2.974026918134502,cost 0.00012446675967330584, iterations 658 m 2.0071454810542, b 2.9742025461454777,cost 0.0001227891811537943, iterations 659 m 2.0070971638528947, b 2.9743769865731546,cost 0.00012113421324692149, iterations 660 m 2.0070491733688427, b 2.974550247447881,cost 0.00011950155120403034, iterations 661 m 2.0070015073928062, b 2.9747223367457045,cost 0.00011789089438390245, iterations 662 m 2.006954163730487, b 2.9748932623887394,cost 0.00011630194619738707, iterations 663 m 2.006907140202424, b 2.9750630322455316,cost 0.00011473441405282892, iterations 664 m 2.006860434643894, b 2.9752316541314197,cost 0.00011318800930212297, iterations 665 m 2.00681404490481, b 2.9753991358088956,cost 0.00011166244718762337, iterations 666 m 2.0067679688496263, b 2.9755654849879627,cost 0.00011015744678970225, iterations 667 m 2.006722204357235, b 2.975730709326489,cost 0.00010867273097494334, iterations 668 m 2.006676749320873, b 2.975894816430561,cost 0.00010720802634521477, iterations 669 m 2.0066316016480217, b 2.976057813854834,cost 0.00010576306318727842, iterations 670 m 2.006586759260312, b 2.976219709102878,cost 0.00010433757542310454, iterations 671 m 2.0065422200934293, b 2.9763805096275258,cost 0.00010293130056091799, iterations 672 m 2.0064979820970175, b 2.9765402228312134,cost 0.00010154397964680622, iterations 673 m 2.006454043234584, b 2.976698856066323,cost 0.00010017535721710157, iterations 674 m 2.0064104014834085, b 2.97685641663552,cost 9.882518125130531e-05, iterations 675 m 2.0063670548344463, b 2.97701291179209,cost 9.749320312568032e-05, iterations 676 m 2.006324001292239, b 2.977168348740273,cost 9.617917756744018e-05, iterations 677 m 2.0062812388748212, b 2.9773227346355933,cost 9.488286260965831e-05, iterations 678 m 2.0062387656136287, b 2.977476076585191,cost 9.36040195466541e-05, iterations 679 m 2.0061965795534094, b 2.977628381648148,cost 9.234241289003594e-05, iterations 680 m 2.0061546787521314, b 2.977779656835813,cost 9.109781032537573e-05, iterations 681 m 2.006113061280896, b 2.9779299091121247,cost 8.986998266940813e-05, iterations 682 m 2.0060717252238467, b 2.978079145393932,cost 8.865870382780499e-05, iterations 683 m 2.006030668678082, b 2.9782273725513133,cost 8.746375075360189e-05, iterations 684 m 2.0059898897535686, b 2.978374597407891,cost 8.628490340603758e-05, iterations 685 m 2.0059493865730516, b 2.9785208267411467,cost 8.51219447101468e-05, iterations 686 m 2.0059091572719714, b 2.9786660672827345,cost 8.397466051668617e-05, iterations 687 m 2.0058691999983758, b 2.9788103257187886,cost 8.284283956276454e-05, iterations 688 m 2.005829512912836, b 2.9789536086902317,cost 8.172627343289656e-05, iterations 689 m 2.0057900941883604, b 2.979095922793082,cost 8.06247565206699e-05, iterations 690 m 2.005750942010312, b 2.9792372745787556,cost 7.953808599084016e-05, iterations 691 m 2.005712054576324, b 2.979377670554368,cost 7.8466061742029e-05, iterations 692 m 2.005673430096217, b 2.9795171171830344,cost 7.740848636982252e-05, iterations 693 m 2.0056350667919176, b 2.979655620884167,cost 7.636516513046073e-05, iterations 694 m 2.005596962897374, b 2.9797931880337702,cost 7.533590590496835e-05, iterations 695 m 2.005559116658477, b 2.9799298249647346,cost 7.432051916375947e-05, iterations 696 m 2.005521526332979, b 2.980065537967128,cost 7.331881793175418e-05, iterations 697 m 2.005484190190413, b 2.980200333288485,cost 7.233061775396131e-05, iterations 698 m 2.005447106512013, b 2.980334217134096,cost 7.135573666148702e-05, iterations 699 m 2.005410273590636, b 2.980467195667291,cost 7.039399513805042e-05, iterations 700 m 2.005373689730681, b 2.9805992750097228,cost 6.944521608686491e-05, iterations 701 m 2.0053373532480148, b 2.9807304612416523,cost 6.850922479815033e-05, iterations 702 m 2.00530126246989, b 2.9808607604022246,cost 6.758584891681455e-05, iterations 703 m 2.0052654157348715, b 2.980990178489749,cost 6.667491841084407e-05, iterations 704 m 2.0052298113927582, b 2.9811187214619745,cost 6.577626553991626e-05, iterations 705 m 2.0051944478045076, b 2.9812463952363646,cost 6.488972482452612e-05, iterations 706 m 2.0051593233421605, b 2.9813732056903692,cost 6.401513301552271e-05, iterations 707 m 2.0051244363887655, b 2.9814991586616952,cost 6.315232906407434e-05, iterations 708 m 2.0050897853383054, b 2.9816242599485756,cost 6.230115409195747e-05, iterations 709 m 2.005055368595622, b 2.981748515310036,cost 6.146145136232946e-05, iterations 710 m 2.005021184576344, b 2.98187193046616,cost 6.063306625088438e-05, iterations 711 m 2.0049872317068136, b 2.9819945110983523,cost 5.9815846217351084e-05, iterations 712 m 2.004953508424013, b 2.9821162628496007,cost 5.900964077740941e-05, iterations 713 m 2.004920013175495, b 2.982237191324735,cost 5.8214301474994934e-05, iterations 714 m 2.0048867444193093, b 2.982357302090686,cost 5.742968185495342e-05, iterations 715 m 2.0048537006239306, b 2.982476600676742,cost 5.665563743606974e-05, iterations 716 m 2.004820880268192, b 2.9825950925748006,cost 5.589202568445709e-05, iterations 717 m 2.0047882818412117, b 2.9827127832396254,cost 5.513870598732138e-05, iterations 718 m 2.0047559038423235, b 2.982829678089095,cost 5.439553962707826e-05, iterations 719 m 2.00472374478101, b 2.9829457825044523,cost 5.366238975578076e-05, iterations 720 m 2.004691803176831, b 2.983061101830553,cost 5.293912136994514e-05, iterations 721 m 2.0046600775593593, b 2.983175641376111,cost 5.2225601285660614e-05, iterations 722 m 2.004628566468108, b 2.9832894064139435,cost 5.1521698114126515e-05, iterations 723 m 2.0045972684524673, b 2.9834024021812127,cost 5.082728223737308e-05, iterations 724 m 2.004566182071636, b 2.983514633879668,cost 5.014222578446906e-05, iterations 725 m 2.004535305894556, b 2.9836261066758847,cost 4.9466402607932226e-05, iterations 726 m 2.0045046384998453, b 2.9837368257015027,cost 4.879968826050252e-05, iterations 727 m 2.004474178475733, b 2.983846796053461,cost 4.814195997224322e-05, iterations 728 m 2.004443924419995, b 2.9839560227942346,cost 4.749309662793733e-05, iterations 729 m 2.004413874939889, b 2.9840645109520656,cost 4.685297874476037e-05, iterations 730 m 2.00438402865209, b 2.984172265521196,cost 4.6221488450300785e-05, iterations 731 m 2.004354384182627, b 2.9842792914620975,cost 4.5598509460834874e-05, iterations 732 m 2.004324940166819, b 2.9843855937016985,cost 4.498392705992401e-05, iterations 733 m 2.004295695249215, b 2.9844911771336124,cost 4.437762807731468e-05, iterations 734 m 2.0042666480835267, b 2.9845960466183623,cost 4.377950086805282e-05, iterations 735 m 2.0042377973325713, b 2.9847002069836046,cost 4.318943529195429e-05, iterations 736 m 2.0042091416682073, b 2.9848036630243517,cost 4.2607322693331814e-05, iterations 737 m 2.004180679771274, b 2.9849064195031927,cost 4.203305588096269e-05, iterations 738 m 2.00415241033153, b 2.985008481150512,cost 4.146652910835078e-05, iterations 739 m 2.004124332047595, b 2.9851098526647077,cost 4.090763805427779e-05, iterations 740 m 2.004096443626888, b 2.985210538712408,cost 4.035627980358285e-05, iterations 741 m 2.0040687437855684, b 2.985310543928685,cost 3.9812352828192685e-05, iterations 742 m 2.004041231248476, b 2.9854098729172693,cost 3.927575696845026e-05, iterations 743 m 2.004013904749074, b 2.9855085302507613,cost 3.874639341467614e-05, iterations 744 m 2.00398676302939, b 2.985606520470842,cost 3.8224164688941783e-05, iterations 745 m 2.0039598048399574, b 2.9857038480884817,cost 3.770897462713656e-05, iterations 746 m 2.003933028939758, b 2.9858005175841473,cost 3.720072836128502e-05, iterations 747 m 2.0039064340961668, b 2.9858965334080105,cost 3.669933230202339e-05, iterations 748 m 2.0038800190848924, b 2.9859918999801502,cost 3.62046941214161e-05, iterations 749 m 2.0038537826899216, b 2.9860866216907573,cost 3.5716722735943035e-05, iterations 750 m 2.0038277237034654, b 2.9861807029003367,cost 3.5235328289697616e-05, iterations 751 m 2.0038018409259, b 2.9862741479399073,cost 3.476042213787072e-05, iterations 752 m 2.003776133165715, b 2.986366961111203,cost 3.429191683042515e-05, iterations 753 m 2.003750599239456, b 2.986459146686869,cost 3.382972609597815e-05, iterations 754 m 2.003725237971671, b 2.9865507089106593,cost 3.33737648259319e-05, iterations 755 m 2.003700048194857, b 2.9866416519976324,cost 3.2923949058785414e-05, iterations 756 m 2.003675028749404, b 2.986731980134344,cost 3.248019596467445e-05, iterations 757 m 2.0036501784835448, b 2.986821697479042,cost 3.204242383015166e-05, iterations 758 m 2.0036254962532998, b 2.986910808161855,cost 3.1610552043082895e-05, iterations 759 m 2.003600980922425, b 2.986999316284985,cost 3.118450107785906e-05, iterations 760 m 2.00357663136236, b 2.9870872259228944,cost 3.0764192480712e-05, iterations 761 m 2.0035524464521743, b 2.9871745411224957,cost 3.0349548855289702e-05, iterations 762 m 2.0035284250785184, b 2.987261265903335,cost 2.9940493848392608e-05, iterations 763 m 2.00350456613557, b 2.9873474042577794,cost 2.9536952135924608e-05, iterations 764 m 2.003480868524986, b 2.9874329601512,cost 2.9138849409024042e-05, iterations 765 m 2.003457331155848, b 2.9875179375221537,cost 2.8746112360347164e-05, iterations 766 m 2.0034339529446163, b 2.9876023402825655,cost 2.8358668670635303e-05, iterations 767 m 2.003410732815077, b 2.987686172317909,cost 2.7976446995315576e-05, iterations 768 m 2.003387669698294, b 2.9877694374873833,cost 2.7599376951437642e-05, iterations 769 m 2.003364762532559, b 2.987852139624093,cost 2.7227389104667664e-05, iterations 770 m 2.0033420102633417, b 2.987934282535222,cost 2.68604149565173e-05, iterations 771 m 2.0033194118432447, b 2.988015870002212,cost 2.6498386931731274e-05, iterations 772 m 2.0032969662319515, b 2.988096905780934,cost 2.6141238365837623e-05, iterations 773 m 2.0032746723961807, b 2.9881773936018625,cost 2.5788903492889327e-05, iterations 774 m 2.0032525293096377, b 2.9882573371702463,cost 2.544131743332682e-05, iterations 775 m 2.0032305359529676, b 2.98833674016628,cost 2.5098416182051792e-05, iterations 776 m 2.0032086913137084, b 2.988415606245273,cost 2.4760136596630823e-05, iterations 777 m 2.003186994386244, b 2.988493939037817,cost 2.442641638568083e-05, iterations 778 m 2.0031654441717586, b 2.988571742149955,cost 2.4097194097381325e-05, iterations 779 m 2.0031440396781903, b 2.9886490191633457,cost 2.377240910816048e-05, iterations 780 m 2.003122779920185, b 2.988725773635429,cost 2.3452001611566463e-05, iterations 781 m 2.003101663919052, b 2.9888020090995897,cost 2.3135912607186477e-05, iterations 782 m 2.0030806907027183, b 2.9888777290653197,cost 2.282408388985804e-05, iterations 783 m 2.003059859305684, b 2.9889529370183805,cost 2.2516458038899648e-05, iterations 784 m 2.0030391687689773, b 2.989027636420963,cost 2.221297840754365e-05, iterations 785 m 2.0030186181401115, b 2.9891018307118475,cost 2.1913589112532525e-05, iterations 786 m 2.0029982064730407, b 2.9891755233065602,cost 2.16182350238027e-05, iterations 787 m 2.0029779328281156, b 2.989248717597533,cost 2.1326861754339324e-05, iterations 788 m 2.0029577962720406, b 2.9893214169542577,cost 2.103941565016654e-05, iterations 789 m 2.002937795877832, b 2.9893936247234425,cost 2.0755843780448178e-05, iterations 790 m 2.002917930724773, b 2.9894653442291648,cost 2.047609392778413e-05, iterations 791 m 2.002898199898373, b 2.9895365787730253,cost 2.020011457854837e-05, iterations 792 m 2.002878602490326, b 2.9896073316342995,cost 1.9927854913414757e-05, iterations 793 m 2.002859137598467, b 2.9896776060700883,cost 1.9659264798030477e-05, iterations 794 m 2.002839804326731, b 2.9897474053154687,cost 1.9394294773731938e-05, iterations 795 m 2.002820601785113, b 2.9898167325836424,cost 1.9132896048495855e-05, iterations 796 m 2.002801529089626, b 2.9898855910660833,cost 1.8875020487894745e-05, iterations 797 m 2.0027825853622607, b 2.989953983932685,cost 1.8620620606280343e-05, iterations 798 m 2.0027637697309437, b 2.9900219143319062,cost 1.836964955801447e-05, iterations 799 m 2.0027450813294996, b 2.9900893853909167,cost 1.812206112885931e-05, iterations 800 m 2.00272651929761, b 2.99015640021574,cost 1.78778097274541e-05, iterations 801 m 2.0027080827807726, b 2.9902229618913974,cost 1.7636850376915798e-05, iterations 802 m 2.002689770930265, b 2.9902890734820486,cost 1.7399138706568567e-05, iterations 803 m 2.0026715829031025, b 2.990354738031135,cost 1.7164630943788163e-05, iterations 804 m 2.0026535178620013, b 2.990419958561517,cost 1.6933283905892294e-05, iterations 805 m 2.0026355749753386, b 2.990484738075616,cost 1.670505499224767e-05, iterations 806 m 2.0026177534171157, b 2.9905490795555507,cost 1.6479902176378674e-05, iterations 807 m 2.0026000523669185, b 2.9906129859632746,cost 1.6257783998267362e-05, iterations 808 m 2.0025824710098816, b 2.990676460240713,cost 1.6038659556672334e-05, iterations 809 m 2.002565008536648, b 2.990739505309899,cost 1.582248850165196e-05, iterations 810 m 2.002547664143335, b 2.990802124073105,cost 1.5609231027080112e-05, iterations 811 m 2.0025304370314947, b 2.9908643194129807,cost 1.539884786337307e-05, iterations 812 m 2.002513326408079, b 2.990926094192682,cost 1.5191300270195693e-05, iterations 813 m 2.0024963314854025, b 2.990987451256005,cost 1.498655002937682e-05, iterations 814 m 2.0024794514811046, b 2.9910483934275165,cost 1.4784559437861915e-05, iterations 815 m 2.0024626856181165, b 2.9911089235126833,cost 1.4585291300739254e-05, iterations 816 m 2.002446033124623, b 2.991169044298002,cost 1.4388708924439143e-05, iterations 817 m 2.0024294932340285, b 2.9912287585511272,cost 1.419477610994067e-05, iterations 818 m 2.002413065184921, b 2.9912880690209986,cost 1.4003457146115877e-05, iterations 819 m 2.002396748221036, b 2.991346978437968,cost 1.3814716803162985e-05, iterations 820 m 2.002380541591224, b 2.991405489513925,cost 1.3628520326105574e-05, iterations 821 m 2.002364444549414, b 2.991463604942421,cost 1.3444833428399817e-05, iterations 822 m 2.0023484563545813, b 2.9915213273987944,cost 1.3263622285624917e-05, iterations 823 m 2.0023325762707103, b 2.9915786595402927,cost 1.3084853529249352e-05, iterations 824 m 2.0023168035667624, b 2.9916356040061958,cost 1.2908494240475517e-05, iterations 825 m 2.0023011375166435, b 2.9916921634179365,cost 1.2734511944206222e-05, iterations 826 m 2.002285577399168, b 2.991748340379222,cost 1.2562874603041377e-05, iterations 827 m 2.0022701224980275, b 2.991804137476153,cost 1.2393550611374503e-05, iterations 828 m 2.002254772101757, b 2.991859557277343,cost 1.2226508789604192e-05, iterations 829 m 2.0022395255037027, b 2.9919146023340386,cost 1.206171837835129e-05, iterations 830 m 2.0022243820019887, b 2.991969275180233,cost 1.1899149032822215e-05, iterations 831 m 2.0022093408994857, b 2.992023578332785,cost 1.1738770817225678e-05, iterations 832 m 2.002194401503778, b 2.9920775142915352,cost 1.158055419922613e-05, iterations 833 m 2.0021795631271315, b 2.9921310855394205,cost 1.1424470044556742e-05, iterations 834 m 2.002164825086463, b 2.992184294542588,cost 1.1270489611594857e-05, iterations 835 m 2.0021501867033087, b 2.992237143750509,cost 1.1118584546126682e-05, iterations 836 m 2.002135647303792, b 2.9922896355960917,cost 1.09687268760896e-05, iterations 837 m 2.0021212062185927, b 2.9923417724957933,cost 1.082088900642614e-05, iterations 838 m 2.0021068627829166, b 2.99239355684973,cost 1.0675043714022198e-05, iterations 839 m 2.0020926163364656, b 2.992444991041791,cost 1.0531164142669102e-05, iterations 840 m 2.0020784662234057, b 2.9924960774397436,cost 1.0389223798133823e-05, iterations 841 m 2.002064411792338, b 2.992546818395345,cost 1.0249196543276394e-05, iterations 842 m 2.0020504523962677, b 2.9925972162444507,cost 1.0111056593234568e-05, iterations 843 m 2.002036587392576, b 2.9926472733071208,cost 9.974778510680268e-06, iterations 844 m 2.002022816142988, b 2.9926969918877266,cost 9.840337201124046e-06, iterations 845 m 2.002009138013546, b 2.992746374275059,cost 9.707707908319728e-06, iterations 846 m 2.0019955523745785, b 2.992795422742431,cost 9.576866209674455e-06, iterations 847 m 2.0019820586006722, b 2.9928441395477843,cost 9.44778801176867e-06, iterations 848 m 2.0019686560706424, b 2.992892526933792,cost 9.320449545926367e-06, iterations 849 m 2.001955344167505, b 2.992940587127963,cost 9.194827363814396e-06, iterations 850 m 2.001942122278447, b 2.992988322342744,cost 9.070898333152398e-06, iterations 851 m 2.001928989794801, b 2.9930357347756207,cost 8.948639633428303e-06, iterations 852 m 2.001915946112014, b 2.9930828266092195,cost 8.828028751719827e-06, iterations 853 m 2.0019029906296213, b 2.993129600011409,cost 8.709043478523298e-06, iterations 854 m 2.001890122751219, b 2.9931760571353982,cost 8.591661903687762e-06, iterations 855 m 2.001877341884435, b 2.993222200119836,cost 8.475862412367906e-06, iterations 856 m 2.0018646474409034, b 2.9932680310889106,cost 8.361623681042194e-06, iterations 857 m 2.0018520388362364, b 2.9933135521524457,cost 8.248924673595522e-06, iterations 858 m 2.001839515489999, b 2.9933587654059997,cost 8.137744637430965e-06, iterations 859 m 2.0018270768256796, b 2.9934036729309597,cost 8.028063099672468e-06, iterations 860 m 2.0018147222706655, b 2.9934482767946395,cost 7.91985986336565e-06, iterations 861 m 2.001802451256216, b 2.993492579050374,cost 7.813115003779874e-06, iterations 862 m 2.001790263217436, b 2.9935365817376134,cost 7.707808864734853e-06, iterations 863 m 2.0017781575932507, b 2.9935802868820165,cost 7.603922054971427e-06, iterations 864 m 2.0017661338263784, b 2.9936236964955456,cost 7.501435444595069e-06, iterations 865 m 2.0017541913633066, b 2.993666812576558,cost 7.400330161543023e-06, iterations 866 m 2.0017423296542645, b 2.993709637109899,cost 7.300587588114491e-06, iterations 867 m 2.0017305481532004, b 2.9937521720669915,cost 7.202189357537416e-06, iterations 868 m 2.001718846317753, b 2.9937944194059276,cost 7.105117350589071e-06, iterations 869 m 2.0017072236092304, b 2.99383638107156,cost 7.009353692268486e-06, iterations 870 m 2.001695679492582, b 2.99387805899559,cost 6.914880748481305e-06, iterations 871 m 2.0016842134363753, b 2.9939194550966564,cost 6.821681122820404e-06, iterations 872 m 2.0016728249127715, b 2.993960571280425,cost 6.729737653348348e-06, iterations 873 m 2.001661513397501, b 2.9940014094396754,cost 6.639033409429171e-06, iterations 874 m 2.0016502783698393, b 2.994041971454388,cost 6.549551688628761e-06, iterations 875 m 2.0016391193125833, b 2.994082259191832,cost 6.461276013629713e-06, iterations 876 m 2.001628035712027, b 2.994122274506649,cost 6.374190129193922e-06, iterations 877 m 2.001617027057937, b 2.9941620192409397,cost 6.288277999175921e-06, iterations 878 m 2.001606092843532, b 2.99420149522435,cost 6.20352380356719e-06, iterations 879 m 2.0015952325654562, b 2.994240704274152,cost 6.1199119355835424e-06, iterations 880 m 2.0015844457237573, b 2.994279648195331,cost 6.037426998790107e-06, iterations 881 m 2.0015737318218645, b 2.994318328780667,cost 5.956053804269762e-06, iterations 882 m 2.001563090366564, b 2.9943567478108166,cost 5.875777367818134e-06, iterations 883 m 2.001552520867978, b 2.9943949070543963,cost 5.796582907195752e-06, iterations 884 m 2.00154202283954, b 2.994432808268063,cost 5.718455839398811e-06, iterations 885 m 2.001531595797975, b 2.9944704531965955,cost 5.641381777972303e-06, iterations 886 m 2.0015212392632744, b 2.9945078435729746,cost 5.565346530359058e-06, iterations 887 m 2.001510952758677, b 2.994544981118463,cost 5.490336095304924e-06, iterations 888 m 2.0015007358106436, b 2.994581867542683,cost 5.416336660255655e-06, iterations 889 m 2.0014905879488385, b 2.994618504543699,cost 5.343334598824987e-06, iterations 890 m 2.0014805087061056, b 2.9946548938080904,cost 5.2713164682891794e-06, iterations 891 m 2.0014704976184485, b 2.9946910370110342,cost 5.200269007102394e-06, iterations 892 m 2.001460554225007, b 2.994726935816379,cost 5.130179132465619e-06, iterations 893 m 2.0014506780680383, b 2.994762591876723,cost 5.061033937905864e-06, iterations 894 m 2.0014408686928946, b 2.994798006833489,cost 4.9928206909089275e-06, iterations 895 m 2.0014311256480024, b 2.9948331823170022,cost 4.925526830564682e-06, iterations 896 m 2.001421448484841, b 2.994868119946562,cost 4.859139965268957e-06, iterations 897 m 2.0014118367579234, b 2.9949028213305184,cost 4.793647870429109e-06, iterations 898 m 2.001402290024775, b 2.994937288066347,cost 4.729038486214975e-06, iterations 899 m 2.0013928078459124, b 2.99497152174072,cost 4.6652999153444475e-06, iterations 900 m 2.0013833897848246, b 2.995005523929582,cost 4.602420420886308e-06, iterations 901 m 2.001374035407952, b 2.9950392961982195,cost 4.540388424101286e-06, iterations 902 m 2.0013647442846665, b 2.9950728401013365,cost 4.4791925023135686e-06, iterations 903 m 2.0013555159872527, b 2.9951061571831232,cost 4.41882138679253e-06, iterations 904 m 2.001346350090887, b 2.995139248977328,cost 4.3592639607009965e-06, iterations 905 m 2.0013372461736174, b 2.9951721170073284,cost 4.300509257031129e-06, iterations 906 m 2.0013282038163465, b 2.995204762786201,cost 4.242546456589921e-06, iterations 907 m 2.00131922260281, b 2.9952371878167914,cost 4.185364886006794e-06, iterations 908 m 2.001310302119559, b 2.9952693935917827,cost 4.1289540157661315e-06, iterations 909 m 2.0013014419559387, b 2.995301381593764,cost 4.073303458277813e-06, iterations 910 m 2.001292641704074, b 2.995333153295301,cost 4.018402965944569e-06, iterations 911 m 2.0012839009588457, b 2.995364710159,cost 3.964242429300016e-06, iterations 912 m 2.0012752193178733, b 2.9953960536375783,cost 3.910811875127131e-06, iterations 913 m 2.0012665963814995, b 2.9954271851739303,cost 3.858101464630568e-06, iterations 914 m 2.001258031752768, b 2.995458106201193,cost 3.806101491625026e-06, iterations 915 m 2.001249525037407, b 2.995488818142813,cost 3.7548023807438514e-06, iterations 916 m 2.00124107584381, b 2.9955193224126115,cost 3.704194685681398e-06, iterations 917 m 2.0012326837830203, b 2.9955496204148497,cost 3.654269087449124e-06, iterations 918 m 2.0012243484687096, b 2.995579713544293,cost 3.6050163926584368e-06, iterations 919 m 2.001216069517162, b 2.9956096031862764,cost 3.556427531837839e-06, iterations 920 m 2.0012078465472576, b 2.995639290716766,cost 3.5084935577443596e-06, iterations 921 m 2.0011996791804525, b 2.9956687775024244,cost 3.4612056437374923e-06, iterations 922 m 2.0011915670407623, b 2.9956980649006733,cost 3.4145550821339204e-06, iterations 923 m 2.001183509754746, b 2.9957271542597548,cost 3.3685332826230735e-06, iterations 924 m 2.0011755069514874, b 2.995756046918795,cost 3.3231317706689276e-06, iterations 925 m 2.0011675582625776, b 2.9957847442078647,cost 3.2783421859578503e-06, iterations 926 m 2.0011596633221, b 2.995813247448041,cost 3.234156280862577e-06, iterations 927 m 2.001151821766611, b 2.9958415579514672,cost 3.190565918906144e-06, iterations 928 m 2.0011440332351262, b 2.9958696770214153,cost 3.147563073289251e-06, iterations 929 m 2.0011362973691007, b 2.9958976059523437,cost 3.1051398253943117e-06, iterations 930 m 2.001128613812415, b 2.9959253460299577,cost 3.063288363325432e-06, iterations 931 m 2.0011209822113574, b 2.99595289853127,cost 3.0220009804874173e-06, iterations 932 m 2.0011134022146075, b 2.9959802647246563,cost 2.981270074145261e-06, iterations 933 m 2.0011058734732217, b 2.996007445869917,cost 2.9410881440416633e-06, iterations 934 m 2.001098395640614, b 2.996034443218334,cost 2.9014477910086123e-06, iterations 935 m 2.001090968372544, b 2.996061258012727,cost 2.8623417156006606e-06, iterations 936 m 2.0010835913270975, b 2.9960878914875124,cost 2.8237627167560373e-06, iterations 937 m 2.001076264164673, b 2.99611434486876,cost 2.785703690472957e-06, iterations 938 m 2.0010689865479656, b 2.996140619374249,cost 2.7481576284951465e-06, iterations 939 m 2.001061758141951, b 2.996166716213523,cost 2.711117617025772e-06, iterations 940 m 2.0010545786138696, b 2.996192636587948,cost 2.6745768354550317e-06, iterations 941 m 2.0010474476332134, b 2.9962183816907655,cost 2.638528555097598e-06, iterations 942 m 2.0010403648717077, b 2.9962439527071494,cost 2.602966137960541e-06, iterations 943 m 2.0010333300032985, b 2.9962693508142584,cost 2.567883035519947e-06, iterations 944 m 2.001026342704136, b 2.9962945771812923,cost 2.533272787513089e-06, iterations 945 m 2.0010194026525614, b 2.9963196329695445,cost 2.4991290207465555e-06, iterations 946 m 2.001012509529089, b 2.9963445193324554,cost 2.465445447929513e-06, iterations 947 m 2.0010056630163953, b 2.9963692374156663,cost 2.4322158665106126e-06, iterations 948 m 2.0009988627993014, b 2.9963937883570724,cost 2.3994341575377206e-06, iterations 949 m 2.00099210856476, b 2.996418173286873,cost 2.367094284529485e-06, iterations 950 m 2.000985400001841, b 2.996442393327627,cost 2.3351902923642085e-06, iterations 951 m 2.0009787368017156, b 2.9964664495943008,cost 2.3037163061880194e-06, iterations 952 m 2.0009721186576446, b 2.996490343194323,cost 2.2726665303260228e-06, iterations 953 m 2.0009655452649624, b 2.9965140752276325,cost 2.2420352472159645e-06, iterations 954 m 2.000959016321063, b 2.9965376467867317,cost 2.211816816363361e-06, iterations 955 m 2.0009525315253875, b 2.996561058956735,cost 2.1820056732919585e-06, iterations 956 m 2.0009460905794088, b 2.996584312815419,cost 2.152596328526005e-06, iterations 957 m 2.0009396931866186, b 2.996607409433273,cost 2.123583366580201e-06, iterations 958 m 2.0009333390525135, b 2.996630349873548,cost 2.0949614449565113e-06, iterations 959 m 2.0009270278845817, b 2.9966531351923047,cost 2.0667252931652652e-06, iterations 960 m 2.0009207593922893, b 2.9966757664384627,cost 2.0388697117491257e-06, iterations 961 m 2.0009145332870664, b 2.9966982446538495,cost 2.011389571337142e-06, iterations 962 m 2.0009083492822954, b 2.9967205708732476,cost 1.9842798116862745e-06, iterations 963 m 2.000902207093296, b 2.9967427461244425,cost 1.957535440758225e-06, iterations 964 m 2.0008961064373127, b 2.9967647714282695,cost 1.931151533800326e-06, iterations 965 m 2.000890047033503, b 2.996786647798661,cost 1.9051232324331423e-06, iterations 966 m 2.0008840286029224, b 2.9968083762426945,cost 1.8794457437598794e-06, iterations 967 m 2.000878050868513, b 2.996829957760636,cost 1.8541143394824397e-06, iterations 968 m 2.000872113555091, b 2.996851393345989,cost 1.8291243550340098e-06, iterations 969 m 2.0008662163893325, b 2.9968726839855386,cost 1.8044711887130977e-06, iterations 970 m 2.0008603590997613, b 2.9968938306593973,cost 1.780150300846419e-06, iterations 971 m 2.000854541416739, b 2.99691483434105,cost 1.756157212940604e-06, iterations 972 m 2.000848763072448, b 2.9969356959973994,cost 1.7324875068678e-06, iterations 973 m 2.0008430238008827, b 2.9969564165888096,cost 1.7091368240464215e-06, iterations 974 m 2.000837323337837, b 2.9969769970691513,cost 1.686100864643299e-06, iterations 975 m 2.0008316614208908, b 2.996997438385845,cost 1.6633753867761383e-06, iterations 976 m 2.0008260377893974, b 2.997017741479904,cost 1.6409562057351241e-06, iterations 977 m 2.000820452184474, b 2.9970379072859803,cost 1.6188391932139061e-06, iterations 978 m 2.000814904348988, b 2.9970579367324044,cost 1.5970202765464212e-06, iterations 979 m 2.0008093940275447, b 2.9970778307412296,cost 1.5754954379612495e-06, iterations 980 m 2.0008039209664776, b 2.997097590228275,cost 1.5542607138371e-06, iterations 981 m 2.0007984849138345, b 2.9971172161031663,cost 1.533312193974424e-06, iterations 982 m 2.0007930856193674, b 2.9971367092693795,cost 1.5126460208781186e-06, iterations 983 m 2.00078772283452, b 2.99715607062428,cost 1.492258389040167e-06, iterations 984 m 2.0007823963124176, b 2.9971753010591664,cost 1.472145544248392e-06, iterations 985 m 2.000777105807854, b 2.9971944014593097,cost 1.4523037828881674e-06, iterations 986 m 2.000771851077281, b 2.9972133727039947,cost 1.4327294512659972e-06, iterations 987 m 2.0007666318787978, b 2.9972322156665614,cost 1.4134189449259868e-06, iterations 988 m 2.0007614479721396, b 2.997250931214443,cost 1.3943687079998799e-06, iterations 989 m 2.000756299118665, b 2.997269520209209,cost 1.3755752325471106e-06, iterations 990 m 2.0007511850813473, b 2.9972879835066006,cost 1.357035057899708e-06, iterations 991 m 2.0007461056247626, b 2.9973063219565748,cost 1.338744770039986e-06, iterations 992 m 2.000741060515078, b 2.9973245364033403,cost 1.3207010009638207e-06, iterations 993 m 2.0007360495200426, b 2.997342627685397,cost 1.3029004280577021e-06, iterations 994 m 2.000731072408976, b 2.997360596635576,cost 1.2853397734944237e-06, iterations 995 m 2.0007261289527576, b 2.997378444081076,cost 1.2680158036262176e-06, iterations 996 m 2.000721218923815, b 2.9973961708435017,cost 1.2509253283842492e-06, iterations 997 m 2.000716342096116, b 2.997413777738904,cost 1.2340652007006093e-06, iterations 998 m 2.0007114982451566, b 2.9974312655778137,cost 1.2174323159230584e-06, iterations 999 EXERCSE df = pd.read_csv(\"/kaggle/input/test-scores/test_scores.csv\") df.head() name math cs 0 david 92 98 1 laura 56 68 2 sanjay 88 81 3 wei 70 80 4 jeff 80 83 X = df['math'] y = df['cs'] X.shape, y.shape X = np.array(X).reshape(-1, 1) y = np.array(y).reshape(-1, 1) from sklearn import linear_model reg = linear_model.LinearRegression() reg.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() # The value of M in y = m * x + b reg.coef_ array([[1.01773624]]) # The value of b in y = m * x + b reg.intercept_ array([1.91521931]) gradient_descent(X, y) m [197.836], b [2.796],cost [5199.1], iterations 0 m [-37074.023872], b [-523.137632],cost [1.83406181e+08], iterations 1 m [6984858.79628262], b [98562.37565926],cost [6.50976192e+12], iterations 2 m [-1.31593116e+09], b [-18568920.02703428],cost [2.31055504e+17], iterations 3 m [2.4791841e+11], b [3.49834191e+09],cost [8.20101356e+21], iterations 4 m [-4.67072593e+13], b [-6.59079584e+11],cost [2.91084274e+26], iterations 5 m [8.79954043e+15], b [1.24169081e+14],cost [1.03316564e+31], iterations 6 m [-1.65781322e+18], b [-2.33931697e+16],cost [3.66708661e+35], iterations 7 m [3.12328204e+20], b [4.40721947e+18],cost [1.30158454e+40], iterations 8 m [-5.88419166e+22], b [-8.3031003e+20],cost [4.61980445e+44], iterations 9 m [1.1085682e+25], b [1.56428503e+23],cost [1.6397393e+49], iterations 10 m [-2.08851704e+27], b [-2.94707709e+25],cost [5.82004065e+53], iterations 11 m [3.93471815e+29], b [5.55222558e+27],cost [2.06574747e+58], iterations 12 m [-7.41291866e+31], b [-1.04602655e+30],cost [7.33210107e+62], iterations 13 m [1.39657686e+34], b [1.97069001e+32],cost [2.60243359e+67], iterations 14 m [-2.63111874e+36], b [-3.71273474e+34],cost [9.23699838e+71], iterations 15 m [4.95696729e+38], b [6.99470701e+36],cost [3.27855203e+76], iterations 16 m [-9.33881258e+40], b [-1.31778674e+39],cost [1.16367926e+81], iterations 17 m [1.75941085e+43], b [2.48267997e+41],cost [4.13032765e+85], iterations 18 m [-3.31468965e+45], b [-4.67731207e+43],cost [1.4660059e+90], iterations 19 m [6.2447992e+47], b [8.81194855e+45],cost [5.20339662e+94], iterations 20 m [-1.17650583e+50], b [-1.66015088e+48],cost [1.84687772e+99], iterations 21 m [2.21650998e+52], b [3.12768614e+50],cost [6.55525145e+103], iterations 22 m [-4.17585392e+54], b [-5.89248888e+52],cost [2.32670095e+108], iterations 23 m [7.86721291e+56], b [1.11013138e+55],cost [8.25832139e+112], iterations 24 m [-1.48216485e+59], b [-2.09146203e+57],cost [2.9311834e+117], iterations 25 m [2.79236455e+61], b [3.94026645e+59],cost [1.04038529e+122], iterations 26 m [-5.26075071e+63], b [-7.42337152e+61],cost [3.69271183e+126], iterations 27 m [9.91113356e+65], b [1.39854615e+64],cost [1.31067987e+131], iterations 28 m [-1.86723481e+68], b [-2.63482884e+66],cost [4.65208716e+135], iterations 29 m [3.51782751e+70], b [4.96395705e+68],cost [1.65119762e+140], iterations 30 m [-6.62750627e+72], b [-9.35198112e+70],cost [5.86071048e+144], iterations 31 m [1.24860697e+75], b [1.76189177e+73],cost [2.0801827e+149], iterations 32 m [-2.35234686e+77], b [-3.31936365e+75],cost [7.38333702e+153], iterations 33 m [4.43176748e+79], b [6.25360491e+77],cost [2.62061912e+158], iterations 34 m [-8.34934818e+81], b [-1.17816481e+80],cost [9.30154556e+162], iterations 35 m [1.57299803e+84], b [2.21963545e+82],cost [3.30146221e+167], iterations 36 m [-2.96349217e+86], b [-4.18174223e+84],cost [1.17181092e+172], iterations 37 m [5.58315122e+88], b [7.87830636e+86],cost [4.15918995e+176], iterations 38 m [-1.05185287e+91], b [-1.48425483e+89],cost [1.47625019e+181], iterations 39 m [1.98166666e+93], b [2.79630203e+91],cost [5.23975741e+185], iterations 40 m [-3.7334145e+95], b [-5.26816883e+93],cost [1.85978351e+190], iterations 41 m [7.03366721e+97], b [9.92510912e+95],cost [6.60105886e+194], iterations 42 m [-1.32512675e+100], b [-1.86986777e+98],cost [2.34295969e+199], iterations 43 m [2.49650838e+102], b [3.52278796e+100],cost [8.31602967e+203], iterations 44 m [-4.70336448e+104], b [-6.63685163e+102],cost [2.9516662e+208], iterations 45 m [8.8610307e+106], b [1.25036761e+105],cost [1.0476554e+213], iterations 46 m [-1.66939784e+109], b [-2.35566387e+107],cost [3.71851611e+217], iterations 47 m [3.1451072e+111], b [4.43801666e+109],cost [1.31983877e+222], iterations 48 m [-5.92530977e+113], b [-8.36112149e+111],cost [4.68459545e+226], iterations 49 m [1.11631476e+116], b [1.57521609e+114],cost [1.66273602e+231], iterations 50 m [-2.10311137e+118], b [-2.96767096e+116],cost [5.90166451e+235], iterations 51 m [3.96221355e+120], b [5.59102395e+118],cost [2.0947188e+240], iterations 52 m [-7.46471936e+122], b [-1.05333608e+121],cost [7.434931e+244], iterations 53 m [1.40633599e+125], b [1.98446099e+123],cost [2.63893173e+249], iterations 54 m [-2.64950472e+127], b [-3.73867894e+125],cost [9.36654378e+253], iterations 55 m [4.99160606e+129], b [7.04358529e+127],cost [3.32453248e+258], iterations 56 m [-9.40407123e+131], b [-1.3269953e+130],cost [1.17999942e+263], iterations 57 m [1.77170543e+134], b [2.50002868e+132],cost [4.18825393e+267], iterations 58 m [-3.33785235e+136], b [-4.70999663e+134],cost [1.48656608e+272], iterations 59 m [6.28843721e+138], b [8.87352552e+136],cost [5.27637228e+276], iterations 60 m [-1.18472713e+141], b [-1.67175184e+139],cost [1.87277948e+281], iterations 61 m [2.23199872e+143], b [3.14954208e+141],cost [6.64718636e+285], iterations 62 m [-4.20503434e+145], b [-5.93366497e+143],cost [2.35933205e+290], iterations 63 m [7.92218817e+147], b [1.11788886e+146],cost [8.37414121e+294], iterations 64 m [-1.49252206e+150], b [-2.10607695e+148],cost [2.97229214e+299], iterations 65 m [2.8118773e+152], b [3.96780061e+150],cost [1.0549763e+304], iterations 66 m [-5.29751228e+154], b [-7.47524526e+152],cost [inf], iterations 67 m [9.98039152e+156], b [1.40831905e+155],cost [inf], iterations 68 m [-1.88028285e+159], b [-2.65324075e+157],cost [inf], iterations 69 m [3.54240972e+161], b [4.99864466e+159],cost [inf], iterations 70 m [-6.67381859e+163], b [-9.41733179e+161],cost [inf], iterations 71 m [1.2573321e+166], b [1.77420369e+164],cost [inf], iterations 72 m [-2.36878481e+168], b [-3.34255902e+166],cost [inf], iterations 73 m [4.4627362e+170], b [6.29730445e+168],cost [inf], iterations 74 m [-8.40769255e+172], b [-1.1863977e+171],cost [inf], iterations 75 m [1.58398997e+175], b [2.23514603e+173],cost [inf], iterations 76 m [-2.98420075e+177], b [-4.21096381e+175],cost [inf], iterations 77 m [5.6221657e+179], b [7.93335914e+177],cost [inf], iterations 78 m [-1.05920311e+182], b [-1.49462665e+180],cost [inf], iterations 79 m [1.99551434e+184], b [2.81584229e+182],cost [inf], iterations 80 m [-3.75950321e+186], b [-5.30498224e+184],cost [inf], iterations 81 m [7.08281773e+188], b [9.99446474e+186],cost [inf], iterations 82 m [-1.3343866e+191], b [-1.88293421e+189],cost [inf], iterations 83 m [2.51395372e+193], b [3.54740483e+191],cost [inf], iterations 84 m [-4.73623109e+195], b [-6.68322926e+193],cost [inf], iterations 85 m [8.92295065e+197], b [1.25910505e+196],cost [inf], iterations 86 m [-1.68106342e+200], b [-2.372125e+198],cost [inf], iterations 87 m [3.16708488e+202], b [4.46902905e+200],cost [inf], iterations 88 m [-5.96671521e+204], b [-8.41954813e+202],cost [inf], iterations 89 m [1.12411545e+207], b [1.58622354e+205],cost [inf], iterations 90 m [-2.1178077e+209], b [-2.98840873e+207],cost [inf], iterations 91 m [3.98990108e+211], b [5.63009344e+209],cost [inf], iterations 92 m [-7.51688203e+213], b [-1.06069668e+212],cost [inf], iterations 93 m [1.41616332e+216], b [1.99832819e+214],cost [inf], iterations 94 m [-2.66801918e+218], b [-3.76480444e+216],cost [inf], iterations 95 m [5.02648688e+220], b [7.09280512e+218],cost [inf], iterations 96 m [-9.46978589e+222], b [-1.3362682e+221],cost [inf], iterations 97 m [1.78408592e+225], b [2.51749861e+223],cost [inf], iterations 98 m [-3.36117692e+227], b [-4.74290959e+225],cost [inf], iterations 99 m [6.33238014e+229], b [8.93553279e+227],cost [inf], iterations 100 m [-1.19300588e+232], b [-1.68343386e+230],cost [inf], iterations 101 m [2.24759569e+234], b [3.17155075e+232],cost [inf], iterations 102 m [-4.23441868e+236], b [-5.9751288e+234],cost [inf], iterations 103 m [7.97754759e+238], b [1.12570055e+237],cost [inf], iterations 104 m [-1.50295165e+241], b [-2.12079399e+239],cost [inf], iterations 105 m [2.83152641e+243], b [3.99552719e+241],cost [inf], iterations 106 m [-5.33453074e+245], b [-7.52748149e+243],cost [inf], iterations 107 m [1.00501334e+248], b [1.41816023e+246],cost [inf], iterations 108 m [-1.89342207e+250], b [-2.67178132e+248],cost [inf], iterations 109 m [3.56716371e+252], b [5.03357467e+250],cost [inf], iterations 110 m [-6.72045453e+254], b [-9.48313911e+252],cost [inf], iterations 111 m [1.26611821e+257], b [1.78660164e+255],cost [inf], iterations 112 m [-2.38533763e+259], b [-3.36591647e+257],cost [inf], iterations 113 m [4.49392134e+261], b [6.34130935e+259],cost [inf], iterations 114 m [-8.46644463e+263], b [-1.19468812e+262],cost [inf], iterations 115 m [1.59505873e+266], b [2.250765e+264],cost [inf], iterations 116 m [-3.00505403e+268], b [-4.24038958e+266],cost [inf], iterations 117 m [5.6614528e+270], b [7.98879662e+268],cost [inf], iterations 118 m [-1.06660471e+273], b [-1.50507094e+271],cost [inf], iterations 119 m [2.00945879e+275], b [2.8355191e+273],cost [inf], iterations 120 m [-3.78577422e+277], b [-5.3420529e+275],cost [inf], iterations 121 m [7.13231172e+279], b [1.0064305e+278],cost [inf], iterations 122 m [-1.34371115e+282], b [-1.89609196e+280],cost [inf], iterations 123 m [2.53152097e+284], b [3.57219372e+282],cost [inf], iterations 124 m [-4.76932738e+286], b [-6.72993096e+284],cost [inf], iterations 125 m [8.98530329e+288], b [1.26790354e+287],cost [inf], iterations 126 m [-1.69281051e+291], b [-2.38870117e+289],cost [inf], iterations 127 m [3.18921614e+293], b [4.50025816e+291],cost [inf], iterations 128 m [-6.00840999e+295], b [-8.47838305e+293],cost [inf], iterations 129 m [1.13197065e+298], b [1.5973079e+296],cost [inf], iterations 130 m [-2.13260671e+300], b [-3.00929142e+298],cost [inf], iterations 131 m [4.01778209e+302], b [5.66943594e+300],cost [inf], iterations 132 m [-7.56940922e+304], b [-1.06810872e+303],cost [inf], iterations 133 m [inf], b [2.0122923e+305],cost [inf], iterations 134 m [nan], b [-inf],cost [inf], iterations 135 m [nan], b [nan],cost [nan], iterations 136 m [nan], b [nan],cost [nan], iterations 137 m [nan], b [nan],cost [nan], iterations 138 m [nan], b [nan],cost [nan], iterations 139 m [nan], b [nan],cost [nan], iterations 140 m [nan], b [nan],cost [nan], iterations 141 m [nan], b [nan],cost [nan], iterations 142 m [nan], b [nan],cost [nan], iterations 143 m [nan], b [nan],cost [nan], iterations 144 m [nan], b [nan],cost [nan], iterations 145 m [nan], b [nan],cost [nan], iterations 146 m [nan], b [nan],cost [nan], iterations 147 m [nan], b [nan],cost [nan], iterations 148 m [nan], b [nan],cost [nan], iterations 149 m [nan], b [nan],cost [nan], iterations 150 m [nan], b [nan],cost [nan], iterations 151 m [nan], b [nan],cost [nan], iterations 152 m [nan], b [nan],cost [nan], iterations 153 m [nan], b [nan],cost [nan], iterations 154 m [nan], b [nan],cost [nan], iterations 155 m [nan], b [nan],cost [nan], iterations 156 m [nan], b [nan],cost [nan], iterations 157 m [nan], b [nan],cost [nan], iterations 158 m [nan], b [nan],cost [nan], iterations 159 m [nan], b [nan],cost [nan], iterations 160 m [nan], b [nan],cost [nan], iterations 161 m [nan], b [nan],cost [nan], iterations 162 m [nan], b [nan],cost [nan], iterations 163 m [nan], b [nan],cost [nan], iterations 164 m [nan], b [nan],cost [nan], iterations 165 m [nan], b [nan],cost [nan], iterations 166 m [nan], b [nan],cost [nan], iterations 167 m [nan], b [nan],cost [nan], iterations 168 m [nan], b [nan],cost [nan], iterations 169 m [nan], b [nan],cost [nan], iterations 170 m [nan], b [nan],cost [nan], iterations 171 m [nan], b [nan],cost [nan], iterations 172 m [nan], b [nan],cost [nan], iterations 173 m [nan], b [nan],cost [nan], iterations 174 m [nan], b [nan],cost [nan], iterations 175 m [nan], b [nan],cost [nan], iterations 176 m [nan], b [nan],cost [nan], iterations 177 m [nan], b [nan],cost [nan], iterations 178 m [nan], b [nan],cost [nan], iterations 179 m [nan], b [nan],cost [nan], iterations 180 m [nan], b [nan],cost [nan], iterations 181 m [nan], b [nan],cost [nan], iterations 182 m [nan], b [nan],cost [nan], iterations 183 m [nan], b [nan],cost [nan], iterations 184 m [nan], b [nan],cost [nan], iterations 185 m [nan], b [nan],cost [nan], iterations 186 m [nan], b [nan],cost [nan], iterations 187 m [nan], b [nan],cost [nan], iterations 188 m [nan], b [nan],cost [nan], iterations 189 m [nan], b [nan],cost [nan], iterations 190 m [nan], b [nan],cost [nan], iterations 191 m [nan], b [nan],cost [nan], iterations 192 m [nan], b [nan],cost [nan], iterations 193 m [nan], b [nan],cost [nan], iterations 194 m [nan], b [nan],cost [nan], iterations 195 m [nan], b [nan],cost [nan], iterations 196 m [nan], b [nan],cost [nan], iterations 197 m [nan], b [nan],cost [nan], iterations 198 m [nan], b [nan],cost [nan], iterations 199 m [nan], b [nan],cost [nan], iterations 200 m [nan], b [nan],cost [nan], iterations 201 m [nan], b [nan],cost [nan], iterations 202 m [nan], b [nan],cost [nan], iterations 203 m [nan], b [nan],cost [nan], iterations 204 m [nan], b [nan],cost [nan], iterations 205 m [nan], b [nan],cost [nan], iterations 206 m [nan], b [nan],cost [nan], iterations 207 m [nan], b [nan],cost [nan], iterations 208 m [nan], b [nan],cost [nan], iterations 209 m [nan], b [nan],cost [nan], iterations 210 m [nan], b [nan],cost [nan], iterations 211 m [nan], b [nan],cost [nan], iterations 212 m [nan], b [nan],cost [nan], iterations 213 m [nan], b [nan],cost [nan], iterations 214 m [nan], b [nan],cost [nan], iterations 215 m [nan], b [nan],cost [nan], iterations 216 m [nan], b [nan],cost [nan], iterations 217 m [nan], b [nan],cost [nan], iterations 218 m [nan], b [nan],cost [nan], iterations 219 m [nan], b [nan],cost [nan], iterations 220 m [nan], b [nan],cost [nan], iterations 221 m [nan], b [nan],cost [nan], iterations 222 m [nan], b [nan],cost [nan], iterations 223 m [nan], b [nan],cost [nan], iterations 224 m [nan], b [nan],cost [nan], iterations 225 m [nan], b [nan],cost [nan], iterations 226 m [nan], b [nan],cost [nan], iterations 227 m [nan], b [nan],cost [nan], iterations 228 m [nan], b [nan],cost [nan], iterations 229 m [nan], b [nan],cost [nan], iterations 230 m [nan], b [nan],cost [nan], iterations 231 m [nan], b [nan],cost [nan], iterations 232 m [nan], b [nan],cost [nan], iterations 233 m [nan], b [nan],cost [nan], iterations 234 m [nan], b [nan],cost [nan], iterations 235 m [nan], b [nan],cost [nan], iterations 236 m [nan], b [nan],cost [nan], iterations 237 m [nan], b [nan],cost [nan], iterations 238 m [nan], b [nan],cost [nan], iterations 239 m [nan], b [nan],cost [nan], iterations 240 m [nan], b [nan],cost [nan], iterations 241 m [nan], b [nan],cost [nan], iterations 242 m [nan], b [nan],cost [nan], iterations 243 m [nan], b [nan],cost [nan], iterations 244 m [nan], b [nan],cost [nan], iterations 245 m [nan], b [nan],cost [nan], iterations 246 m [nan], b [nan],cost [nan], iterations 247 m [nan], b [nan],cost [nan], iterations 248 m [nan], b [nan],cost [nan], iterations 249 m [nan], b [nan],cost [nan], iterations 250 m [nan], b [nan],cost [nan], iterations 251 m [nan], b [nan],cost [nan], iterations 252 m [nan], b [nan],cost [nan], iterations 253 m [nan], b [nan],cost [nan], iterations 254 m [nan], b [nan],cost [nan], iterations 255 m [nan], b [nan],cost [nan], iterations 256 m [nan], b [nan],cost [nan], iterations 257 m [nan], b [nan],cost [nan], iterations 258 m [nan], b [nan],cost [nan], iterations 259 m [nan], b [nan],cost [nan], iterations 260 m [nan], b [nan],cost [nan], iterations 261 m [nan], b [nan],cost [nan], iterations 262 m [nan], b [nan],cost [nan], iterations 263 m [nan], b [nan],cost [nan], iterations 264 m [nan], b [nan],cost [nan], iterations 265 m [nan], b [nan],cost [nan], iterations 266 m [nan], b [nan],cost [nan], iterations 267 m [nan], b [nan],cost [nan], iterations 268 m [nan], b [nan],cost [nan], iterations 269 m [nan], b [nan],cost [nan], iterations 270 m [nan], b [nan],cost [nan], iterations 271 m [nan], b [nan],cost [nan], iterations 272 m [nan], b [nan],cost [nan], iterations 273 m [nan], b [nan],cost [nan], iterations 274 m [nan], b [nan],cost [nan], iterations 275 m [nan], b [nan],cost [nan], iterations 276 m [nan], b [nan],cost [nan], iterations 277 m [nan], b [nan],cost [nan], iterations 278 m [nan], b [nan],cost [nan], iterations 279 m [nan], b [nan],cost [nan], iterations 280 m [nan], b [nan],cost [nan], iterations 281 m [nan], b [nan],cost [nan], iterations 282 m [nan], b [nan],cost [nan], iterations 283 m [nan], b [nan],cost [nan], iterations 284 m [nan], b [nan],cost [nan], iterations 285 m [nan], b [nan],cost [nan], iterations 286 m [nan], b [nan],cost [nan], iterations 287 m [nan], b [nan],cost [nan], iterations 288 m [nan], b [nan],cost [nan], iterations 289 m [nan], b [nan],cost [nan], iterations 290 m [nan], b [nan],cost [nan], iterations 291 m [nan], b [nan],cost [nan], iterations 292 m [nan], b [nan],cost [nan], iterations 293 m [nan], b [nan],cost [nan], iterations 294 m [nan], b [nan],cost [nan], iterations 295 m [nan], b [nan],cost [nan], iterations 296 m [nan], b [nan],cost [nan], iterations 297 m [nan], b [nan],cost [nan], iterations 298 m [nan], b [nan],cost [nan], iterations 299 m [nan], b [nan],cost [nan], iterations 300 m [nan], b [nan],cost [nan], iterations 301 m [nan], b [nan],cost [nan], iterations 302 m [nan], b [nan],cost [nan], iterations 303 m [nan], b [nan],cost [nan], iterations 304 m [nan], b [nan],cost [nan], iterations 305 m [nan], b [nan],cost [nan], iterations 306 m [nan], b [nan],cost [nan], iterations 307 m [nan], b [nan],cost [nan], iterations 308 m [nan], b [nan],cost [nan], iterations 309 m [nan], b [nan],cost [nan], iterations 310 m [nan], b [nan],cost [nan], iterations 311 m [nan], b [nan],cost [nan], iterations 312 m [nan], b [nan],cost [nan], iterations 313 m [nan], b [nan],cost [nan], iterations 314 m [nan], b [nan],cost [nan], iterations 315 m [nan], b [nan],cost [nan], iterations 316 m [nan], b [nan],cost [nan], iterations 317 m [nan], b [nan],cost [nan], iterations 318 m [nan], b [nan],cost [nan], iterations 319 m [nan], b [nan],cost [nan], iterations 320 m [nan], b [nan],cost [nan], iterations 321 m [nan], b [nan],cost [nan], iterations 322 m [nan], b [nan],cost [nan], iterations 323 m [nan], b [nan],cost [nan], iterations 324 m [nan], b [nan],cost [nan], iterations 325 m [nan], b [nan],cost [nan], iterations 326 m [nan], b [nan],cost [nan], iterations 327 m [nan], b [nan],cost [nan], iterations 328 m [nan], b [nan],cost [nan], iterations 329 m [nan], b [nan],cost [nan], iterations 330 m [nan], b [nan],cost [nan], iterations 331 m [nan], b [nan],cost [nan], iterations 332 m [nan], b [nan],cost [nan], iterations 333 m [nan], b [nan],cost [nan], iterations 334 m [nan], b [nan],cost [nan], iterations 335 m [nan], b [nan],cost [nan], iterations 336 m [nan], b [nan],cost [nan], iterations 337 m [nan], b [nan],cost [nan], iterations 338 m [nan], b [nan],cost [nan], iterations 339 m [nan], b [nan],cost [nan], iterations 340 m [nan], b [nan],cost [nan], iterations 341 m [nan], b [nan],cost [nan], iterations 342 m [nan], b [nan],cost [nan], iterations 343 m [nan], b [nan],cost [nan], iterations 344 m [nan], b [nan],cost [nan], iterations 345 m [nan], b [nan],cost [nan], iterations 346 m [nan], b [nan],cost [nan], iterations 347 m [nan], b [nan],cost [nan], iterations 348 m [nan], b [nan],cost [nan], iterations 349 m [nan], b [nan],cost [nan], iterations 350 m [nan], b [nan],cost [nan], iterations 351 m [nan], b [nan],cost [nan], iterations 352 m [nan], b [nan],cost [nan], iterations 353 m [nan], b [nan],cost [nan], iterations 354 m [nan], b [nan],cost [nan], iterations 355 m [nan], b [nan],cost [nan], iterations 356 m [nan], b [nan],cost [nan], iterations 357 m [nan], b [nan],cost [nan], iterations 358 m [nan], b [nan],cost [nan], iterations 359 m [nan], b [nan],cost [nan], iterations 360 m [nan], b [nan],cost [nan], iterations 361 m [nan], b [nan],cost [nan], iterations 362 m [nan], b [nan],cost [nan], iterations 363 m [nan], b [nan],cost [nan], iterations 364 m [nan], b [nan],cost [nan], iterations 365 m [nan], b [nan],cost [nan], iterations 366 m [nan], b [nan],cost [nan], iterations 367 m [nan], b [nan],cost [nan], iterations 368 m [nan], b [nan],cost [nan], iterations 369 m [nan], b [nan],cost [nan], iterations 370 m [nan], b [nan],cost [nan], iterations 371 m [nan], b [nan],cost [nan], iterations 372 m [nan], b [nan],cost [nan], iterations 373 m [nan], b [nan],cost [nan], iterations 374 m [nan], b [nan],cost [nan], iterations 375 m [nan], b [nan],cost [nan], iterations 376 m [nan], b [nan],cost [nan], iterations 377 m [nan], b [nan],cost [nan], iterations 378 m [nan], b [nan],cost [nan], iterations 379 m [nan], b [nan],cost [nan], iterations 380 m [nan], b [nan],cost [nan], iterations 381 m [nan], b [nan],cost [nan], iterations 382 m [nan], b [nan],cost [nan], iterations 383 m [nan], b [nan],cost [nan], iterations 384 m [nan], b [nan],cost [nan], iterations 385 m [nan], b [nan],cost [nan], iterations 386 m [nan], b [nan],cost [nan], iterations 387 m [nan], b [nan],cost [nan], iterations 388 m [nan], b [nan],cost [nan], iterations 389 m [nan], b [nan],cost [nan], iterations 390 m [nan], b [nan],cost [nan], iterations 391 m [nan], b [nan],cost [nan], iterations 392 m [nan], b [nan],cost [nan], iterations 393 m [nan], b [nan],cost [nan], iterations 394 m [nan], b [nan],cost [nan], iterations 395 m [nan], b [nan],cost [nan], iterations 396 m [nan], b [nan],cost [nan], iterations 397 m [nan], b [nan],cost [nan], iterations 398 m [nan], b [nan],cost [nan], iterations 399 m [nan], b [nan],cost [nan], iterations 400 m [nan], b [nan],cost [nan], iterations 401 m [nan], b [nan],cost [nan], iterations 402 m [nan], b [nan],cost [nan], iterations 403 m [nan], b [nan],cost [nan], iterations 404 m [nan], b [nan],cost [nan], iterations 405 m [nan], b [nan],cost [nan], iterations 406 m [nan], b [nan],cost [nan], iterations 407 m [nan], b [nan],cost [nan], iterations 408 m [nan], b [nan],cost [nan], iterations 409 m [nan], b [nan],cost [nan], iterations 410 m [nan], b [nan],cost [nan], iterations 411 m [nan], b [nan],cost [nan], iterations 412 m [nan], b [nan],cost [nan], iterations 413 m [nan], b [nan],cost [nan], iterations 414 m [nan], b [nan],cost [nan], iterations 415 m [nan], b [nan],cost [nan], iterations 416 m [nan], b [nan],cost [nan], iterations 417 m [nan], b [nan],cost [nan], iterations 418 m [nan], b [nan],cost [nan], iterations 419 m [nan], b [nan],cost [nan], iterations 420 m [nan], b [nan],cost [nan], iterations 421 m [nan], b [nan],cost [nan], iterations 422 m [nan], b [nan],cost [nan], iterations 423 m [nan], b [nan],cost [nan], iterations 424 m [nan], b [nan],cost [nan], iterations 425 m [nan], b [nan],cost [nan], iterations 426 m [nan], b [nan],cost [nan], iterations 427 m [nan], b [nan],cost [nan], iterations 428 m [nan], b [nan],cost [nan], iterations 429 m [nan], b [nan],cost [nan], iterations 430 m [nan], b [nan],cost [nan], iterations 431 m [nan], b [nan],cost [nan], iterations 432 m [nan], b [nan],cost [nan], iterations 433 m [nan], b [nan],cost [nan], iterations 434 m [nan], b [nan],cost [nan], iterations 435 m [nan], b [nan],cost [nan], iterations 436 m [nan], b [nan],cost [nan], iterations 437 m [nan], b [nan],cost [nan], iterations 438 m [nan], b [nan],cost [nan], iterations 439 m [nan], b [nan],cost [nan], iterations 440 m [nan], b [nan],cost [nan], iterations 441 m [nan], b [nan],cost [nan], iterations 442 m [nan], b [nan],cost [nan], iterations 443 m [nan], b [nan],cost [nan], iterations 444 m [nan], b [nan],cost [nan], iterations 445 m [nan], b [nan],cost [nan], iterations 446 m [nan], b [nan],cost [nan], iterations 447 m [nan], b [nan],cost [nan], iterations 448 m [nan], b [nan],cost [nan], iterations 449 m [nan], b [nan],cost [nan], iterations 450 m [nan], b [nan],cost [nan], iterations 451 m [nan], b [nan],cost [nan], iterations 452 m [nan], b [nan],cost [nan], iterations 453 m [nan], b [nan],cost [nan], iterations 454 m [nan], b [nan],cost [nan], iterations 455 m [nan], b [nan],cost [nan], iterations 456 m [nan], b [nan],cost [nan], iterations 457 m [nan], b [nan],cost [nan], iterations 458 m [nan], b [nan],cost [nan], iterations 459 m [nan], b [nan],cost [nan], iterations 460 m [nan], b [nan],cost [nan], iterations 461 m [nan], b [nan],cost [nan], iterations 462 m [nan], b [nan],cost [nan], iterations 463 m [nan], b [nan],cost [nan], iterations 464 m [nan], b [nan],cost [nan], iterations 465 m [nan], b [nan],cost [nan], iterations 466 m [nan], b [nan],cost [nan], iterations 467 m [nan], b [nan],cost [nan], iterations 468 m [nan], b [nan],cost [nan], iterations 469 m [nan], b [nan],cost [nan], iterations 470 m [nan], b [nan],cost [nan], iterations 471 m [nan], b [nan],cost [nan], iterations 472 m [nan], b [nan],cost [nan], iterations 473 m [nan], b [nan],cost [nan], iterations 474 m [nan], b [nan],cost [nan], iterations 475 m [nan], b [nan],cost [nan], iterations 476 m [nan], b [nan],cost [nan], iterations 477 m [nan], b [nan],cost [nan], iterations 478 m [nan], b [nan],cost [nan], iterations 479 m [nan], b [nan],cost [nan], iterations 480 m [nan], b [nan],cost [nan], iterations 481 m [nan], b [nan],cost [nan], iterations 482 m [nan], b [nan],cost [nan], iterations 483 m [nan], b [nan],cost [nan], iterations 484 m [nan], b [nan],cost [nan], iterations 485 m [nan], b [nan],cost [nan], iterations 486 m [nan], b [nan],cost [nan], iterations 487 m [nan], b [nan],cost [nan], iterations 488 m [nan], b [nan],cost [nan], iterations 489 m [nan], b [nan],cost [nan], iterations 490 m [nan], b [nan],cost [nan], iterations 491 m [nan], b [nan],cost [nan], iterations 492 m [nan], b [nan],cost [nan], iterations 493 m [nan], b [nan],cost [nan], iterations 494 m [nan], b [nan],cost [nan], iterations 495 m [nan], b [nan],cost [nan], iterations 496 m [nan], b [nan],cost [nan], iterations 497 m [nan], b [nan],cost [nan], iterations 498 m [nan], b [nan],cost [nan], iterations 499 m [nan], b [nan],cost [nan], iterations 500 m [nan], b [nan],cost [nan], iterations 501 m [nan], b [nan],cost [nan], iterations 502 m [nan], b [nan],cost [nan], iterations 503 m [nan], b [nan],cost [nan], iterations 504 m [nan], b [nan],cost [nan], iterations 505 m [nan], b [nan],cost [nan], iterations 506 m [nan], b [nan],cost [nan], iterations 507 m [nan], b [nan],cost [nan], iterations 508 m [nan], b [nan],cost [nan], iterations 509 m [nan], b [nan],cost [nan], iterations 510 m [nan], b [nan],cost [nan], iterations 511 m [nan], b [nan],cost [nan], iterations 512 m [nan], b [nan],cost [nan], iterations 513 m [nan], b [nan],cost [nan], iterations 514 m [nan], b [nan],cost [nan], iterations 515 m [nan], b [nan],cost [nan], iterations 516 m [nan], b [nan],cost [nan], iterations 517 m [nan], b [nan],cost [nan], iterations 518 m [nan], b [nan],cost [nan], iterations 519 m [nan], b [nan],cost [nan], iterations 520 m [nan], b [nan],cost [nan], iterations 521 m [nan], b [nan],cost [nan], iterations 522 m [nan], b [nan],cost [nan], iterations 523 m [nan], b [nan],cost [nan], iterations 524 m [nan], b [nan],cost [nan], iterations 525 m [nan], b [nan],cost [nan], iterations 526 m [nan], b [nan],cost [nan], iterations 527 m [nan], b [nan],cost [nan], iterations 528 m [nan], b [nan],cost [nan], iterations 529 m [nan], b [nan],cost [nan], iterations 530 m [nan], b [nan],cost [nan], iterations 531 m [nan], b [nan],cost [nan], iterations 532 m [nan], b [nan],cost [nan], iterations 533 m [nan], b [nan],cost [nan], iterations 534 m [nan], b [nan],cost [nan], iterations 535 m [nan], b [nan],cost [nan], iterations 536 m [nan], b [nan],cost [nan], iterations 537 m [nan], b [nan],cost [nan], iterations 538 m [nan], b [nan],cost [nan], iterations 539 m [nan], b [nan],cost [nan], iterations 540 m [nan], b [nan],cost [nan], iterations 541 m [nan], b [nan],cost [nan], iterations 542 m [nan], b [nan],cost [nan], iterations 543 m [nan], b [nan],cost [nan], iterations 544 m [nan], b [nan],cost [nan], iterations 545 m [nan], b [nan],cost [nan], iterations 546 m [nan], b [nan],cost [nan], iterations 547 m [nan], b [nan],cost [nan], iterations 548 m [nan], b [nan],cost [nan], iterations 549 m [nan], b [nan],cost [nan], iterations 550 m [nan], b [nan],cost [nan], iterations 551 m [nan], b [nan],cost [nan], iterations 552 m [nan], b [nan],cost [nan], iterations 553 m [nan], b [nan],cost [nan], iterations 554 m [nan], b [nan],cost [nan], iterations 555 m [nan], b [nan],cost [nan], iterations 556 m [nan], b [nan],cost [nan], iterations 557 m [nan], b [nan],cost [nan], iterations 558 m [nan], b [nan],cost [nan], iterations 559 m [nan], b [nan],cost [nan], iterations 560 m [nan], b [nan],cost [nan], iterations 561 m [nan], b [nan],cost [nan], iterations 562 m [nan], b [nan],cost [nan], iterations 563 m [nan], b [nan],cost [nan], iterations 564 m [nan], b [nan],cost [nan], iterations 565 m [nan], b [nan],cost [nan], iterations 566 m [nan], b [nan],cost [nan], iterations 567 m [nan], b [nan],cost [nan], iterations 568 m [nan], b [nan],cost [nan], iterations 569 m [nan], b [nan],cost [nan], iterations 570 m [nan], b [nan],cost [nan], iterations 571 m [nan], b [nan],cost [nan], iterations 572 m [nan], b [nan],cost [nan], iterations 573 m [nan], b [nan],cost [nan], iterations 574 m [nan], b [nan],cost [nan], iterations 575 m [nan], b [nan],cost [nan], iterations 576 m [nan], b [nan],cost [nan], iterations 577 m [nan], b [nan],cost [nan], iterations 578 m [nan], b [nan],cost [nan], iterations 579 m [nan], b [nan],cost [nan], iterations 580 m [nan], b [nan],cost [nan], iterations 581 m [nan], b [nan],cost [nan], iterations 582 m [nan], b [nan],cost [nan], iterations 583 m [nan], b [nan],cost [nan], iterations 584 m [nan], b [nan],cost [nan], iterations 585 m [nan], b [nan],cost [nan], iterations 586 m [nan], b [nan],cost [nan], iterations 587 m [nan], b [nan],cost [nan], iterations 588 m [nan], b [nan],cost [nan], iterations 589 m [nan], b [nan],cost [nan], iterations 590 m [nan], b [nan],cost [nan], iterations 591 m [nan], b [nan],cost [nan], iterations 592 m [nan], b [nan],cost [nan], iterations 593 m [nan], b [nan],cost [nan], iterations 594 m [nan], b [nan],cost [nan], iterations 595 m [nan], b [nan],cost [nan], iterations 596 m [nan], b [nan],cost [nan], iterations 597 m [nan], b [nan],cost [nan], iterations 598 m [nan], b [nan],cost [nan], iterations 599 m [nan], b [nan],cost [nan], iterations 600 m [nan], b [nan],cost [nan], iterations 601 m [nan], b [nan],cost [nan], iterations 602 m [nan], b [nan],cost [nan], iterations 603 m [nan], b [nan],cost [nan], iterations 604 m [nan], b [nan],cost [nan], iterations 605 m [nan], b [nan],cost [nan], iterations 606 m [nan], b [nan],cost [nan], iterations 607 m [nan], b [nan],cost [nan], iterations 608 m [nan], b [nan],cost [nan], iterations 609 m [nan], b [nan],cost [nan], iterations 610 m [nan], b [nan],cost [nan], iterations 611 m [nan], b [nan],cost [nan], iterations 612 m [nan], b [nan],cost [nan], iterations 613 m [nan], b [nan],cost [nan], iterations 614 m [nan], b [nan],cost [nan], iterations 615 m [nan], b [nan],cost [nan], iterations 616 m [nan], b [nan],cost [nan], iterations 617 m [nan], b [nan],cost [nan], iterations 618 m [nan], b [nan],cost [nan], iterations 619 m [nan], b [nan],cost [nan], iterations 620 m [nan], b [nan],cost [nan], iterations 621 m [nan], b [nan],cost [nan], iterations 622 m [nan], b [nan],cost [nan], iterations 623 m [nan], b [nan],cost [nan], iterations 624 m [nan], b [nan],cost [nan], iterations 625 m [nan], b [nan],cost [nan], iterations 626 m [nan], b [nan],cost [nan], iterations 627 m [nan], b [nan],cost [nan], iterations 628 m [nan], b [nan],cost [nan], iterations 629 m [nan], b [nan],cost [nan], iterations 630 m [nan], b [nan],cost [nan], iterations 631 m [nan], b [nan],cost [nan], iterations 632 m [nan], b [nan],cost [nan], iterations 633 m [nan], b [nan],cost [nan], iterations 634 m [nan], b [nan],cost [nan], iterations 635 m [nan], b [nan],cost [nan], iterations 636 m [nan], b [nan],cost [nan], iterations 637 m [nan], b [nan],cost [nan], iterations 638 m [nan], b [nan],cost [nan], iterations 639 m [nan], b [nan],cost [nan], iterations 640 m [nan], b [nan],cost [nan], iterations 641 m [nan], b [nan],cost [nan], iterations 642 m [nan], b [nan],cost [nan], iterations 643 m [nan], b [nan],cost [nan], iterations 644 m [nan], b [nan],cost [nan], iterations 645 m [nan], b [nan],cost [nan], iterations 646 m [nan], b [nan],cost [nan], iterations 647 m [nan], b [nan],cost [nan], iterations 648 m [nan], b [nan],cost [nan], iterations 649 m [nan], b [nan],cost [nan], iterations 650 m [nan], b [nan],cost [nan], iterations 651 m [nan], b [nan],cost [nan], iterations 652 m [nan], b [nan],cost [nan], iterations 653 m [nan], b [nan],cost [nan], iterations 654 m [nan], b [nan],cost [nan], iterations 655 m [nan], b [nan],cost [nan], iterations 656 m [nan], b [nan],cost [nan], iterations 657 m [nan], b [nan],cost [nan], iterations 658 m [nan], b [nan],cost [nan], iterations 659 m [nan], b [nan],cost [nan], iterations 660 m [nan], b [nan],cost [nan], iterations 661 m [nan], b [nan],cost [nan], iterations 662 m [nan], b [nan],cost [nan], iterations 663 m [nan], b [nan],cost [nan], iterations 664 m [nan], b [nan],cost [nan], iterations 665 m [nan], b [nan],cost [nan], iterations 666 m [nan], b [nan],cost [nan], iterations 667 m [nan], b [nan],cost [nan], iterations 668 m [nan], b [nan],cost [nan], iterations 669 m [nan], b [nan],cost [nan], iterations 670 m [nan], b [nan],cost [nan], iterations 671 m [nan], b [nan],cost [nan], iterations 672 m [nan], b [nan],cost [nan], iterations 673 m [nan], b [nan],cost [nan], iterations 674 m [nan], b [nan],cost [nan], iterations 675 m [nan], b [nan],cost [nan], iterations 676 m [nan], b [nan],cost [nan], iterations 677 m [nan], b [nan],cost [nan], iterations 678 m [nan], b [nan],cost [nan], iterations 679 m [nan], b [nan],cost [nan], iterations 680 m [nan], b [nan],cost [nan], iterations 681 m [nan], b [nan],cost [nan], iterations 682 m [nan], b [nan],cost [nan], iterations 683 m [nan], b [nan],cost [nan], iterations 684 m [nan], b [nan],cost [nan], iterations 685 m [nan], b [nan],cost [nan], iterations 686 m [nan], b [nan],cost [nan], iterations 687 m [nan], b [nan],cost [nan], iterations 688 m [nan], b [nan],cost [nan], iterations 689 m [nan], b [nan],cost [nan], iterations 690 m [nan], b [nan],cost [nan], iterations 691 m [nan], b [nan],cost [nan], iterations 692 m [nan], b [nan],cost [nan], iterations 693 m [nan], b [nan],cost [nan], iterations 694 m [nan], b [nan],cost [nan], iterations 695 m [nan], b [nan],cost [nan], iterations 696 m [nan], b [nan],cost [nan], iterations 697 m [nan], b [nan],cost [nan], iterations 698 m [nan], b [nan],cost [nan], iterations 699 m [nan], b [nan],cost [nan], iterations 700 m [nan], b [nan],cost [nan], iterations 701 m [nan], b [nan],cost [nan], iterations 702 m [nan], b [nan],cost [nan], iterations 703 m [nan], b [nan],cost [nan], iterations 704 m [nan], b [nan],cost [nan], iterations 705 m [nan], b [nan],cost [nan], iterations 706 m [nan], b [nan],cost [nan], iterations 707 m [nan], b [nan],cost [nan], iterations 708 m [nan], b [nan],cost [nan], iterations 709 m [nan], b [nan],cost [nan], iterations 710 m [nan], b [nan],cost [nan], iterations 711 m [nan], b [nan],cost [nan], iterations 712 m [nan], b [nan],cost [nan], iterations 713 m [nan], b [nan],cost [nan], iterations 714 m [nan], b [nan],cost [nan], iterations 715 m [nan], b [nan],cost [nan], iterations 716 m [nan], b [nan],cost [nan], iterations 717 m [nan], b [nan],cost [nan], iterations 718 m [nan], b [nan],cost [nan], iterations 719 m [nan], b [nan],cost [nan], iterations 720 m [nan], b [nan],cost [nan], iterations 721 m [nan], b [nan],cost [nan], iterations 722 m [nan], b [nan],cost [nan], iterations 723 m [nan], b [nan],cost [nan], iterations 724 m [nan], b [nan],cost [nan], iterations 725 m [nan], b [nan],cost [nan], iterations 726 m [nan], b [nan],cost [nan], iterations 727 m [nan], b [nan],cost [nan], iterations 728 m [nan], b [nan],cost [nan], iterations 729 m [nan], b [nan],cost [nan], iterations 730 m [nan], b [nan],cost [nan], iterations 731 m [nan], b [nan],cost [nan], iterations 732 m [nan], b [nan],cost [nan], iterations 733 m [nan], b [nan],cost [nan], iterations 734 m [nan], b [nan],cost [nan], iterations 735 m [nan], b [nan],cost [nan], iterations 736 m [nan], b [nan],cost [nan], iterations 737 m [nan], b [nan],cost [nan], iterations 738 m [nan], b [nan],cost [nan], iterations 739 m [nan], b [nan],cost [nan], iterations 740 m [nan], b [nan],cost [nan], iterations 741 m [nan], b [nan],cost [nan], iterations 742 m [nan], b [nan],cost [nan], iterations 743 m [nan], b [nan],cost [nan], iterations 744 m [nan], b [nan],cost [nan], iterations 745 m [nan], b [nan],cost [nan], iterations 746 m [nan], b [nan],cost [nan], iterations 747 m [nan], b [nan],cost [nan], iterations 748 m [nan], b [nan],cost [nan], iterations 749 m [nan], b [nan],cost [nan], iterations 750 m [nan], b [nan],cost [nan], iterations 751 m [nan], b [nan],cost [nan], iterations 752 m [nan], b [nan],cost [nan], iterations 753 m [nan], b [nan],cost [nan], iterations 754 m [nan], b [nan],cost [nan], iterations 755 m [nan], b [nan],cost [nan], iterations 756 m [nan], b [nan],cost [nan], iterations 757 m [nan], b [nan],cost [nan], iterations 758 m [nan], b [nan],cost [nan], iterations 759 m [nan], b [nan],cost [nan], iterations 760 m [nan], b [nan],cost [nan], iterations 761 m [nan], b [nan],cost [nan], iterations 762 m [nan], b [nan],cost [nan], iterations 763 m [nan], b [nan],cost [nan], iterations 764 m [nan], b [nan],cost [nan], iterations 765 m [nan], b [nan],cost [nan], iterations 766 m [nan], b [nan],cost [nan], iterations 767 m [nan], b [nan],cost [nan], iterations 768 m [nan], b [nan],cost [nan], iterations 769 m [nan], b [nan],cost [nan], iterations 770 m [nan], b [nan],cost [nan], iterations 771 m [nan], b [nan],cost [nan], iterations 772 m [nan], b [nan],cost [nan], iterations 773 m [nan], b [nan],cost [nan], iterations 774 m [nan], b [nan],cost [nan], iterations 775 m [nan], b [nan],cost [nan], iterations 776 m [nan], b [nan],cost [nan], iterations 777 m [nan], b [nan],cost [nan], iterations 778 m [nan], b [nan],cost [nan], iterations 779 m [nan], b [nan],cost [nan], iterations 780 m [nan], b [nan],cost [nan], iterations 781 m [nan], b [nan],cost [nan], iterations 782 m [nan], b [nan],cost [nan], iterations 783 m [nan], b [nan],cost [nan], iterations 784 m [nan], b [nan],cost [nan], iterations 785 m [nan], b [nan],cost [nan], iterations 786 m [nan], b [nan],cost [nan], iterations 787 m [nan], b [nan],cost [nan], iterations 788 m [nan], b [nan],cost [nan], iterations 789 m [nan], b [nan],cost [nan], iterations 790 m [nan], b [nan],cost [nan], iterations 791 m [nan], b [nan],cost [nan], iterations 792 m [nan], b [nan],cost [nan], iterations 793 m [nan], b [nan],cost [nan], iterations 794 m [nan], b [nan],cost [nan], iterations 795 m [nan], b [nan],cost [nan], iterations 796 m [nan], b [nan],cost [nan], iterations 797 m [nan], b [nan],cost [nan], iterations 798 m [nan], b [nan],cost [nan], iterations 799 m [nan], b [nan],cost [nan], iterations 800 m [nan], b [nan],cost [nan], iterations 801 m [nan], b [nan],cost [nan], iterations 802 m [nan], b [nan],cost [nan], iterations 803 m [nan], b [nan],cost [nan], iterations 804 m [nan], b [nan],cost [nan], iterations 805 m [nan], b [nan],cost [nan], iterations 806 m [nan], b [nan],cost [nan], iterations 807 m [nan], b [nan],cost [nan], iterations 808 m [nan], b [nan],cost [nan], iterations 809 m [nan], b [nan],cost [nan], iterations 810 m [nan], b [nan],cost [nan], iterations 811 m [nan], b [nan],cost [nan], iterations 812 m [nan], b [nan],cost [nan], iterations 813 m [nan], b [nan],cost [nan], iterations 814 m [nan], b [nan],cost [nan], iterations 815 m [nan], b [nan],cost [nan], iterations 816 m [nan], b [nan],cost [nan], iterations 817 m [nan], b [nan],cost [nan], iterations 818 m [nan], b [nan],cost [nan], iterations 819 m [nan], b [nan],cost [nan], iterations 820 m [nan], b [nan],cost [nan], iterations 821 m [nan], b [nan],cost [nan], iterations 822 m [nan], b [nan],cost [nan], iterations 823 m [nan], b [nan],cost [nan], iterations 824 m [nan], b [nan],cost [nan], iterations 825 m [nan], b [nan],cost [nan], iterations 826 m [nan], b [nan],cost [nan], iterations 827 m [nan], b [nan],cost [nan], iterations 828 m [nan], b [nan],cost [nan], iterations 829 m [nan], b [nan],cost [nan], iterations 830 m [nan], b [nan],cost [nan], iterations 831 m [nan], b [nan],cost [nan], iterations 832 m [nan], b [nan],cost [nan], iterations 833 m [nan], b [nan],cost [nan], iterations 834 m [nan], b [nan],cost [nan], iterations 835 m [nan], b [nan],cost [nan], iterations 836 m [nan], b [nan],cost [nan], iterations 837 m [nan], b [nan],cost [nan], iterations 838 m [nan], b [nan],cost [nan], iterations 839 m [nan], b [nan],cost [nan], iterations 840 m [nan], b [nan],cost [nan], iterations 841 m [nan], b [nan],cost [nan], iterations 842 m [nan], b [nan],cost [nan], iterations 843 m [nan], b [nan],cost [nan], iterations 844 m [nan], b [nan],cost [nan], iterations 845 m [nan], b [nan],cost [nan], iterations 846 m [nan], b [nan],cost [nan], iterations 847 m [nan], b [nan],cost [nan], iterations 848 m [nan], b [nan],cost [nan], iterations 849 m [nan], b [nan],cost [nan], iterations 850 m [nan], b [nan],cost [nan], iterations 851 m [nan], b [nan],cost [nan], iterations 852 m [nan], b [nan],cost [nan], iterations 853 m [nan], b [nan],cost [nan], iterations 854 m [nan], b [nan],cost [nan], iterations 855 m [nan], b [nan],cost [nan], iterations 856 m [nan], b [nan],cost [nan], iterations 857 m [nan], b [nan],cost [nan], iterations 858 m [nan], b [nan],cost [nan], iterations 859 m [nan], b [nan],cost [nan], iterations 860 m [nan], b [nan],cost [nan], iterations 861 m [nan], b [nan],cost [nan], iterations 862 m [nan], b [nan],cost [nan], iterations 863 m [nan], b [nan],cost [nan], iterations 864 m [nan], b [nan],cost [nan], iterations 865 m [nan], b [nan],cost [nan], iterations 866 m [nan], b [nan],cost [nan], iterations 867 m [nan], b [nan],cost [nan], iterations 868 m [nan], b [nan],cost [nan], iterations 869 m [nan], b [nan],cost [nan], iterations 870 m [nan], b [nan],cost [nan], iterations 871 m [nan], b [nan],cost [nan], iterations 872 m [nan], b [nan],cost [nan], iterations 873 m [nan], b [nan],cost [nan], iterations 874 m [nan], b [nan],cost [nan], iterations 875 m [nan], b [nan],cost [nan], iterations 876 m [nan], b [nan],cost [nan], iterations 877 m [nan], b [nan],cost [nan], iterations 878 m [nan], b [nan],cost [nan], iterations 879 m [nan], b [nan],cost [nan], iterations 880 m [nan], b [nan],cost [nan], iterations 881 m [nan], b [nan],cost [nan], iterations 882 m [nan], b [nan],cost [nan], iterations 883 m [nan], b [nan],cost [nan], iterations 884 m [nan], b [nan],cost [nan], iterations 885 m [nan], b [nan],cost [nan], iterations 886 m [nan], b [nan],cost [nan], iterations 887 m [nan], b [nan],cost [nan], iterations 888 m [nan], b [nan],cost [nan], iterations 889 m [nan], b [nan],cost [nan], iterations 890 /tmp/ipykernel_47/3687616195.py:9: RuntimeWarning: overflow encountered in square cost = (1/n) * sum([value**2 for value in (y - y_predicted)]) /tmp/ipykernel_47/3687616195.py:11: RuntimeWarning: overflow encountered in multiply md = -(2/n)*sum(x*(y-y_predicted)) /tmp/ipykernel_47/3687616195.py:13: RuntimeWarning: invalid value encountered in subtract m_curr = m_curr - learning_rate * md m [nan], b [nan],cost [nan], iterations 891 m [nan], b [nan],cost [nan], iterations 892 m [nan], b [nan],cost [nan], iterations 893 m [nan], b [nan],cost [nan], iterations 894 m [nan], b [nan],cost [nan], iterations 895 m [nan], b [nan],cost [nan], iterations 896 m [nan], b [nan],cost [nan], iterations 897 m [nan], b [nan],cost [nan], iterations 898 m [nan], b [nan],cost [nan], iterations 899 m [nan], b [nan],cost [nan], iterations 900 m [nan], b [nan],cost [nan], iterations 901 m [nan], b [nan],cost [nan], iterations 902 m [nan], b [nan],cost [nan], iterations 903 m [nan], b [nan],cost [nan], iterations 904 m [nan], b [nan],cost [nan], iterations 905 m [nan], b [nan],cost [nan], iterations 906 m [nan], b [nan],cost [nan], iterations 907 m [nan], b [nan],cost [nan], iterations 908 m [nan], b [nan],cost [nan], iterations 909 m [nan], b [nan],cost [nan], iterations 910 m [nan], b [nan],cost [nan], iterations 911 m [nan], b [nan],cost [nan], iterations 912 m [nan], b [nan],cost [nan], iterations 913 m [nan], b [nan],cost [nan], iterations 914 m [nan], b [nan],cost [nan], iterations 915 m [nan], b [nan],cost [nan], iterations 916 m [nan], b [nan],cost [nan], iterations 917 m [nan], b [nan],cost [nan], iterations 918 m [nan], b [nan],cost [nan], iterations 919 m [nan], b [nan],cost [nan], iterations 920 m [nan], b [nan],cost [nan], iterations 921 m [nan], b [nan],cost [nan], iterations 922 m [nan], b [nan],cost [nan], iterations 923 m [nan], b [nan],cost [nan], iterations 924 m [nan], b [nan],cost [nan], iterations 925 m [nan], b [nan],cost [nan], iterations 926 m [nan], b [nan],cost [nan], iterations 927 m [nan], b [nan],cost [nan], iterations 928 m [nan], b [nan],cost [nan], iterations 929 m [nan], b [nan],cost [nan], iterations 930 m [nan], b [nan],cost [nan], iterations 931 m [nan], b [nan],cost [nan], iterations 932 m [nan], b [nan],cost [nan], iterations 933 m [nan], b [nan],cost [nan], iterations 934 m [nan], b [nan],cost [nan], iterations 935 m [nan], b [nan],cost [nan], iterations 936 m [nan], b [nan],cost [nan], iterations 937 m [nan], b [nan],cost [nan], iterations 938 m [nan], b [nan],cost [nan], iterations 939 m [nan], b [nan],cost [nan], iterations 940 m [nan], b [nan],cost [nan], iterations 941 m [nan], b [nan],cost [nan], iterations 942 m [nan], b [nan],cost [nan], iterations 943 m [nan], b [nan],cost [nan], iterations 944 m [nan], b [nan],cost [nan], iterations 945 m [nan], b [nan],cost [nan], iterations 946 m [nan], b [nan],cost [nan], iterations 947 m [nan], b [nan],cost [nan], iterations 948 m [nan], b [nan],cost [nan], iterations 949 m [nan], b [nan],cost [nan], iterations 950 m [nan], b [nan],cost [nan], iterations 951 m [nan], b [nan],cost [nan], iterations 952 m [nan], b [nan],cost [nan], iterations 953 m [nan], b [nan],cost [nan], iterations 954 m [nan], b [nan],cost [nan], iterations 955 m [nan], b [nan],cost [nan], iterations 956 m [nan], b [nan],cost [nan], iterations 957 m [nan], b [nan],cost [nan], iterations 958 m [nan], b [nan],cost [nan], iterations 959 m [nan], b [nan],cost [nan], iterations 960 m [nan], b [nan],cost [nan], iterations 961 m [nan], b [nan],cost [nan], iterations 962 m [nan], b [nan],cost [nan], iterations 963 m [nan], b [nan],cost [nan], iterations 964 m [nan], b [nan],cost [nan], iterations 965 m [nan], b [nan],cost [nan], iterations 966 m [nan], b [nan],cost [nan], iterations 967 m [nan], b [nan],cost [nan], iterations 968 m [nan], b [nan],cost [nan], iterations 969 m [nan], b [nan],cost [nan], iterations 970 m [nan], b [nan],cost [nan], iterations 971 m [nan], b [nan],cost [nan], iterations 972 m [nan], b [nan],cost [nan], iterations 973 m [nan], b [nan],cost [nan], iterations 974 m [nan], b [nan],cost [nan], iterations 975 m [nan], b [nan],cost [nan], iterations 976 m [nan], b [nan],cost [nan], iterations 977 m [nan], b [nan],cost [nan], iterations 978 m [nan], b [nan],cost [nan], iterations 979 m [nan], b [nan],cost [nan], iterations 980 m [nan], b [nan],cost [nan], iterations 981 m [nan], b [nan],cost [nan], iterations 982 m [nan], b [nan],cost [nan], iterations 983 m [nan], b [nan],cost [nan], iterations 984 m [nan], b [nan],cost [nan], iterations 985 m [nan], b [nan],cost [nan], iterations 986 m [nan], b [nan],cost [nan], iterations 987 m [nan], b [nan],cost [nan], iterations 988 m [nan], b [nan],cost [nan], iterations 989 m [nan], b [nan],cost [nan], iterations 990 m [nan], b [nan],cost [nan], iterations 991 m [nan], b [nan],cost [nan], iterations 992 m [nan], b [nan],cost [nan], iterations 993 m [nan], b [nan],cost [nan], iterations 994 m [nan], b [nan],cost [nan], iterations 995 m [nan], b [nan],cost [nan], iterations 996 m [nan], b [nan],cost [nan], iterations 997 m [nan], b [nan],cost [nan], iterations 998 m [nan], b [nan],cost [nan], iterations 999"
  },
  
  
  
  {
    "title": "Multiple Linear Regression - Notebook",
    "url": "/Machine-Learningprojects/2-multiple-linear-regression/notebook",
    "content": "Multiple Linear Regression - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session /kaggle/input/prices/homeprices (1).csv import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model df = pd.read_csv(\"/kaggle/input/prices/homeprices (1).csv\") df.shape (6, 4) df.head() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater has_large_values = (abs_vals &gt; 1e6).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() area bedrooms age price 0 2600 3.0 20 550000 1 3000 4.0 15 565000 2 3200 NaN 18 610000 3 3600 3.0 30 595000 4 4000 5.0 8 760000 import math median = math.floor(df.bedrooms.median()) median 4 df.bedrooms = df.bedrooms.fillna(median) df.head() area bedrooms age price 0 2600 3.0 20 550000 1 3000 4.0 15 565000 2 3200 4.0 18 610000 3 3600 3.0 30 595000 4 4000 5.0 8 760000 df.columns Index(['area', 'bedrooms', 'age', 'price'], dtype='object') X = df.drop(columns = ['price']) y = df['price'] X.shape, y.shape ((6, 3), (6,)) reg = linear_model.LinearRegression() reg.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() reg.coef_ # Coefficients for all the features array([ 112.06244194, 23388.88007794, -3231.71790863]) reg.intercept_ 221323.00186540396 EXERCISE ! pip install word2number Collecting word2number Downloading word2number-1.1.zip (9.7 kB) Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone Building wheels for collected packages: word2number Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=984933753a2a5e54641beece9aa2be748ba771834a67570b7dbb4bc0eca13469 Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06 Successfully built word2number Installing collected packages: word2number Successfully installed word2number-1.1 from word2number import w2n df = pd.read_csv(\"/kaggle/input/hiring/hiring.csv\") df.head() experience test_score(out of 10) interview_score(out of 10) salary($) 0 NaN 8.0 9 50000 1 NaN 8.0 6 45000 2 five 6.0 7 60000 3 two 10.0 10 65000 4 seven 9.0 6 70000 df.shape (8, 4) df.isna().sum() experience 2 test_score(out of 10) 1 interview_score(out of 10) 0 salary($) 0 dtype: int64 df['test_score(out of 10)'] /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater has_large_values = (abs_vals &gt; 1e6).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() 0 8.0 1 8.0 2 6.0 3 10.0 4 9.0 5 7.0 6 NaN 7 7.0 Name: test_score(out of 10), dtype: float64 median = math.floor(df['test_score(out of 10)'].median()) median df['test_score(out of 10)'] = df['test_score(out of 10)'].fillna(median) df['test_score(out of 10)'] 0 8.0 1 8.0 2 6.0 3 10.0 4 9.0 5 7.0 6 8.0 7 7.0 Name: test_score(out of 10), dtype: float64 df['experience'] 0 NaN 1 NaN 2 five 3 two 4 seven 5 three 6 ten 7 eleven Name: experience, dtype: object def convert_experience(value): if isinstance(value, str): return w2n.word_to_num(value) return value # Apply the function to the 'experience' column df['experience'] = df['experience'].apply(convert_experience) # Print the updated DataFrame to verify the conversion print(df) experience test_score(out of 10) interview_score(out of 10) salary($) 0 NaN 8.0 9 50000 1 NaN 8.0 6 45000 2 5.0 6.0 7 60000 3 2.0 10.0 10 65000 4 7.0 9.0 6 70000 5 3.0 7.0 10 62000 6 10.0 8.0 7 72000 7 11.0 7.0 8 80000 /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater has_large_values = (abs_vals &gt; 1e6).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() /usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater has_small_values = ((abs_vals &lt; 10 ** (-self.digits)) &amp; (abs_vals &gt; 0)).any() median = math.floor(df['experience'].median()) median df['experience'] = df['experience'].fillna(median) df['experience'] 0 6.0 1 6.0 2 5.0 3 2.0 4 7.0 5 3.0 6 10.0 7 11.0 Name: experience, dtype: float64 X = df.drop(columns = ['salary($)']) y = df['salary($)'] X.shape, y.shape ((8, 3), (8,)) reg = linear_model.LinearRegression() reg.fit(X, y) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() reg.coef_ array([2813.00813008, 1333.33333333, 2926.82926829]) reg.intercept_ 11869.91869918695 reg.predict([[2, 9, 6]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([47056.91056911]) reg.predict([[12,10, 10]]) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([88227.64227642])"
  },
  
  
  
  {
    "title": "K-Fold Cross Validation - Notebook",
    "url": "/Machine-Learningprojects/k-fold-cross-validation/notebook",
    "content": "K-Fold Cross Validation - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session LOAD THE DATASETS AND LIBRARIES import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier import warnings warnings.filterwarnings('ignore') from sklearn.datasets import load_digits data = load_digits() dir(data) ['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names'] df = pd.DataFrame(data.data) df['target'] = data.target df.sample(10) 0 1 2 3 4 5 6 7 8 9 ... 55 56 57 58 59 60 61 62 63 target 478 0.0 0.0 4.0 10.0 13.0 3.0 0.0 0.0 0.0 4.0 ... 0.0 0.0 0.0 4.0 12.0 14.0 11.0 2.0 0.0 9 927 0.0 2.0 13.0 16.0 10.0 0.0 0.0 0.0 0.0 12.0 ... 0.0 0.0 1.0 13.0 16.0 16.0 16.0 16.0 3.0 2 224 0.0 0.0 0.0 10.0 12.0 3.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 2.0 10.0 14.0 13.0 4.0 0.0 8 288 0.0 0.0 6.0 8.0 8.0 13.0 3.0 0.0 0.0 1.0 ... 0.0 0.0 0.0 9.0 15.0 8.0 0.0 0.0 0.0 5 313 0.0 2.0 11.0 16.0 12.0 1.0 0.0 0.0 0.0 9.0 ... 0.0 0.0 2.0 15.0 16.0 16.0 13.0 16.0 1.0 2 1072 0.0 0.0 5.0 10.0 11.0 13.0 12.0 0.0 0.0 2.0 ... 0.0 0.0 0.0 7.0 11.0 0.0 0.0 0.0 0.0 7 345 0.0 0.0 12.0 16.0 16.0 13.0 1.0 0.0 0.0 4.0 ... 0.0 0.0 0.0 12.0 16.0 16.0 9.0 0.0 0.0 3 209 0.0 0.0 0.0 14.0 7.0 0.0 0.0 0.0 0.0 1.0 ... 0.0 0.0 0.0 2.0 10.0 13.0 6.0 0.0 0.0 0 820 0.0 0.0 3.0 15.0 16.0 15.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 4.0 14.0 1.0 0.0 0.0 0.0 7 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 10 rows √ó 65 columns X = df.drop(columns = ['target']) y = df['target'] X.shape, y.shape ((1797, 64), (1797,)) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) X_train.shape, y_train.shape ((1257, 64), (1257,)) lr = LogisticRegression() lr.fit(X_train, y_train) lr.score(X_test, y_test) 0.9648148148148148 svm = SVC() svm.fit(X_train, y_train) svm.score(X_test, y_test) 0.987037037037037 rf = RandomForestClassifier() rf.fit(X_train, y_train) rf.score(X_test, y_test) 0.9722222222222222 This method of checking which model is better is too naive, and it is not a best practical method to know it. Because the dataset is not split correctly from sklearn.model_selection import KFold kf = KFold(n_splits = 3) kf KFold(n_splits=3, random_state=None, shuffle=False) for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]): print(train_index, test_index) # this is how kfold works, where you can see the whole dataset is split in a balanced way, and given all the dataset in train and test split [3 4 5 6 7 8] [0 1 2] [0 1 2 6 7 8] [3 4 5] [0 1 2 3 4 5] [6 7 8] def get_score(model, X_train, X_test, y_train, y_test): model.fit(X_train, y_train) return model.score(X_test, y_test) # This function makes it easy to return the scores of all the comparing models get_score(lr, X_train, X_test, y_train, y_test) 0.9648148148148148 get_score(svm, X_train, X_test, y_train, y_test) 0.987037037037037 get_score(rf, X_train, X_test, y_train, y_test) 0.975925925925926 # The stratified version of KFold, this devides the target column in uniform way to get the balaced data in all the folds from sklearn.model_selection import StratifiedKFold folds = StratifiedKFold(n_splits = 3) scores_l = [] scores_svm = [] scores_rf = [] for train_index, test_index in folds.split(X, y): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y.iloc[train_index], y.iloc[test_index] print(get_score(lr, X_train, X_test, y_train, y_test)) print(get_score(svm, X_train, X_test, y_train, y_test)) print(get_score(rf, X_train, X_test, y_train, y_test)) 0.9215358931552587 0.9649415692821369 0.9432387312186978 0.9415692821368948 0.9799666110183639 0.9532554257095158 0.9165275459098498 0.9649415692821369 0.9332220367278798 scores_l = [] scores_svm = [] scores_rf = [] for train_index, test_index in folds.split(X, y): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y.iloc[train_index], y.iloc[test_index] scores_l.append(get_score(lr, X_train, X_test, y_train, y_test)) scores_svm.append(get_score(svm, X_train, X_test, y_train, y_test)) scores_rf.append(get_score(rf, X_train, X_test, y_train, y_test)) scores_l np.average(scores_l) 0.9265442404006677 scores_svm np.average(scores_svm) # This is the best model that we got 0.9699499165275459 scores_rf np.average(scores_rf) # lets take the average and get which model is the best 0.9393433500278241 # Instead of creating a different funcion as above, we have a built in library from sklearn.model_selection import cross_val_score cross_val_score(LogisticRegression(), X, y) array([0.92222222, 0.86944444, 0.94150418, 0.93871866, 0.89693593]) cross_val_score(svm, X, y) array([0.96111111, 0.94444444, 0.98328691, 0.98885794, 0.93871866]) cross_val_score(rf, X, y) array([0.93055556, 0.90277778, 0.95821727, 0.95543175, 0.93036212]) # we can do directly like this also scores1 = cross_val_score(LogisticRegression(), X, y, cv = 10) np.average(scores1) 0.928193668528864 scores2 = cross_val_score(svm, X, y, cv = 10) np.average(scores2) # The best model 0.9699503414028554 scores3 = cross_val_score(rf, X, y, cv = 10) np.average(scores3) 0.9487988826815641"
  },
  
  
  
  {
    "title": "Random Forest Classifier - Notebook",
    "url": "/Machine-Learningprojects/random-forest/notebook",
    "content": "Random Forest Classifier - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session IMPORT ALL THE NECESSARY LIBRARIES, AND LOAD THE DATASET import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings('ignore') from sklearn.datasets import load_digits data = load_digits() dir(data) ['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names'] data.data array([[ 0., 0., 5., ..., 0., 0., 0.], [ 0., 0., 0., ..., 10., 0., 0.], [ 0., 0., 0., ..., 16., 9., 0.], ..., [ 0., 0., 1., ..., 6., 0., 0.], [ 0., 0., 2., ..., 12., 0., 0.], [ 0., 0., 10., ..., 12., 1., 0.]]) data.feature_names ['pixel_0_0', 'pixel_0_1', 'pixel_0_2', 'pixel_0_3', 'pixel_0_4', 'pixel_0_5', 'pixel_0_6', 'pixel_0_7', 'pixel_1_0', 'pixel_1_1', 'pixel_1_2', 'pixel_1_3', 'pixel_1_4', 'pixel_1_5', 'pixel_1_6', 'pixel_1_7', 'pixel_2_0', 'pixel_2_1', 'pixel_2_2', 'pixel_2_3', 'pixel_2_4', 'pixel_2_5', 'pixel_2_6', 'pixel_2_7', 'pixel_3_0', 'pixel_3_1', 'pixel_3_2', 'pixel_3_3', 'pixel_3_4', 'pixel_3_5', 'pixel_3_6', 'pixel_3_7', 'pixel_4_0', 'pixel_4_1', 'pixel_4_2', 'pixel_4_3', 'pixel_4_4', 'pixel_4_5', 'pixel_4_6', 'pixel_4_7', 'pixel_5_0', 'pixel_5_1', 'pixel_5_2', 'pixel_5_3', 'pixel_5_4', 'pixel_5_5', 'pixel_5_6', 'pixel_5_7', 'pixel_6_0', 'pixel_6_1', 'pixel_6_2', 'pixel_6_3', 'pixel_6_4', 'pixel_6_5', 'pixel_6_6', 'pixel_6_7', 'pixel_7_0', 'pixel_7_1', 'pixel_7_2', 'pixel_7_3', 'pixel_7_4', 'pixel_7_5', 'pixel_7_6', 'pixel_7_7'] data.target array([0, 1, 2, ..., 8, 9, 8]) plt.imshow(data.images[2]) &lt;matplotlib.image.AxesImage at 0x7afbf597b8d0&gt; df = pd.DataFrame(data.data) df.head() 0 1 2 3 4 5 6 7 8 9 ... 54 55 56 57 58 59 60 61 62 63 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 ... 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 ... 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 5 rows √ó 64 columns df['target'] = data.target df.head() 0 1 2 3 4 5 6 7 8 9 ... 55 56 57 58 59 60 61 62 63 target 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 ... 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 5 rows √ó 65 columns plt.plot(df) [&lt;matplotlib.lines.Line2D at 0x7afbf51aaa10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d12e10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d13190&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d134d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d13750&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d13b90&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d13fd0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc42d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d137d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d13b10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf36e4f10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc5150&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc5510&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc5910&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc5cd0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc6090&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc6410&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d97190&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc6b10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc6ed0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc7290&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc7690&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc7a10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d3fe90&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc7cd0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d97b50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcc8d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d04d90&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcd010&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc6a50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc66d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcdb50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcdf10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dce2d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dce650&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcea50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcee50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d3f310&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcf5d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcf9d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc7e90&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd4150&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcc610&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcc9d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd4c50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcd110&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd53d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcd890&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dcdc50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd5ed0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd62d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd6690&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dc5450&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d3d0d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd7110&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd7490&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2dd7850&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2d97d10&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2cec110&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2cec4d0&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2ddc750&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2ddcb50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2ddcf50&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf3a9cf90&gt;, &lt;matplotlib.lines.Line2D at 0x7afbf2ddd6d0&gt;] X = df.drop(columns = ['target']) y = df['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,y ,test_size = 0.2, random_state = 42) X_train.shape, y_train.shape ((1437, 64), (1437,)) from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier() model.fit(X_train, y_train) RandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier() model.score(X_test, y_test) 0.9694444444444444 y_pred = model.predict(X_test) y_pred[:3], y_test[:3] (array([6, 9, 3]), 1245 6 220 9 1518 3 Name: target, dtype: int64) # Lets tune this model using 20 trees model = RandomForestClassifier(n_estimators = 30) model.fit(X_train, y_train) RandomForestClassifier(n_estimators=30)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=30) model.score(X_test, y_test) # WE are getting slightly impproved model 0.9666666666666667 # Lets do some more tuning model = RandomForestClassifier(n_estimators = 100) model.fit(X_train, y_train) RandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier() model.score(X_test, y_test) 0.9722222222222222 # Lets do some more tuning model = RandomForestClassifier(n_estimators = 200) model.fit(X_train, y_train) RandomForestClassifier(n_estimators=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=200) model.score(X_test, y_test) 0.9777777777777777 y_pred = model.predict(X_test) y_pred[:3], y_test[:3] (array([6, 9, 3]), 1245 6 220 9 1518 3 Name: target, dtype: int64) from sklearn.metrics import confusion_matrix, classification_report cr = classification_report(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(cr) precision recall f1-score support 0 1.00 1.00 1.00 33 1 0.93 1.00 0.97 28 2 1.00 1.00 1.00 33 3 1.00 0.97 0.99 34 4 1.00 1.00 1.00 46 5 0.94 0.98 0.96 47 6 0.97 0.97 0.97 35 7 0.97 0.97 0.97 34 8 1.00 0.93 0.97 30 9 0.97 0.95 0.96 40 accuracy 0.98 360 macro avg 0.98 0.98 0.98 360 weighted avg 0.98 0.98 0.98 360 import seaborn as sns plt.figure(figsize = (10, 7)) sns.heatmap(cm, annot=True) plt.xlabel('Predicted') plt.ylabel('Actual') Text(95.72222222222221, 0.5, 'Actual')"
  },
  
  
  
  {
    "title": "Support Vector Machine - Notebook",
    "url": "/Machine-Learningprojects/support-vector-machine/notebook",
    "content": "Support Vector Machine - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings('ignore') from sklearn.datasets import load_iris iris = load_iris() dir(iris) ['DESCR', 'data', 'data_module', 'feature_names', 'filename', 'frame', 'target', 'target_names'] iris.feature_names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] df = pd.DataFrame(iris.data, columns = iris.feature_names) df.head() sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 df['target'] = iris.target df.sample(10) # Now we have target columns also sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 24 4.8 3.4 1.9 0.2 0 114 5.8 2.8 5.1 2.4 2 1 4.9 3.0 1.4 0.2 0 36 5.5 3.5 1.3 0.2 0 148 6.2 3.4 5.4 2.3 2 93 5.0 2.3 3.3 1.0 1 72 6.3 2.5 4.9 1.5 1 69 5.6 2.5 3.9 1.1 1 83 6.0 2.7 5.1 1.6 1 124 6.7 3.3 5.7 2.1 2 iris.target_names # We have three types of iris flower, they are represented by 0, 1, 2 array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10') df[df.target == 0].head() # This is for setosa sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 df[df.target == 1].head() # This is for versicolor sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 50 7.0 3.2 4.7 1.4 1 51 6.4 3.2 4.5 1.5 1 52 6.9 3.1 4.9 1.5 1 53 5.5 2.3 4.0 1.3 1 54 6.5 2.8 4.6 1.5 1 df[df.target == 2].head() # This is for verginica sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target 100 6.3 3.3 6.0 2.5 2 101 5.8 2.7 5.1 1.9 2 102 7.1 3.0 5.9 2.1 2 103 6.3 2.9 5.6 1.8 2 104 6.5 3.0 5.8 2.2 2 df['flower_name'] = df.target.apply(lambda x: iris.target_names[x]) df.sample(10) # Now we have new column sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target flower_name 52 6.9 3.1 4.9 1.5 1 versicolor 140 6.7 3.1 5.6 2.4 2 virginica 130 7.4 2.8 6.1 1.9 2 virginica 65 6.7 3.1 4.4 1.4 1 versicolor 74 6.4 2.9 4.3 1.3 1 versicolor 75 6.6 3.0 4.4 1.4 1 versicolor 24 4.8 3.4 1.9 0.2 0 setosa 107 7.3 2.9 6.3 1.8 2 virginica 91 6.1 3.0 4.6 1.4 1 versicolor 18 5.7 3.8 1.7 0.3 0 setosa # Creating three different dataframe df0 = df[df.target == 0] df1 = df[df.target == 1] df2 = df[df.target == 2] df0.head() sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target flower_name 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa plt.xlabel('Sepal Length') plt.xlabel('Sepal Width') plt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'], color = 'green') plt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'], color = 'blue') &lt;matplotlib.collections.PathCollection at 0x7aeb1c8f07d0&gt; plt.xlabel('Petal Length') plt.xlabel('Petal Width') plt.scatter(df0['petal length (cm)'], df0['petal width (cm)'], color = 'green') plt.scatter(df1['petal length (cm)'], df1['petal width (cm)'], color = 'blue') &lt;matplotlib.collections.PathCollection at 0x7aeb1c7bf810&gt; df.head() sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target flower_name 0 5.1 3.5 1.4 0.2 0 setosa 1 4.9 3.0 1.4 0.2 0 setosa 2 4.7 3.2 1.3 0.2 0 setosa 3 4.6 3.1 1.5 0.2 0 setosa 4 5.0 3.6 1.4 0.2 0 setosa X = df.drop(columns = ['target', 'flower_name']) y = df['target'] X.shape, y.shape ((150, 4), (150,)) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape, y_train.shape ((120, 4), (120,)) from sklearn.svm import SVC model = SVC() model.fit(X_train, y_train) SVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC() y_pred = model.predict(X_test) y_pred[:10], y_test[:10] # As you can see the model is predicting every value correct (array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1]), 73 1 18 0 118 2 78 1 76 1 31 0 64 1 141 2 68 1 82 1 Name: target, dtype: int64) model.score(X_test, y_test) 1.0 import pickle with open('model_1', 'wb') as f: pickle.dump(model, f) with open('model_1', 'rb') as f: model = pickle.load(f) import joblib joblib.dump(model, 'model_2') ['model_2'] model = joblib.load('model_2') Exercise from sklearn.datasets import load_digits digits = load_digits() dir(digits) ['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names'] df = pd.DataFrame(digits.data) df.head() 0 1 2 3 4 5 6 7 8 9 ... 54 55 56 57 58 59 60 61 62 63 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 ... 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 ... 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 5 rows √ó 64 columns df.shape (1797, 64) df.columns RangeIndex(start=0, stop=64, step=1) df['target'] = digits.target df.head() 0 1 2 3 4 5 6 7 8 9 ... 55 56 57 58 59 60 61 62 63 target 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 ... 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 5 rows √ó 65 columns X = df.drop(columns = ['target']) y = df['target'] X.shape, y.shape ((1797, 64), (1797,)) from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) X_train.shape, y_train.shape ((1437, 64), (1437,)) from sklearn.svm import SVC model = SVC() model.fit(X_train, y_train) SVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC() model.score(X_test, y_test) 0.9861111111111112 y_pred=model.predict(X_test) y_pred[:5], y_test[:5] (array([6, 9, 3, 7, 2]), 1245 6 220 9 1518 3 438 7 1270 2 Name: target, dtype: int64) from sklearn.metrics import classification_report, confusion_matrix cr = classification_report(y_test, y_pred) cm = confusion_matrix(y_test, y_pred) print(cr) precision recall f1-score support 0 1.00 1.00 1.00 33 1 1.00 1.00 1.00 28 2 1.00 1.00 1.00 33 3 1.00 1.00 1.00 34 4 1.00 1.00 1.00 46 5 0.98 0.98 0.98 47 6 0.97 1.00 0.99 35 7 0.97 0.97 0.97 34 8 1.00 0.97 0.98 30 9 0.95 0.95 0.95 40 accuracy 0.99 360 macro avg 0.99 0.99 0.99 360 weighted avg 0.99 0.99 0.99 360 import seaborn as sns plt.xlabel(\"Predictions\") plt.xlabel(\"Actual\") sns.heatmap(cm, cmap = \"Blues\", annot = True) plt.legend() plt.show() No artists with labels found to put in legend. Note that artists whose label start with an underscore are ignored when legend() is called with no argument. import pickle with open(\"model_svm\", 'wb') as f: pickle.dump(model, f) with open(\"model_svm\", 'rb') as f: model = pickle.load(f) import joblib joblib.dump(model, 'model_svm_job') ['model_svm_job'] model = joblib.load('model_svm_job')"
  },
  
  
  
  {
    "title": "Linear Regression - Notebook",
    "url": "/Machine-Learningprojects/1-linear-regression/notebook",
    "content": "Linear Regression - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session LOAD AND IMPORT LIBRARIES AND DATASETS import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import linear_model df = pd.read_csv('/kaggle/input/homeprices/homeprices.csv') df.head() area price 0 2600 550000 1 3000 565000 2 3200 610000 3 3600 680000 4 4000 725000 df.shape (5, 2) df.columns Index(['area', 'price'], dtype='object') plt.scatter(df.area, df.price) &lt;matplotlib.collections.PathCollection at 0x77fcb19945d0&gt; plt.scatter(df.area, df.price, color = \"red\", marker = \"+\") plt.xlabel(\"Area\") plt.ylabel(\"Price\") Text(0, 0.5, 'Price') BUILD THE MODEL AND PREDICT SOMETHINGS df.area = np.array(df.area) df.price= np.array(df.price) # Linear Regression model built reg = linear_model.LinearRegression() reg.fit(df[['area']], df.price) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() reg.predict(np.array([3300]).reshape(1, -1)) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([628715.75342466]) reg.coef_ array([135.78767123]) reg.intercept_ 180616.43835616432 y = m * x + b m is coefficient x is input b is intercept y = 135.78767123 * 3300 + 180616.43835616432 y # This matches the output that we got from predicting the value 628715.7534151643 plt.xlabel('Area', fontsize = 20) plt.ylabel('Prices', fontsize = 20) plt.scatter(df.area, df.price, color = \"red\", marker = \"+\") plt.plot(df.area, reg.predict(df[['area']]), color='blue') [&lt;matplotlib.lines.Line2D at 0x77fcb3de3050&gt;] EXERCISE df = pd.read_csv(\"/kaggle/input/per-capita-income/canada_per_capita_income.csv\") df.shape (47, 2) df.columns Index(['year', 'per capita income (US$)'], dtype='object') type(df.columns) pandas.core.indexes.base.Index df.head() year per capita income (US$) 0 1970 3399.299037 1 1971 3768.297935 2 1972 4251.175484 3 1973 4804.463248 4 1974 5576.514583 plt.xlabel('Year') plt.xlabel('Per capita income') plt.scatter(df.year, df['per capita income (US$)'], marker = '+', color = 'green') &lt;matplotlib.collections.PathCollection at 0x77fcb0fce510&gt; reg = linear_model.LinearRegression() reg.fit(df[['year']], df['per capita income (US$)']) LinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression() reg.coef_ array([828.46507522]) reg.intercept_ -1632210.7578554575 reg.predict(np.array([2000]).reshape(1, -1)) /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names warnings.warn( array([24719.39258996]) # y = m * x + b y = 828.46507522 * 2000 + -1632210.7578554575 y 24719.39258454251 plt.xlabel(\"Year\") plt.ylabel(\"per capita income\") plt.scatter(df.year, df['per capita income (US$)'], marker = '+', color = 'green') plt.plot(df.year, reg.predict(df[['year']]), color = \"red\") [&lt;matplotlib.lines.Line2D at 0x77fcb0e9a890&gt;]"
  },
  
  
  
  {
    "title": "Logistic Regression - Notebook",
    "url": "/Machine-Learningprojects/logistic-regression/notebook",
    "content": "Logistic Regression - Jupyter Notebook # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the read-only \"../input/\" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk('/kaggle/input'): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save &amp; Run All\" # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LogisticRegression df = pd.read_csv(\"/kaggle/input/dataset-csv/insurance_data.csv\") df.head() age bought_insurance 0 22 0 1 25 0 2 47 1 3 52 0 4 46 1 df.shape (27, 2) plt.scatter(df['age'], df['bought_insurance'], marker = '+', color = 'red') # As you can see the plot, we can't actually draw a line because the data is distributed on top and bottom &lt;matplotlib.collections.PathCollection at 0x7c020c47f090&gt; from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(df[['age']], df.bought_insurance, test_size = 0.2, random_state = 42) X_train.shape, X_test.shape ((21, 1), (6, 1)) model = LogisticRegression() model.fit(X_train, y_train) LogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression() y_pred = model.predict(X_test) y_pred, y_test # As you can see, the model is getting every answers right (array([1, 0, 1, 0, 0, 0]), 8 1 13 0 9 1 21 0 0 0 11 0 Name: bought_insurance, dtype: int64) model.score(X_test, y_test) # Our model is perfect 1.0 age_sorted = np.sort(df.age) probabilities = model.predict_proba(age_sorted.reshape(-1, 1))[:, 1] probabilities /usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names warnings.warn( array([0.1073847 , 0.1073847 , 0.11871444, 0.14448766, 0.15903454, 0.17474682, 0.20978901, 0.20978901, 0.22914939, 0.24973178, 0.27151175, 0.29444574, 0.59144934, 0.71815862, 0.740472 , 0.76160524, 0.80021757, 0.81768362, 0.84901371, 0.87577782, 0.88756613, 0.88756613, 0.8983656 , 0.91723431, 0.93286179, 0.93960657, 0.94571319]) age_sorted.reshape(-1, 1) # this is the 2D array, that we had to give to the predict_proba() array([[18], [18], [19], [21], [22], [23], [25], [25], [26], [27], [28], [29], [40], [45], [46], [47], [49], [50], [52], [54], [55], [55], [56], [58], [60], [61], [62]]) plt.xlabel(\"Age\") plt.ylabel(\"Bought insurance (Probability)\") plt.scatter(df.age, df.bought_insurance, marker = '+', color = 'green', label='Actual Data') plt.plot(age_sorted, probabilities, color = \"red\", label='Logistic Regression Curve') plt.title('Logistic Regression Fitted Curve') plt.show()"
  },
  
  
  
  {
    "title": "Project 1: Linear Regression Analysis",
    "url": "/Machine-Learningprojects/project1-linear-regression.html",
    "content": "üìä Project 1: Linear Regression Analysis üéØ Project Overview Objective: Implement linear regression from scratch and analyze its performance on a real dataset. Status: üöß In Progress Skills Learned: Mathematical foundation of linear regression Gradient descent optimization Cost function analysis Data visualization üîó Related Concepts Before starting this project, review these concept pages: üìñ Linear Regression Theory üìñ Gradient Descent (to be created) üìñ Cost Functions (to be created) üìù Problem Statement We‚Äôll predict house prices based on features like size, number of bedrooms, and location. This is a classic regression problem where we want to find the best line (or hyperplane) that fits our data. Dataset Source: Housing price dataset Features: Square footage, bedrooms, bathrooms, age Target: Price in thousands of dollars üîß Implementation Step 1: Data Preparation import numpy as np import matplotlib.pyplot as plt import pandas as pd # Load and explore data def load_data(): # Implementation here pass # Feature normalization def normalize_features(X): # Implementation here pass Step 2: Cost Function def compute_cost(X, y, theta): \"\"\" Compute cost function J(Œ∏) = 1/(2m) * Œ£(hŒ∏(x) - y)¬≤ \"\"\" m = len(y) predictions = X.dot(theta) cost = (1/(2*m)) * np.sum((predictions - y)**2) return cost Step 3: Gradient Descent def gradient_descent(X, y, theta, alpha, iterations): \"\"\" Perform gradient descent to learn Œ∏ Œ∏ := Œ∏ - Œ± * (1/m) * X^T * (X*Œ∏ - y) \"\"\" m = len(y) cost_history = [] for i in range(iterations): predictions = X.dot(theta) theta = theta - (alpha/m) * X.T.dot(predictions - y) cost_history.append(compute_cost(X, y, theta)) return theta, cost_history üìä Results &amp; Analysis Cost Function Convergence # Plot cost function over iterations def plot_cost_function(cost_history): plt.figure(figsize=(10, 6)) plt.plot(cost_history) plt.title('Cost Function Convergence') plt.xlabel('Iterations') plt.ylabel('Cost J(Œ∏)') plt.grid(True) plt.show() Model Performance Final Cost: [To be calculated] R¬≤ Score: [To be calculated] Mean Squared Error: [To be calculated] Visualizations Scatter plot of actual vs predicted prices Cost function convergence over iterations Feature importance analysis üí° Key Learnings Mathematical Insights How gradient descent finds the optimal parameters The importance of feature normalization Relationship between learning rate and convergence Implementation Insights Vectorized operations are much faster than loops Proper data preprocessing is crucial Visualization helps understand the learning process Next Steps Experiment with different learning rates Try polynomial features Compare with scikit-learn implementation üîÑ Concept Updates This project helped update the following concept pages: ‚úÖ Linear Regression - Added practical examples üîÑ Gradient Descent - To be created with project insights üîÑ Cost Functions - To be created with visualizations üìÅ Project Files project1-linear-regression/ ‚îú‚îÄ‚îÄ data/ ‚îÇ ‚îî‚îÄ‚îÄ housing_data.csv ‚îú‚îÄ‚îÄ notebooks/ ‚îÇ ‚îî‚îÄ‚îÄ linear_regression_analysis.ipynb ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ linear_regression.py ‚îÇ ‚îî‚îÄ‚îÄ utils.py ‚îî‚îÄ‚îÄ results/ ‚îî‚îÄ‚îÄ plots/ [Add actual implementation files and results as you complete the project]"
  },
  
  
  
  
  
  {
    "title": "Syllabus",
    "url": "/Machine-Learningintro/syllabus.html",
    "content": "## Course Syllabus This section will contain the course syllabus for CS229 Machine Learning. ### Topics Covered - Introduction to Machine Learning - Supervised Learning - Unsupervised Learning - Reinforcement Learning - And more... _Content to be added as the course progresses._"
  },
  
  
  
  
  
  
  
  
]
